{"./":{"url":"./","title":"PyTorch 中文官方教程 1.7","keywords":"","body":"PyTorch 中文官方教程 1.7 原文：WELCOME TO PYTORCH TUTORIALS 协议：CC BY-NC-SA 4.0 自豪地采用谷歌翻译 不要担心自己的形象，只关心如何实现目标。——《原则》，生活原则 2.3.c 在线阅读 ApacheCN 面试求职交流群 724187166 ApacheCN 学习资源 贡献指南 本项目需要校对，欢迎大家提交 Pull Request。 请您勇敢地去翻译和改进翻译。虽然我们追求卓越，但我们并不要求您做到十全十美，因此请不要担心因为翻译上犯错——在大部分情况下，我们的服务器已经记录所有的翻译，因此您不必担心会因为您的失误遭到无法挽回的破坏。（改编自维基百科） 联系方式 负责人 飞龙: 562826179 其他 在我们的 apachecn/apachecn-tf-zh github 上提 issue. 发邮件到 Email: apachecn@163.com. 在我们的 组织学习交流群 中联系群主/管理员即可. 赞助我们 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"01.html":{"url":"01.html","title":"学习 PyTorch","keywords":"","body":"学习 PyTorch 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"02.html":{"url":"02.html","title":"PyTorch 深度学习：60 分钟的突击","keywords":"","body":"PyTorch 深度学习：60分钟快速入门 原文：https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html 作者： Soumith Chintala https://www.youtube.com/embed/u7x8RXwLKcA 什么是 PyTorch？ PyTorch 是基于以下两个目的而打造的python科学计算框架： 无缝替换NumPy，并且通过利用GPU的算力来实现神经网络的加速。 通过自动微分机制，来让神经网络的实现变得更加容易。 本次教程的目标： 深入了解PyTorch的张量单元以及如何使用Pytorch来搭建神经网络。 自己动手训练一个小型神经网络来实现图像的分类。 注意 确保已安装torch和torchvision包。 张量 torch.autograd的简要介绍 神经网络简介 自己动手训练一个图像分类器 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"03.html":{"url":"03.html","title":"张量","keywords":"","body":"张量 原文: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py 张量如同数组和矩阵一样, 是一种特殊的数据结构。在PyTorch中, 神经网络的输入、输出以及网络的参数等数据, 都是使用张量来进行描述。 张量的使用和Numpy中的ndarrays很类似, 区别在于张量可以在GPU或其它专用硬件上运行, 这样可以得到更快的加速效果。如果你对ndarrays很熟悉的话, 张量的使用对你来说就很容易了。如果不太熟悉的话, 希望这篇有关张量API的快速入门教程能够帮到你。 import torch import numpy as np 张量初始化 张量有很多种不同的初始化方法, 先来看看四个简单的例子： 1. 直接生成张量 由原始数据直接生成张量, 张量类型由原始数据类型决定。 data = [[1, 2], [3, 4]] x_data = torch.tensor(data) 2. 通过Numpy数组来生成张量 由已有的Numpy数组来生成张量(反过来也可以由张量来生成Numpy数组, 参考张量与Numpy之间的转换)。 np_array = np.array(data) x_np = torch.from_numpy(np_array) 3. 通过已有的张量来生成新的张量 新的张量将继承已有张量的数据属性(结构、类型), 也可以重新指定新的数据类型。 x_ones = torch.ones_like(x_data) # 保留 x_data 的属性 print(f\"Ones Tensor: \\n {x_ones} \\n\") x_rand = torch.rand_like(x_data, dtype=torch.float) # 重写 x_data 的数据类型 int -> float print(f\"Random Tensor: \\n {x_rand} \\n\") 显示: Ones Tensor: tensor([[1, 1], [1, 1]]) Random Tensor: tensor([[0.0381, 0.5780], [0.3963, 0.0840]]) 4. 通过指定数据维度来生成张量 shape是元组类型, 用来描述张量的维数, 下面3个函数通过传入shape来指定生成张量的维数。 shape = (2,3,) rand_tensor = torch.rand(shape) ones_tensor = torch.ones(shape) zeros_tensor = torch.zeros(shape) print(f\"Random Tensor: \\n {rand_tensor} \\n\") print(f\"Ones Tensor: \\n {ones_tensor} \\n\") print(f\"Zeros Tensor: \\n {zeros_tensor}\") 显示: Random Tensor: tensor([[0.0266, 0.0553, 0.9843], [0.0398, 0.8964, 0.3457]]) Ones Tensor: tensor([[1., 1., 1.], [1., 1., 1.]]) Zeros Tensor: tensor([[0., 0., 0.], [0., 0., 0.]]) 张量属性 从张量属性我们可以得到张量的维数、数据类型以及它们所存储的设备(CPU或GPU)。 来看一个简单的例子: tensor = torch.rand(3,4) print(f\"Shape of tensor: {tensor.shape}\") print(f\"Datatype of tensor: {tensor.dtype}\") print(f\"Device tensor is stored on: {tensor.device}\") 显示: Shape of tensor: torch.Size([3, 4]) # 维数 Datatype of tensor: torch.float32 # 数据类型 Device tensor is stored on: cpu # 存储设备 张量运算 有超过100种张量相关的运算操作, 例如转置、索引、切片、数学运算、线性代数、随机采样等。更多的运算可以在这里查看。 所有这些运算都可以在GPU上运行(相对于CPU来说可以达到更高的运算速度)。如果你使用的是Google的Colab环境, 可以通过 Edit > Notebook Settings 来分配一个GPU使用。 # 判断当前环境GPU是否可用, 然后将tensor导入GPU内运行 if torch.cuda.is_available(): tensor = tensor.to('cuda') 光说不练假把式, 接下来的例子一定要动手跑一跑。如果你对Numpy的运算非常熟悉的话, 那tensor的运算对你来说就是小菜一碟。 1. 张量的索引和切片 tensor = torch.ones(4, 4) tensor[:,1] = 0 # 将第1列(从0开始)的数据全部赋值为0 print(tensor) 显示: tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) 2. 张量的拼接 你可以通过torch.cat方法将一组张量按照指定的维度进行拼接, 也可以参考torch.stack方法。这个方法也可以实现拼接操作, 但和torch.cat稍微有点不同。 t1 = torch.cat([tensor, tensor, tensor], dim=1) print(t1) 显示: tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) 3. 张量的乘积和矩阵乘法 # 逐个元素相乘结果 print(f\"tensor.mul(tensor): \\n {tensor.mul(tensor)} \\n\") # 等价写法: print(f\"tensor * tensor: \\n {tensor * tensor}\") 显示: tensor.mul(tensor): tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) tensor * tensor: tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) 下面写法表示张量与张量的矩阵乘法: print(f\"tensor.matmul(tensor.T): \\n {tensor.matmul(tensor.T)} \\n\") # 等价写法: print(f\"tensor @ tensor.T: \\n {tensor @ tensor.T}\") 显示: tensor.matmul(tensor.T): tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) tensor @ tensor.T: tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) 4. 自动赋值运算 自动赋值运算通常在方法后有 _ 作为后缀, 例如: x.copy_(y), x.t_()操作会改变 x 的取值。 print(tensor, \"\\n\") tensor.add_(5) print(tensor) 显示: tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) tensor([[6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.]]) 注意: 自动赋值运算虽然可以节省内存, 但在求导时会因为丢失了中间过程而导致一些问题, 所以我们并不鼓励使用它。 Tensor与Numpy的转化 张量和Numpy array数组在CPU上可以共用一块内存区域, 改变其中一个另一个也会随之改变。 1. 由张量变换为Numpy array数组 t = torch.ones(5) print(f\"t: {t}\") n = t.numpy() print(f\"n: {n}\") 显示: t: tensor([1., 1., 1., 1., 1.]) n: [1. 1. 1. 1. 1.] 修改张量的值，则Numpy array数组值也会随之改变。 t.add_(1) print(f\"t: {t}\") print(f\"n: {n}\") 显示: t: tensor([2., 2., 2., 2., 2.]) n: [2. 2. 2. 2. 2.] 2. 由Numpy array数组转为张量 n = np.ones(5) t = torch.from_numpy(n) 修改Numpy array数组的值，则张量值也会随之改变。 np.add(n, 1, out=n) print(f\"t: {t}\") print(f\"n: {n}\") 显示: t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64) n: [2. 2. 2. 2. 2.] 脚本的总运行时间：（0 分钟 0.045 秒） 下载 Python 源码：tensor_tutorial.py 下载 Jupyter 笔记本：tensor_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"04.html":{"url":"04.html","title":"torch.autograd的简要介绍","keywords":"","body":"torch.autograd的简要介绍 原文：https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py torch.autograd是 PyTorch 的自动差分引擎，可为神经网络训练提供支持。 在本节中，您将获得有关 Autograd 如何帮助神经网络训练的概念性理解。 背景 神经网络（NN）是在某些输入数据上执行的嵌套函数的集合。 这些函数由参数（由权重和偏差组成）定义，这些参数在 PyTorch 中存储在张量中。 训练 NN 分为两个步骤： 正向传播：在正向传播中，NN 对正确的输出进行最佳猜测。 它通过其每个函数运行输入数据以进行猜测。 反向传播：在反向传播中，NN 根据其猜测中的误差调整其参数。 它通过从输出向后遍历，收集有关函数参数（梯度）的误差导数并使用梯度下降来优化参数来实现。 有关反向传播的更详细的演练，请查看 3Blue1Brown 的视频。 在 PyTorch 中的用法 让我们来看一个训练步骤。 对于此示例，我们从torchvision加载了经过预训练的 resnet18 模型。 我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&宽度为 64，其对应的label初始化为一些随机值。 import torch, torchvision model = torchvision.models.resnet18(pretrained=True) data = torch.rand(1, 3, 64, 64) labels = torch.rand(1, 1000) 接下来，我们通过模型的每一层运行输入数据以进行预测。 这是正向传播。 prediction = model(data) # forward pass 我们使用模型的预测和相应的标签来计算误差（loss）。 下一步是通过网络反向传播此误差。 当我们在误差张量上调用.backward()时，开始反向传播。 然后，Autograd 会为每个模型参数计算梯度并将其存储在参数的.grad属性中。 loss = (prediction - labels).sum() loss.backward() # backward pass 接下来，我们加载一个优化器，在本例中为 SGD，学习率为 0.01，动量为 0.9。 我们在优化器中注册模型的所有参数。 optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) 最后，我们调用.step()启动梯度下降。 优化器通过.grad中存储的梯度来调整每个参数。 optim.step() #gradient descent 至此，您已经具备了训练神经网络所需的一切。 以下各节详细介绍了 Autograd 的工作原理-随时跳过它们。 Autograd 的微分 让我们来看看autograd如何收集梯度。 我们用requires_grad=True创建两个张量a和b。 这向autograd发出信号，应跟踪对它们的所有操作。 import torch a = torch.tensor([2., 3.], requires_grad=True) b = torch.tensor([6., 4.], requires_grad=True) 我们从a和b创建另一个张量Q。 Q = 3*a**3 - b**2 假设a和b是神经网络的参数，Q是误差。 在 NN 训练中，我们想要相对于参数的误差，即 当我们在Q上调用.backward()时，Autograd 将计算这些梯度并将其存储在各个张量的.grad属性中。 我们需要在Q.backward()中显式传递gradient参数，因为它是向量。 gradient是与Q形状相同的张量，它表示Q相对于本身的梯度，即 同样，我们也可以将Q聚合为一个标量，然后隐式地向后调用，例如Q.sum().backward()。 external_grad = torch.tensor([1., 1.]) Q.backward(gradient=external_grad) 梯度现在沉积在a.grad和b.grad中 # check if collected gradients are correct print(9*a**2 == a.grad) print(-2*b == b.grad) 出： tensor([True, True]) tensor([True, True]) 可选阅读-使用autograd的向量微积分 从数学上讲，如果您具有向量值函数y = f(x)，则y相对于x的雅可比矩阵J： 一般来说，torch.autograd是用于计算向量雅可比积的引擎。 也就是说，给定任何向量v，计算乘积J^T · v 如果v恰好是标量函数的梯度 然后根据链式规则，向量-雅可比积将是l相对于x的梯度： 上面的示例中使用的是 vector-Jacobian 乘积的这一特征。 external_grad表示v。 计算图 从概念上讲，Autograd 在由函数对象组成的有向无环图（DAG）中记录数据（张量）和所有已执行的操作（以及由此产生的新张量）。 在此 DAG 中，叶子是输入张量，根是输出张量。 通过从根到叶跟踪此图，可以使用链式规则自动计算梯度。 在正向传播中，Autograd 同时执行两项操作： 运行请求的操作以计算结果张量，并且 在 DAG 中维护操作的梯度函数。 当在 DAG 根目录上调用.backward()时，后退通道开始。 autograd然后： 从每个.grad_fn计算梯度， 将它们累积在各自的张量的.grad属性中，然后 使用链式规则，一直传播到叶子张量。 下面是我们示例中 DAG 的直观表示。 在图中，箭头指向前进的方向。 节点代表正向传播中每个操作的反向函数。 蓝色的叶节点代表我们的叶张量a和b。 注意 DAG 在 PyTorch 中是动态的。要注意的重要一点是，图是从头开始重新创建的； 在每个.backward()调用之后，Autograd 开始填充新图。 这正是允许您在模型中使用控制流语句的原因。 您可以根据需要在每次迭代中更改形状，大小和操作。 从 DAG 中排除 torch.autograd跟踪所有将其requires_grad标志设置为True的张量的操作。 对于不需要梯度的张量，将此属性设置为False会将其从梯度计算 DAG 中排除。 即使只有一个输入张量具有requires_grad=True，操作的输出张量也将需要梯度。 x = torch.rand(5, 5) y = torch.rand(5, 5) z = torch.rand((5, 5), requires_grad=True) a = x + y print(f\"Does `a` require gradients? : {a.requires_grad}\") b = x + z print(f\"Does `b` require gradients?: {b.requires_grad}\") 出： Does `a` require gradients? : False Does `b` require gradients?: True 在 NN 中，不计算梯度的参数通常称为冻结参数。 如果事先知道您不需要这些参数的梯度，则“冻结”模型的一部分很有用（通过减少自动梯度计算，这会带来一些表现优势）。 从 DAG 中排除很重要的另一个常见用例是调整预训练网络 在微调中，我们冻结了大部分模型，通常仅修改分类器层以对新标签进行预测。 让我们来看一个小例子来说明这一点。 和以前一样，我们加载一个预训练的 resnet18 模型，并冻结所有参数。 from torch import nn, optim model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False 假设我们要在具有 10 个标签的新数据集中微调模型。 在 resnet 中，分类器是最后一个线性层model.fc。 我们可以简单地将其替换为充当我们的分类器的新线性层（默认情况下未冻结）。 model.fc = nn.Linear(512, 10) 现在，除了model.fc的参数外，模型中的所有参数都将冻结。 计算梯度的唯一参数是model.fc的权重和偏差。 # Optimize only the classifier optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9) 请注意，尽管我们在优化器中注册了所有参数，但唯一可计算梯度的参数（因此会在梯度下降中进行更新）是分类器的权重和偏差。 torch.no_grad()中的上下文管理器可以使用相同的排除功能。 进一步阅读： 原地操作&多线程 Autograd 反向模式自动微分 的示例实现 脚本的总运行时间：（0 分钟 5.184 秒） 下载 Python 源码：autograd_tutorial.py 下载 Jupyter 笔记本：autograd_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"05.html":{"url":"05.html","title":"神经网络","keywords":"","body":"神经网络 原文：https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py 可以使用torch.nn包构建神经网络。 现在您已经了解了autograd，nn依赖于autograd来定义模型并对其进行微分。 nn.Module包含层，以及返回output的方法forward(input)。 例如，查看以下对数字图像进行分类的网络： 卷积网 这是一个简单的前馈网络。 它获取输入，将其一层又一层地馈入，然后最终给出输出。 神经网络的典型训练过程如下： 定义具有一些可学习参数（或权重）的神经网络 遍历输入数据集 通过网络处理输入 计算损失（输出正确的距离有多远） 将梯度传播回网络参数 通常使用简单的更新规则来更新网络的权重：weight = weight - learning_rate * gradient 定义网络 让我们定义这个网络： import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 3x3 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6*6 from image dimension self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) 出： Net( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 您只需要定义forward函数，就可以使用autograd为您自动定义backward函数（计算梯度）。 您可以在forward函数中使用任何张量操作。 模型的可学习参数由net.parameters()返回 params = list(net.parameters()) print(len(params)) print(params[0].size()) # conv1's .weight 出： 10 torch.Size([6, 1, 3, 3]) 让我们尝试一个32x32随机输入。 注意：该网络的预期输入大小（LeNet）为32x32。 要在 MNIST 数据集上使用此网络，请将图像从数据集中调整为32x32。 input = torch.randn(1, 1, 32, 32) out = net(input) print(out) 出： tensor([[ 0.1002, -0.0694, -0.0436, 0.0103, 0.0488, -0.0429, -0.0941, -0.0146, -0.0031, -0.0923]], grad_fn=) 使用随机梯度将所有参数和反向传播的梯度缓冲区归零： net.zero_grad() out.backward(torch.randn(1, 10)) 注意 torch.nn仅支持小批量。 整个torch.nn包仅支持作为微型样本而不是单个样本的输入。 例如，nn.Conv2d将采用nSamples x nChannels x Height x Width的 4D 张量。 如果您只有一个样本，只需使用input.unsqueeze(0)添加一个假批量尺寸。 在继续之前，让我们回顾一下到目前为止所看到的所有类。 回顾： torch.Tensor-一个多维数组，支持诸如backward()的自动微分操作。 同样，保持相对于张量的梯度。 nn.Module-神经网络模块。 封装参数的便捷方法，并带有将其移动到 GPU，导出，加载等的帮助器。 nn.Parameter-一种张量，即将其分配为Module的属性时，自动注册为参数。 autograd.Function-实现自动微分操作的正向和反向定义。 每个Tensor操作都会创建至少一个Function节点，该节点连接到创建Tensor的函数，并且编码其历史记录。 目前为止，我们涵盖了： 定义神经网络 处理输入并向后调用 仍然剩下： 计算损失 更新网络的权重 损失函数 损失函数采用一对（输出，目标）输入，并计算一个值，该值估计输出与目标之间的距离。 nn包下有几种不同的损失函数。 一个简单的损失是：nn.MSELoss，它计算输入和目标之间的均方误差。 例如： output = net(input) target = torch.randn(10) # a dummy target, for example target = target.view(1, -1) # make it the same shape as output criterion = nn.MSELoss() loss = criterion(output, target) print(loss) 出： tensor(0.4969, grad_fn=) 现在，如果使用.grad_fn属性向后跟随loss，您将看到一个计算图，如下所示： input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss 因此，当我们调用loss.backward()时，整个图将被微分。 损失，并且图中具有requires_grad=True的所有张量将随梯度累积其.grad张量。 为了说明，让我们向后走几步： print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU 出： 反向传播 要反向传播误差，我们要做的只是对loss.backward()。 不过，您需要清除现有的梯度，否则梯度将累积到现有的梯度中。 现在，我们将其称为loss.backward()，然后看一下向后前后conv1的偏差梯度。 net.zero_grad() # zeroes the gradient buffers of all parameters print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) 出： conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0111, -0.0064, 0.0053, -0.0047, 0.0026, -0.0153]) 现在，我们已经看到了如何使用损失函数。 稍后阅读： 神经网络包包含各种模块和损失函数，这些模块和损失函数构成了深度神经网络的构建块。 带有文档的完整列表位于此处。 唯一需要学习的是： 更新网络的权重 更新权重 实践中使用的最简单的更新规则是随机梯度下降（SGD）： weight = weight - learning_rate * gradient 我们可以使用简单的 Python 代码实现此目标： learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 但是，在使用神经网络时，您希望使用各种不同的更新规则，例如 SGD，Nesterov-SGD，Adam，RMSProp 等。为实现此目的，我们构建了一个小包装：torch.optim，可实现所有这些方法。 使用它非常简单： import torch.optim as optim # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # Does the update 注意 观察如何使用optimizer.zero_grad()将梯度缓冲区手动设置为零。 这是因为如反向传播部分中所述累积了梯度。 脚本的总运行时间：（0 分钟 3.778 秒） 下载 Python 源码：neural_networks_tutorial.py 下载 Jupyter 笔记本：neural_networks_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"06.html":{"url":"06.html","title":"训练分类器","keywords":"","body":"训练分类器 原文：https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py 就是这个。 您已经了解了如何定义神经网络，计算损失并更新网络的权重。 现在您可能在想， 数据呢？ 通常，当您必须处理图像，文本，音频或视频数据时，可以使用将数据加载到 NumPy 数组中的标准 Python 包。 然后，您可以将该数组转换为torch.*Tensor。 对于图像，Pillow，OpenCV 等包很有用 对于音频，请使用 SciPy 和 librosa 等包 对于文本，基于 Python 或 Cython 的原始加载，或者 NLTK 和 SpaCy 很有用 专门针对视觉，我们创建了一个名为torchvision的包，其中包含用于常见数据集（例如 Imagenet，CIFAR10，MNIST 等）的数据加载器，以及用于图像（即torchvision.datasets和torch.utils.data.DataLoader）的数据转换器。 这提供了极大的便利，并且避免了编写样板代码。 在本教程中，我们将使用 CIFAR10 数据集。 它具有以下类别：“飞机”，“汽车”，“鸟”，“猫”，“鹿”，“狗”，“青蛙”，“马”，“船”，“卡车”。 CIFAR-10 中的图像尺寸为3x32x32，即尺寸为32x32像素的 3 通道彩色图像。 cifar10 训练图像分类器 我们将按顺序执行以下步骤： 使用torchvision加载并标准化 CIFAR10 训练和测试数据集 定义卷积神经网络 定义损失函数 根据训练数据训练网络 在测试数据上测试网络 1.加载并标准化 CIFAR10 使用torchvision，加载 CIFAR10 非常容易。 import torch import torchvision import torchvision.transforms as transforms TorchVision 数据集的输出是[0, 1]范围的PILImage图像。 我们将它们转换为归一化范围[-1, 1]的张量。 .. 注意： If running on Windows and you get a BrokenPipeError, try setting the num_worker of torch.utils.data.DataLoader() to 0. transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 出： Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified 让我们展示一些训练图像，很有趣。 import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # show images imshow(torchvision.utils.make_grid(images)) # print labels print(' '.join('%5s' % classes[labels[j]] for j in range(4))) 出： dog truck frog horse 2.定义卷积神经网络 之前从“神经网络”部分复制神经网络，然后对其进行修改以获取 3 通道图像（而不是定义的 1 通道图像）。 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 3.定义损失函数和优化器 让我们使用分类交叉熵损失和带有动量的 SGD。 import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 4.训练网络 这是事情开始变得有趣的时候。 我们只需要遍历数据迭代器，然后将输入馈送到网络并进行优化即可。 for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') 出： [1, 2000] loss: 2.196 [1, 4000] loss: 1.849 [1, 6000] loss: 1.671 [1, 8000] loss: 1.589 [1, 10000] loss: 1.547 [1, 12000] loss: 1.462 [2, 2000] loss: 1.382 [2, 4000] loss: 1.389 [2, 6000] loss: 1.369 [2, 8000] loss: 1.332 [2, 10000] loss: 1.304 [2, 12000] loss: 1.288 Finished Training 让我们快速保存我们训练过的模型： PATH = './cifar_net.pth' torch.save(net.state_dict(), PATH) 有关保存 PyTorch 模型的更多详细信息，请参见此处。 5.根据测试数据测试网络 我们已经在训练数据集中对网络进行了 2 次训练。 但是我们需要检查网络是否学到了什么。 我们将通过预测神经网络输出的类别标签并根据实际情况进行检查来进行检查。 如果预测正确，则将样本添加到正确预测列表中。 好的，第一步。 让我们显示测试集中的图像以使其熟悉。 dataiter = iter(testloader) images, labels = dataiter.next() # print images imshow(torchvision.utils.make_grid(images)) print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) 出： GroundTruth: cat ship ship plane 接下来，让我们重新加载保存的模型（注意：这里不需要保存和重新加载模型，我们只是为了说明如何这样做）： net = Net() net.load_state_dict(torch.load(PATH)) 好的，现在让我们看看神经网络对以上这些示例的看法： outputs = net(images) 输出是 10 类的能量。 一个类别的能量越高，网络就认为该图像属于特定类别。 因此，让我们获取最高能量的指数： _, predicted = torch.max(outputs, 1) print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4))) 出： Predicted: cat ship ship plane 结果似乎还不错。 让我们看一下网络在整个数据集上的表现。 correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 出： Accuracy of the network on the 10000 test images: 53 % 看起来比偶然更好，准确率是 10%（从 10 个类中随机选择一个类）。 好像网络学到了一些东西。 嗯，哪些类的表现良好，哪些类的表现不佳： class_correct = list(0\\. for i in range(10)) class_total = list(0\\. for i in range(10)) with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1 for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) 出： Accuracy of plane : 50 % Accuracy of car : 62 % Accuracy of bird : 51 % Accuracy of cat : 32 % Accuracy of deer : 31 % Accuracy of dog : 35 % Accuracy of frog : 77 % Accuracy of horse : 70 % Accuracy of ship : 71 % Accuracy of truck : 52 % 好的，那下一步呢？ 我们如何在 GPU 上运行这些神经网络？ 在 GPU 上进行训练 就像将张量转移到 GPU 上一样，您也将神经网络转移到 GPU 上。 如果可以使用 CUDA，首先将我们的设备定义为第一个可见的 cuda 设备： device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Assuming that we are on a CUDA machine, this should print a CUDA device: print(device) 出： cuda:0 本节的其余部分假定device是 CUDA 设备。 然后，这些方法将递归遍历所有模块，并将其参数和缓冲区转换为 CUDA 张量： net.to(device) 请记住，您还必须将每一步的输入和目标也发送到 GPU： inputs, labels = data[0].to(device), data[1].to(device) 与 CPU 相比，为什么我没有注意到 MASSIVE 加速？ 因为您的网络真的很小。 练习：尝试增加网络的宽度（第一个nn.Conv2d的参数 2 和第二个nn.Conv2d的参数 1 –它们必须是相同的数字），看看您可以得到哪种加速。 已实现的目标： 全面了解 PyTorch 的张量库和神经网络。 训练一个小型神经网络对图像进行分类 在多个 GPU 上进行训练 如果您想使用所有 GPU 来获得更大的大规模加速，请查看可选：数据并行。 我下一步要去哪里？ 训练神经网络玩视频游戏 在 imagenet 上训练最先进的 ResNet 网络 使用生成对抗网络训练人脸生成器 使用递归 LSTM 网络训练单词级语言模型 更多示例 更多教程 在论坛上讨论 PyTorch 在 Slack 上与其他用户聊天 脚本的总运行时间：（2 分钟 39.965 秒） 下载 Python 源码：cifar10_tutorial.py 下载 Jupyter 笔记本：cifar10_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"07.html":{"url":"07.html","title":"通过示例学习 PyTorch","keywords":"","body":"通过示例学习 PyTorch 原文：https://pytorch.org/tutorials/beginner/pytorch_with_examples.html 作者：Justin Johnson 本教程通过独立的示例介绍 PyTorch 的基本概念。 PyTorch 的核心是提供两个主要功能： n 维张量，类似于 NumPy，但可以在 GPU 上运行 用于构建和训练神经网络的自动微分 我们将使用将三阶多项式拟合y = sin(x)的问题作为运行示例。 该网络将具有四个参数，并且将通过使网络输出与实际输出之间的欧几里德距离最小化来进行梯度下降训练，以适应随机数据。 注意 您可以在本页浏览各个示例。 张量 预热：NumPy 在介绍 PyTorch 之前，我们将首先使用 numpy 实现网络。 Numpy 提供了一个 n 维数组对象，以及许多用于操纵这些数组的函数。 Numpy 是用于科学计算的通用框架。 它对计算图，深度学习或梯度一无所知。 但是，通过使用 numpy 操作手动实现网络的前向和后向传递，我们可以轻松地使用 numpy 使三阶多项式适合正弦函数： # -*- coding: utf-8 -*- import numpy as np import math # Create random input and output data x = np.linspace(-math.pi, math.pi, 2000) y = np.sin(x) # Randomly initialize weights a = np.random.randn() b = np.random.randn() c = np.random.randn() d = np.random.randn() learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y # y = a + b x + c x^2 + d x^3 y_pred = a + b * x + c * x ** 2 + d * x ** 3 # Compute and print loss loss = np.square(y_pred - y).sum() if t % 100 == 99: print(t, loss) # Backprop to compute gradients of a, b, c, d with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_a = grad_y_pred.sum() grad_b = (grad_y_pred * x).sum() grad_c = (grad_y_pred * x ** 2).sum() grad_d = (grad_y_pred * x ** 3).sum() # Update weights a -= learning_rate * grad_a b -= learning_rate * grad_b c -= learning_rate * grad_c d -= learning_rate * grad_d print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3') PyTorch：张量 Numpy 是一个很棒的框架，但是它不能利用 GPU 来加速其数值计算。 对于现代深度神经网络，GPU 通常会提供 50 倍或更高的加速，因此遗憾的是，numpy 不足以实现现代深度学习。 在这里，我们介绍最基本的 PyTorch 概念：张量。 PyTorch 张量在概念上与 numpy 数组相同：张量是 n 维数组，PyTorch 提供了许多在这些张量上进行操作的函数。 在幕后，张量可以跟踪计算图和梯度，但它们也可用作科学计算的通用工具。 与 numpy 不同，PyTorch 张量可以利用 GPU 加速其数字计算。 要在 GPU 上运行 PyTorch 张量，您只需要指定正确的设备即可。 在这里，我们使用 PyTorch 张量将三阶多项式拟合为正弦函数。 像上面的 numpy 示例一样，我们需要手动实现通过网络的正向和反向传递： # -*- coding: utf-8 -*- import torch import math dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Create random input and output data x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # Randomly initialize weights a = torch.randn((), device=device, dtype=dtype) b = torch.randn((), device=device, dtype=dtype) c = torch.randn((), device=device, dtype=dtype) d = torch.randn((), device=device, dtype=dtype) learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y y_pred = a + b * x + c * x ** 2 + d * x ** 3 # Compute and print loss loss = (y_pred - y).pow(2).sum().item() if t % 100 == 99: print(t, loss) # Backprop to compute gradients of a, b, c, d with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_a = grad_y_pred.sum() grad_b = (grad_y_pred * x).sum() grad_c = (grad_y_pred * x ** 2).sum() grad_d = (grad_y_pred * x ** 3).sum() # Update weights using gradient descent a -= learning_rate * grad_a b -= learning_rate * grad_b c -= learning_rate * grad_c d -= learning_rate * grad_d print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3') Autograd PyTorch：张量和 Autograd 在上述示例中，我们必须手动实现神经网络的前向和后向传递。 对于小型的两层网络，手动实现反向传递并不是什么大问题，但是对于大型的复杂网络来说，可以很快变得非常麻烦。 幸运的是，我们可以使用自动微分来自动计算神经网络中的反向传递。 PyTorch 中的 Autograd 包正是提供了此功能。 使用 Autograd 时，网络的正向传播将定义计算图； 图中的节点为张量，边为从输入张量产生输出张量的函数。 然后通过该图进行反向传播，可以轻松计算梯度。 这听起来很复杂，在实践中非常简单。 每个张量代表计算图中的一个节点。 如果x是具有x.requires_grad=True的张量，则x.grad是另一个张量，其保持x相对于某个标量值的梯度。 在这里，我们使用 PyTorch 张量和 Autograd 来实现我们的正弦波与三阶多项式示例； 现在我们不再需要通过网络手动实现反向传递： # -*- coding: utf-8 -*- import torch import math dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Create Tensors to hold input and outputs. # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # Create random Tensors for weights. For a third order polynomial, we need # 4 weights: y = a + b x + c x^2 + d x^3 # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. a = torch.randn((), device=device, dtype=dtype, requires_grad=True) b = torch.randn((), device=device, dtype=dtype, requires_grad=True) c = torch.randn((), device=device, dtype=dtype, requires_grad=True) d = torch.randn((), device=device, dtype=dtype, requires_grad=True) learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y using operations on Tensors. y_pred = a + b * x + c * x ** 2 + d * x ** 3 # Compute and print loss using operations on Tensors. # Now loss is a Tensor of shape (1,) # loss.item() gets the scalar value held in the loss. loss = (y_pred - y).pow(2).sum() if t % 100 == 99: print(t, loss.item()) # Use autograd to compute the backward pass. This call will compute the # gradient of loss with respect to all Tensors with requires_grad=True. # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding # the gradient of the loss with respect to a, b, c, d respectively. loss.backward() # Manually update weights using gradient descent. Wrap in torch.no_grad() # because weights have requires_grad=True, but we don't need to track this # in autograd. with torch.no_grad(): a -= learning_rate * a.grad b -= learning_rate * b.grad c -= learning_rate * c.grad d -= learning_rate * d.grad # Manually zero the gradients after updating weights a.grad = None b.grad = None c.grad = None d.grad = None print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3') PyTorch：定义新的 Autograd 函数 在幕后，每个原始的 Autograd 运算符实际上都是在张量上运行的两个函数。 正向函数从输入张量计算输出张量。 反向函数接收相对于某个标量值的输出张量的梯度，并计算相对于相同标量值的输入张量的梯度。 在 PyTorch 中，我们可以通过定义torch.autograd.Function的子类并实现forward和backward函数来轻松定义自己的 Autograd 运算符。 然后，我们可以通过构造实例并像调用函数一样调用新的 Autograd 运算符，并传递包含输入数据的张量。 在此示例中，我们将模型定义为y = a + b P[3](c + dx)而不是y = a + bx + cx ^ 2 + dx ^ 3，其中P[3](x) = 1/2 (5x ^ 3 - 3x)是三次的勒让德多项式。 我们编写了自己的自定义 Autograd 函数来计算P[3]的前进和后退，并使用它来实现我们的模型： # -*- coding: utf-8 -*- import torch import math class LegendrePolynomial3(torch.autograd.Function): \"\"\" We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. \"\"\" @staticmethod def forward(ctx, input): \"\"\" In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. \"\"\" ctx.save_for_backward(input) return 0.5 * (5 * input ** 3 - 3 * input) @staticmethod def backward(ctx, grad_output): \"\"\" In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. \"\"\" input, = ctx.saved_tensors return grad_output * 1.5 * (5 * input ** 2 - 1) dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Create Tensors to hold input and outputs. # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # Create random Tensors for weights. For this example, we need # 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized # not too far from the correct result to ensure convergence. # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True) b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True) c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True) d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True) learning_rate = 5e-6 for t in range(2000): # To apply our Function, we use Function.apply method. We alias this as 'P3'. P3 = LegendrePolynomial3.apply # Forward pass: compute predicted y using operations; we compute # P3 using our custom autograd operation. y_pred = a + b * P3(c + d * x) # Compute and print loss loss = (y_pred - y).pow(2).sum() if t % 100 == 99: print(t, loss.item()) # Use autograd to compute the backward pass. loss.backward() # Update weights using gradient descent with torch.no_grad(): a -= learning_rate * a.grad b -= learning_rate * b.grad c -= learning_rate * c.grad d -= learning_rate * d.grad # Manually zero the gradients after updating weights a.grad = None b.grad = None c.grad = None d.grad = None print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)') nn模块 PyTorch：nn 计算图和 Autograd 是定义复杂运算符并自动采用导数的非常强大的范例。 但是对于大型神经网络，原始的 Autograd 可能会太低级。 在构建神经网络时，我们经常想到将计算安排在层中，其中某些层具有可学习的参数，这些参数会在学习期间进行优化。 在 TensorFlow 中，像 Keras ， TensorFlow-Slim 和 TFLearn 之类的包在原始计算图上提供了更高层次的抽象，可用于构建神经网络。 在 PyTorch 中，nn包也达到了相同的目的。 nn包定义了一组模块，它们大致等效于神经网络层。 模块接收输入张量并计算输出张量，但也可以保持内部状态，例如包含可学习参数的张量。 nn包还定义了一组有用的损失函数，这些函数通常在训练神经网络时使用。 在此示例中，我们使用nn包来实现我们的多项式模型网络： # -*- coding: utf-8 -*- import torch import math # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # tensor (x, x^2, x^3). p = torch.tensor([1, 2, 3]) xx = x.unsqueeze(-1).pow(p) # In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape # (3,), for this case, broadcasting semantics will apply to obtain a tensor # of shape (2000, 3) # Use the nn package to define our model as a sequence of layers. nn.Sequential # is a Module which contains other Modules, and applies them in sequence to # produce its output. The Linear Module computes output from input using a # linear function, and holds internal Tensors for its weight and bias. # The Flatten layer flatens the output of the linear layer to a 1D tensor, # to match the shape of `y`. model = torch.nn.Sequential( torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) # The nn package also contains definitions of popular loss functions; in this # case we will use Mean Squared Error (MSE) as our loss function. loss_fn = torch.nn.MSELoss(reduction='sum') learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Tensor of input data to the Module and it produces # a Tensor of output data. y_pred = model(xx) # Compute and print loss. We pass Tensors containing the predicted and true # values of y, and the loss function returns a Tensor containing the # loss. loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # Zero the gradients before running the backward pass. model.zero_grad() # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each Module are stored # in Tensors with requires_grad=True, so this call will compute gradients for # all learnable parameters in the model. loss.backward() # Update the weights using gradient descent. Each parameter is a Tensor, so # we can access its gradients like we did before. with torch.no_grad(): for param in model.parameters(): param -= learning_rate * param.grad # You can access the first layer of `model` like accessing the first item of a list linear_layer = model[0] # For linear layer, its parameters are stored as `weight` and `bias`. print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3') PyTorch：optim 到目前为止，我们已经通过使用torch.no_grad()手动更改持有可学习参数的张量来更新模型的权重。 对于像随机梯度下降这样的简单优化算法来说，这并不是一个巨大的负担，但是在实践中，我们经常使用更复杂的优化器（例如 AdaGrad，RMSProp，Adam 等）来训练神经网络。 PyTorch 中的optim包抽象了优化算法的思想，并提供了常用优化算法的实现。 在此示例中，我们将使用nn包像以前一样定义我们的模型，但是我们将使用optim包提供的 RMSprop 算法来优化模型： # -*- coding: utf-8 -*- import torch import math # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # Prepare the input tensor (x, x^2, x^3). p = torch.tensor([1, 2, 3]) xx = x.unsqueeze(-1).pow(p) # Use the nn package to define our model and loss function. model = torch.nn.Sequential( torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) loss_fn = torch.nn.MSELoss(reduction='sum') # Use the optim package to define an Optimizer that will update the weights of # the model for us. Here we will use RMSprop; the optim package contains many other # optimization algorithms. The first argument to the RMSprop constructor tells the # optimizer which Tensors it should update. learning_rate = 1e-3 optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate) for t in range(2000): # Forward pass: compute predicted y by passing x to the model. y_pred = model(xx) # Compute and print loss. loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # Before the backward pass, use the optimizer object to zero all of the # gradients for the variables it will update (which are the learnable # weights of the model). This is because by default, gradients are # accumulated in buffers( i.e, not overwritten) whenever .backward() # is called. Checkout docs of torch.autograd.backward for more details. optimizer.zero_grad() # Backward pass: compute gradient of the loss with respect to model # parameters loss.backward() # Calling the step function on an Optimizer makes an update to its # parameters optimizer.step() linear_layer = model[0] print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3') PyTorch：自定义nn模块 有时，您将需要指定比一系列现有模块更复杂的模型。 对于这些情况，您可以通过子类化nn.Module并定义一个forward来定义自己的模块，该模块使用其他模块或在 Tensors 上的其他自动转换操作来接收输入 Tensors 并生成输出 Tensors。 在此示例中，我们将三阶多项式实现为自定义Module子类： # -*- coding: utf-8 -*- import torch import math class Polynomial3(torch.nn.Module): def __init__(self): \"\"\" In the constructor we instantiate four parameters and assign them as member parameters. \"\"\" super().__init__() self.a = torch.nn.Parameter(torch.randn(())) self.b = torch.nn.Parameter(torch.randn(())) self.c = torch.nn.Parameter(torch.randn(())) self.d = torch.nn.Parameter(torch.randn(())) def forward(self, x): \"\"\" In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. \"\"\" return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3 def string(self): \"\"\" Just like any class in Python, you can also define custom method on PyTorch modules \"\"\" return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3' # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # Construct our model by instantiating the class defined above model = Polynomial3() # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters of the nn.Linear # module which is members of the model. criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-6) for t in range(2000): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x) # Compute and print loss loss = criterion(y_pred, y) if t % 100 == 99: print(t, loss.item()) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() print(f'Result: {model.string()}') PyTorch：控制流 + 权重共享 作为动态图和权重共享的示例，我们实现了一个非常奇怪的模型：一个三阶多项式，在每个正向传播中选择 3 到 5 之间的一个随机数，并使用该阶数，多次使用相同的权重重复计算四和五阶。 对于此模型，我们可以使用常规的 Python 流控制来实现循环，并且可以通过在定义正向传播时简单地多次重复使用相同的参数来实现权重共享。 我们可以轻松地将此模型实现为Module子类： # -*- coding: utf-8 -*- import random import torch import math class DynamicNet(torch.nn.Module): def __init__(self): \"\"\" In the constructor we instantiate five parameters and assign them as members. \"\"\" super().__init__() self.a = torch.nn.Parameter(torch.randn(())) self.b = torch.nn.Parameter(torch.randn(())) self.c = torch.nn.Parameter(torch.randn(())) self.d = torch.nn.Parameter(torch.randn(())) self.e = torch.nn.Parameter(torch.randn(())) def forward(self, x): \"\"\" For the forward pass of the model, we randomly choose either 4, 5 and reuse the e parameter to compute the contribution of these orders. Since each forward pass builds a dynamic computation graph, we can use normal Python control-flow operators like loops or conditional statements when defining the forward pass of the model. Here we also see that it is perfectly safe to reuse the same parameter many times when defining a computational graph. \"\"\" y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3 for exp in range(4, random.randint(4, 6)): y = y + self.e * x ** exp return y def string(self): \"\"\" Just like any class in Python, you can also define custom method on PyTorch modules \"\"\" return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?' # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # Construct our model by instantiating the class defined above model = DynamicNet() # Construct our loss function and an Optimizer. Training this strange model with # vanilla stochastic gradient descent is tough, so we use momentum criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9) for t in range(30000): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x) # Compute and print loss loss = criterion(y_pred, y) if t % 2000 == 1999: print(t, loss.item()) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() print(f'Result: {model.string()}') 示例 您可以在此处浏览以上示例。 张量 热身：NumPy PyTorch：张量 Autograd PyTorch：张量和 Autograd PyTorch：定义新的 Autograd 函数 nn模块 PyTorch：nn PyTorch：optim PyTorch：自定义nn模块 PyTorch：控制流 + 权重共享 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"08.html":{"url":"08.html","title":"热身：NumPy","keywords":"","body":"热身：NumPy 原文：https://pytorch.org/tutorials/beginner/examples_tensor/polynomial_numpy.html#sphx-glr-beginner-examples-tensor-polynomial-numpy-py 经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测y = sin(x)从-pi到pi。 此实现使用 numpy 手动计算正向传播，损失和后向通过。 numpy 数组是通用的 n 维数组； 它对深度学习，梯度或计算图一无所知，而只是执行通用数值计算的一种方法。 import numpy as np import math # Create random input and output data x = np.linspace(-math.pi, math.pi, 2000) y = np.sin(x) # Randomly initialize weights a = np.random.randn() b = np.random.randn() c = np.random.randn() d = np.random.randn() learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y # y = a + b x + c x^2 + d x^3 y_pred = a + b * x + c * x ** 2 + d * x ** 3 # Compute and print loss loss = np.square(y_pred - y).sum() if t % 100 == 99: print(t, loss) # Backprop to compute gradients of a, b, c, d with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_a = grad_y_pred.sum() grad_b = (grad_y_pred * x).sum() grad_c = (grad_y_pred * x ** 2).sum() grad_d = (grad_y_pred * x ** 3).sum() # Update weights a -= learning_rate * grad_a b -= learning_rate * grad_b c -= learning_rate * grad_c d -= learning_rate * grad_d print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：polynomial_numpy.py 下载 Jupyter 笔记本：polynomial_numpy.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"09.html":{"url":"09.html","title":"PyTorch：张量","keywords":"","body":"PyTorch：张量 原文：https://pytorch.org/tutorials/beginner/examples_tensor/polynomial_tensor.html#sphx-glr-beginner-examples-tensor-polynomial-tensor-py 经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测y = sin(x)从-pi到pi。 此实现使用 PyTorch 张量手动计算正向传播，损失和后向通过。 PyTorch 张量基本上与 numpy 数组相同：它对深度学习或计算图或梯度一无所知，只是用于任意数值计算的通用 n 维数组。 numpy 数组和 PyTorch 张量之间的最大区别是 PyTorch 张量可以在 CPU 或 GPU 上运行。 要在 GPU 上运行操作，只需将张量转换为 cuda 数据类型。 import torch import math dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Create random input and output data x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # Randomly initialize weights a = torch.randn((), device=device, dtype=dtype) b = torch.randn((), device=device, dtype=dtype) c = torch.randn((), device=device, dtype=dtype) d = torch.randn((), device=device, dtype=dtype) learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y y_pred = a + b * x + c * x ** 2 + d * x ** 3 # Compute and print loss loss = (y_pred - y).pow(2).sum().item() if t % 100 == 99: print(t, loss) # Backprop to compute gradients of a, b, c, d with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_a = grad_y_pred.sum() grad_b = (grad_y_pred * x).sum() grad_c = (grad_y_pred * x ** 2).sum() grad_d = (grad_y_pred * x ** 3).sum() # Update weights using gradient descent a -= learning_rate * grad_a b -= learning_rate * grad_b c -= learning_rate * grad_c d -= learning_rate * grad_d print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：polynomial_tensor.py 下载 Jupyter 笔记本：polynomial_tensor.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"10.html":{"url":"10.html","title":"PyTorch：张量和 Autograd","keywords":"","body":"PyTorch：张量和 Autograd 原文：https://pytorch.org/tutorials/beginner/examples_autograd/polynomial_autograd.html#sphx-glr-beginner-examples-autograd-polynomial-autograd-py 经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测y = sin(x)从-pi到pi。 此实现使用 PyTorch 张量上的运算来计算正向传播，并使用 PyTorch Autograd 来计算梯度。 PyTorch 张量表示计算图中的一个节点。 如果x是具有x.requires_grad=True的张量，则x.grad是另一个张量，其保持x相对于某个标量值的梯度。 import torch import math dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Create Tensors to hold input and outputs. # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # Create random Tensors for weights. For a third order polynomial, we need # 4 weights: y = a + b x + c x^2 + d x^3 # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. a = torch.randn((), device=device, dtype=dtype, requires_grad=True) b = torch.randn((), device=device, dtype=dtype, requires_grad=True) c = torch.randn((), device=device, dtype=dtype, requires_grad=True) d = torch.randn((), device=device, dtype=dtype, requires_grad=True) learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y using operations on Tensors. y_pred = a + b * x + c * x ** 2 + d * x ** 3 # Compute and print loss using operations on Tensors. # Now loss is a Tensor of shape (1,) # loss.item() gets the scalar value held in the loss. loss = (y_pred - y).pow(2).sum() if t % 100 == 99: print(t, loss.item()) # Use autograd to compute the backward pass. This call will compute the # gradient of loss with respect to all Tensors with requires_grad=True. # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding # the gradient of the loss with respect to a, b, c, d respectively. loss.backward() # Manually update weights using gradient descent. Wrap in torch.no_grad() # because weights have requires_grad=True, but we don't need to track this # in autograd. with torch.no_grad(): a -= learning_rate * a.grad b -= learning_rate * b.grad c -= learning_rate * c.grad d -= learning_rate * d.grad # Manually zero the gradients after updating weights a.grad = None b.grad = None c.grad = None d.grad = None print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：polynomial_autograd.py 下载 Jupyter 笔记本：polynomial_autograd.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"11.html":{"url":"11.html","title":"PyTorch：定义新的 Autograd 函数","keywords":"","body":"PyTorch：定义新的 Autograd 函数 原文：https://pytorch.org/tutorials/beginner/examples_autograd/polynomial_custom_function.html#sphx-glr-beginner-examples-autograd-polynomial-custom-function-py 经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测y = sin(x)从-pi到pi。 而不是将多项式写为y = a + bx + cx ^ 2 + dx ^ 3，我们将多项式写为y = a + b P[3](c + dx)其中P[3](x) = 1/2 (5x ^ 3 - 3x)是三次的勒让德多项式。 此实现使用 PyTorch 张量上的运算来计算正向传播，并使用 PyTorch Autograd 来计算梯度。 在此实现中，我们实现了自己的自定义 Autograd 函数来执行P'[3](x)。 通过数学，P'[3](x) = 3/2 (5x ^ 2 - 1)： import torch import math class LegendrePolynomial3(torch.autograd.Function): \"\"\" We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. \"\"\" @staticmethod def forward(ctx, input): \"\"\" In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. \"\"\" ctx.save_for_backward(input) return 0.5 * (5 * input ** 3 - 3 * input) @staticmethod def backward(ctx, grad_output): \"\"\" In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. \"\"\" input, = ctx.saved_tensors return grad_output * 1.5 * (5 * input ** 2 - 1) dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Create Tensors to hold input and outputs. # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # Create random Tensors for weights. For this example, we need # 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized # not too far from the correct result to ensure convergence. # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True) b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True) c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True) d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True) learning_rate = 5e-6 for t in range(2000): # To apply our Function, we use Function.apply method. We alias this as 'P3'. P3 = LegendrePolynomial3.apply # Forward pass: compute predicted y using operations; we compute # P3 using our custom autograd operation. y_pred = a + b * P3(c + d * x) # Compute and print loss loss = (y_pred - y).pow(2).sum() if t % 100 == 99: print(t, loss.item()) # Use autograd to compute the backward pass. loss.backward() # Update weights using gradient descent with torch.no_grad(): a -= learning_rate * a.grad b -= learning_rate * b.grad c -= learning_rate * c.grad d -= learning_rate * d.grad # Manually zero the gradients after updating weights a.grad = None b.grad = None c.grad = None d.grad = None print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：polynomial_custom_function.py 下载 Jupyter 笔记本：polynomial_custom_function.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"12.html":{"url":"12.html","title":"PyTorch：nn","keywords":"","body":"PyTorch：nn 原文：https://pytorch.org/tutorials/beginner/examples_nn/polynomial_nn.html#sphx-glr-beginner-examples-nn-polynomial-nn-py 经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测y = sin(x)从-pi到pi。 此实现使用来自 PyTorch 的nn包来构建网络。 PyTorch Autograd 使定义计算图和获取梯度变得容易，但是原始的 Autograd 对于定义复杂的神经网络来说可能太低了。 这是nn包可以提供帮助的地方。 nn包定义了一组模块，您可以将其视为神经网络层，该神经网络层从输入产生输出并且可能具有一些可训练的权重。 import torch import math # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # tensor (x, x^2, x^3). p = torch.tensor([1, 2, 3]) xx = x.unsqueeze(-1).pow(p) # In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape # (3,), for this case, broadcasting semantics will apply to obtain a tensor # of shape (2000, 3) # Use the nn package to define our model as a sequence of layers. nn.Sequential # is a Module which contains other Modules, and applies them in sequence to # produce its output. The Linear Module computes output from input using a # linear function, and holds internal Tensors for its weight and bias. # The Flatten layer flatens the output of the linear layer to a 1D tensor, # to match the shape of `y`. model = torch.nn.Sequential( torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) # The nn package also contains definitions of popular loss functions; in this # case we will use Mean Squared Error (MSE) as our loss function. loss_fn = torch.nn.MSELoss(reduction='sum') learning_rate = 1e-6 for t in range(2000): # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Tensor of input data to the Module and it produces # a Tensor of output data. y_pred = model(xx) # Compute and print loss. We pass Tensors containing the predicted and true # values of y, and the loss function returns a Tensor containing the # loss. loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # Zero the gradients before running the backward pass. model.zero_grad() # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each Module are stored # in Tensors with requires_grad=True, so this call will compute gradients for # all learnable parameters in the model. loss.backward() # Update the weights using gradient descent. Each parameter is a Tensor, so # we can access its gradients like we did before. with torch.no_grad(): for param in model.parameters(): param -= learning_rate * param.grad # You can access the first layer of `model` like accessing the first item of a list linear_layer = model[0] # For linear layer, its parameters are stored as `weight` and `bias`. print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：polynomial_nn.py 下载 Jupyter 笔记本：polynomial_nn.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"13.html":{"url":"13.html","title":"PyTorch：optim","keywords":"","body":"PyTorch：optim 原文：https://pytorch.org/tutorials/beginner/examples_nn/polynomial_optim.html#sphx-glr-beginner-examples-nn-polynomial-optim-py 经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测y = sin(x)从-pi到pi。 此实现使用来自 PyTorch 的nn包来构建网络。 与其像以前那样手动更新模型的权重，不如使用optim包定义一个优化器，该优化器将为我们更新权重。 optim包定义了许多深度学习常用的优化算法，包括 SGD + 动量，RMSProp，Adam 等。 import torch import math # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # Prepare the input tensor (x, x^2, x^3). p = torch.tensor([1, 2, 3]) xx = x.unsqueeze(-1).pow(p) # Use the nn package to define our model and loss function. model = torch.nn.Sequential( torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) loss_fn = torch.nn.MSELoss(reduction='sum') # Use the optim package to define an Optimizer that will update the weights of # the model for us. Here we will use RMSprop; the optim package contains many other # optimization algorithms. The first argument to the RMSprop constructor tells the # optimizer which Tensors it should update. learning_rate = 1e-3 optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate) for t in range(2000): # Forward pass: compute predicted y by passing x to the model. y_pred = model(xx) # Compute and print loss. loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # Before the backward pass, use the optimizer object to zero all of the # gradients for the variables it will update (which are the learnable # weights of the model). This is because by default, gradients are # accumulated in buffers( i.e, not overwritten) whenever .backward() # is called. Checkout docs of torch.autograd.backward for more details. optimizer.zero_grad() # Backward pass: compute gradient of the loss with respect to model # parameters loss.backward() # Calling the step function on an Optimizer makes an update to its # parameters optimizer.step() linear_layer = model[0] print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：polynomial_optim.py 下载 Jupyter 笔记本：polynomial_optim.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"14.html":{"url":"14.html","title":"PyTorch：自定义nn模块","keywords":"","body":"PyTorch：自定义nn模块 原文：https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html#sphx-glr-beginner-examples-nn-polynomial-module-py 经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测y = sin(x)从-pi到pi。 此实现将模型定义为自定义Module子类。 每当您想要一个比现有模块的简单序列更复杂的模型时，都需要以这种方式定义模型。 import torch import math class Polynomial3(torch.nn.Module): def __init__(self): \"\"\" In the constructor we instantiate four parameters and assign them as member parameters. \"\"\" super().__init__() self.a = torch.nn.Parameter(torch.randn(())) self.b = torch.nn.Parameter(torch.randn(())) self.c = torch.nn.Parameter(torch.randn(())) self.d = torch.nn.Parameter(torch.randn(())) def forward(self, x): \"\"\" In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. \"\"\" return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3 def string(self): \"\"\" Just like any class in Python, you can also define custom method on PyTorch modules \"\"\" return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3' # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # Construct our model by instantiating the class defined above model = Polynomial3() # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters of the nn.Linear # module which is members of the model. criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-6) for t in range(2000): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x) # Compute and print loss loss = criterion(y_pred, y) if t % 100 == 99: print(t, loss.item()) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() print(f'Result: {model.string()}') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：polynomial_module.py 下载 Jupyter 笔记本：polynomial_module.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"15.html":{"url":"15.html","title":"PyTorch：控制流 + 权重共享","keywords":"","body":"PyTorch：控制流 + 权重共享 原文：https://pytorch.org/tutorials/beginner/examples_nn/dynamic_net.html#sphx-glr-beginner-examples-nn-dynamic-net-py 为了展示 PyTorch 动态图的强大功能，我们将实现一个非常奇怪的模型：一个三阶多项式，在每个正向传播中选择 3 到 5 之间的一个随机数，并使用该数量的阶次，多次使用相同的权重重复计算四和五阶。 import random import torch import math class DynamicNet(torch.nn.Module): def __init__(self): \"\"\" In the constructor we instantiate five parameters and assign them as members. \"\"\" super().__init__() self.a = torch.nn.Parameter(torch.randn(())) self.b = torch.nn.Parameter(torch.randn(())) self.c = torch.nn.Parameter(torch.randn(())) self.d = torch.nn.Parameter(torch.randn(())) self.e = torch.nn.Parameter(torch.randn(())) def forward(self, x): \"\"\" For the forward pass of the model, we randomly choose either 4, 5 and reuse the e parameter to compute the contribution of these orders. Since each forward pass builds a dynamic computation graph, we can use normal Python control-flow operators like loops or conditional statements when defining the forward pass of the model. Here we also see that it is perfectly safe to reuse the same parameter many times when defining a computational graph. \"\"\" y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3 for exp in range(4, random.randint(4, 6)): y = y + self.e * x ** exp return y def string(self): \"\"\" Just like any class in Python, you can also define custom method on PyTorch modules \"\"\" return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?' # Create Tensors to hold input and outputs. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # Construct our model by instantiating the class defined above model = DynamicNet() # Construct our loss function and an Optimizer. Training this strange model with # vanilla stochastic gradient descent is tough, so we use momentum criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9) for t in range(30000): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x) # Compute and print loss loss = criterion(y_pred, y) if t % 2000 == 1999: print(t, loss.item()) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() print(f'Result: {model.string()}') 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：dynamic_net.py 下载 Jupyter 笔记本：dynamic_net.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"16.html":{"url":"16.html","title":"torch.nn到底是什么？","keywords":"","body":"torch.nn到底是什么？ 原文：https://pytorch.org/tutorials/beginner/nn_tutorial.html 作者：Jeremy Howard，fast.ai。 感谢 Rachel Thomas 和 Francisco Ingham。 我们建议将本教程作为笔记本而不是脚本来运行。 要下载笔记本（.ipynb）文件，请单击页面顶部的链接。 PyTorch 提供设计精美的模块和类torch.nn，torch.optim，Dataset和DataLoader神经网络。 为了充分利用它们的功能并针对您的问题对其进行自定义，您需要真正了解它们在做什么。 为了建立这种理解，我们将首先在 MNIST 数据集上训练基本神经网络，而无需使用这些模型的任何功能。 我们最初将仅使用最基本的 PyTorch 张量函数。 然后，我们将一次从torch.nn，torch.optim，Dataset或DataLoader中逐个添加一个函数，以准确显示每个函数，以及如何使代码更简洁或更有效。 灵活。 本教程假定您已经安装了 PyTorch，并且熟悉张量操作的基础知识。 （如果您熟悉 Numpy 数组操作，将会发现此处使用的 PyTorch 张量操作几乎相同）。 MNIST 数据集 我们将使用经典的 MNIST 数据集，该数据集由手绘数字的黑白图像组成（0 到 9 之间）。 我们将使用pathlib处理路径（Python 3 标准库的一部分），并使用requests下载数据集。 我们只会在使用模块时才导入它们，因此您可以确切地看到每个位置上正在使用的模块。 from pathlib import Path import requests DATA_PATH = Path(\"data\") PATH = DATA_PATH / \"mnist\" PATH.mkdir(parents=True, exist_ok=True) URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\" FILENAME = \"mnist.pkl.gz\" if not (PATH / FILENAME).exists(): content = requests.get(URL + FILENAME).content (PATH / FILENAME).open(\"wb\").write(content) 该数据集为 numpy 数组格式，并已使用pickle（一种用于序列化数据的 python 特定格式）存储。 import pickle import gzip with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\") 每个图像为28 x 28，并存储为长度为784 = 28x28的扁平行。 让我们来看一个； 我们需要先将其重塑为 2d。 from matplotlib import pyplot import numpy as np pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\") print(x_train.shape) 出： (50000, 784) PyTorch 使用torch.tensor而不是 numpy 数组，因此我们需要转换数据。 import torch x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid) ) n, c = x_train.shape x_train, x_train.shape, y_train.min(), y_train.max() print(x_train, y_train) print(x_train.shape) print(y_train.min(), y_train.max()) 出： tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) tensor([5, 0, 4, ..., 8, 4, 8]) torch.Size([50000, 784]) tensor(0) tensor(9) 从零开始的神经网络（没有torch.nn） 首先，我们仅使用 PyTorch 张量操作创建模型。 我们假设您已经熟悉神经网络的基础知识。 （如果不是，则可以在 course.fast.ai 中学习它们）。 PyTorch 提供了创建随机或零填充张量的方法，我们将使用它们来为简单的线性模型创建权重和偏差。 这些只是常规张量，还有一个非常特殊的附加值：我们告诉 PyTorch 它们需要梯度。 这使 PyTorch 记录了在张量上完成的所有操作，因此它可以在反向传播时自动计算的梯度！ 对于权重，我们在初始化之后设置requires_grad，因为我们不希望该步骤包含在梯度中。 （请注意，PyTorch 中的尾随_表示该操作是原地执行的。） 注意 我们在这里用 Xavier 初始化（通过乘以1 / sqrt(n)）来初始化权重。 import math weights = torch.randn(784, 10) / math.sqrt(784) weights.requires_grad_() bias = torch.zeros(10, requires_grad=True) 由于 PyTorch 具有自动计算梯度的功能，我们可以将任何标准的 Python 函数（或可调用对象）用作模型！ 因此，我们只需编写一个普通矩阵乘法和广播加法即可创建一个简单的线性模型。 我们还需要激活函数，因此我们将编写并使用log_softmax。 请记住：尽管 PyTorch 提供了许多预写的损失函数，激活函数等，但是您可以使用纯 Python 轻松编写自己的函数。 PyTorch 甚至会自动为您的函数创建快速 GPU 或向量化的 CPU 代码。 def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) def model(xb): return log_softmax(xb @ weights + bias) 在上面，@代表点积运算。 我们将对一批数据（在本例中为 64 张图像）调用函数。 这是一个正向传播。 请注意，由于我们从随机权重开始，因此在这一阶段，我们的预测不会比随机预测更好。 bs = 64 # batch size xb = x_train[0:bs] # a mini-batch from x preds = model(xb) # predictions preds[0], preds.shape print(preds[0], preds.shape) 出： tensor([-2.5964, -2.3153, -2.1321, -2.4480, -2.2930, -1.9507, -2.1289, -2.4175, -2.5332, -2.3967], grad_fn=) torch.Size([64, 10]) 如您所见，preds张量不仅包含张量值，还包含梯度函数。 稍后我们将使用它进行反向传播。 让我们实现负对数可能性作为损失函数（同样，我们只能使用标准 Python）： def nll(input, target): return -input[range(target.shape[0]), target].mean() loss_func = nll 让我们使用随机模型来检查损失，以便我们稍后查看反向传播后是否可以改善我们的损失。 yb = y_train[0:bs] print(loss_func(preds, yb)) 出： tensor(2.3735, grad_fn=) 我们还实现一个函数来计算模型的准确率。 对于每个预测，如果具有最大值的索引与目标值匹配，则该预测是正确的。 def accuracy(out, yb): preds = torch.argmax(out, dim=1) return (preds == yb).float().mean() 让我们检查一下随机模型的准确率，以便我们可以看出随着损失的增加，准确率是否有所提高。 print(accuracy(preds, yb)) 出： tensor(0.0938) 现在，我们可以运行一个训练循环。 对于每次迭代，我们将： 选择一个小批量数据（大小为bs） 使用模型进行预测 计算损失 loss.backward()更新模型的梯度，在这种情况下为weights和bias。 现在，我们使用这些梯度来更新权重和偏差。 我们在torch.no_grad()上下文管理器中执行此操作，因为我们不希望在下一步的梯度计算中记录这些操作。 您可以在这里阅读有关 PyTorch 的 Autograd 如何记录操作的更多信息。 然后，将梯度设置为零，以便为下一个循环做好准备。 否则，我们的梯度会记录所有已发生操作的运行记录（即loss.backward()将梯度添加到已存储的内容中，而不是替换它们）。 小费 您可以使用标准的 python 调试器逐步浏览 PyTorch 代码，从而可以在每一步检查各种变量值。 取消注释以下set_trace()即可尝试。 from IPython.core.debugger import set_trace lr = 0.5 # learning rate epochs = 2 # how many epochs to train for for epoch in range(epochs): for i in range((n - 1) // bs + 1): # set_trace() start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() 就是这样：我们完全从头开始创建并训练了一个最小的神经网络（在这种情况下，是逻辑回归，因为我们没有隐藏的层）！ 让我们检查损失和准确率，并将其与我们之前获得的进行比较。 我们希望损失会减少，准确率会增加，而且确实如此。 print(loss_func(model(xb), yb), accuracy(model(xb), yb)) 出： tensor(0.0811, grad_fn=) tensor(1.) 使用torch.nn.functional 现在，我们将重构代码，使其执行与以前相同的操作，只是我们将开始利用 PyTorch 的nn类使其更加简洁和灵活。 从这里开始的每一步，我们都应该使代码中的一个或多个：更短，更易理解和/或更灵活。 第一步也是最简单的步骤，就是用torch.nn.functional（通常按照惯例将其导入到名称空间F中）替换我们的手写激活和损失函数，从而缩短代码长度。 该模块包含torch.nn库中的所有函数（而该库的其他部分包含类）。 除了广泛的损失和激活函数外，您还会在这里找到一些方便的函数来创建神经网络，例如合并函数。 （还有一些用于进行卷积，线性层等的函数，但是正如我们将看到的那样，通常可以使用库的其他部分来更好地处理这些函数。） 如果您使用的是负对数似然损失和对数 softmax 激活，那么 Pytorch 会提供结合了两者的单一函数F.cross_entropy。 因此，我们甚至可以从模型中删除激活函数。 import torch.nn.functional as F loss_func = F.cross_entropy def model(xb): return xb @ weights + bias 请注意，我们不再在model函数中调用log_softmax。 让我们确认我们的损失和准确率与以前相同： print(loss_func(model(xb), yb), accuracy(model(xb), yb)) 出： tensor(0.0811, grad_fn=) tensor(1.) 使用nn.Module重构 接下来，我们将使用nn.Module和nn.Parameter进行更清晰，更简洁的训练循环。 我们将nn.Module子类化（它本身是一个类并且能够跟踪状态）。 在这种情况下，我们要创建一个类，该类包含前进步骤的权重，偏置和方法。 nn.Module具有许多我们将要使用的属性和方法（例如.parameters()和.zero_grad()）。 注意 nn.Module（大写M）是 PyTorch 的特定概念，并且是我们将经常使用的一类。 不要将nn.Module与模块（小写m）的 Python 概念混淆，该模块是可以导入的 Python 代码文件。 from torch import nn class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784)) self.bias = nn.Parameter(torch.zeros(10)) def forward(self, xb): return xb @ self.weights + self.bias 由于我们现在使用的是对象而不是仅使用函数，因此我们首先必须实例化模型： model = Mnist_Logistic() 现在我们可以像以前一样计算损失。 请注意，nn.Module对象的使用就好像它们是函数一样（即，它们是可调用的），但是在后台 Pytorch 会自动调用我们的forward方法。 print(loss_func(model(xb), yb)) 出： tensor(2.3903, grad_fn=) 以前，在我们的训练循环中，我们必须按名称更新每个参数的值，并手动将每个参数的梯度分别归零，如下所示： with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() 现在我们可以利用model.parameters()和model.zero_grad()（它们都由 PyTorch 为nn.Module定义）来使这些步骤更简洁，并且更不会出现忘记某些参数的错误，尤其是当我们有一个更复杂的模型的时候： with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() 我们将把小的训练循环包装在fit函数中，以便稍后再运行。 def fit(): for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() fit() 让我们仔细检查一下我们的损失是否减少了： print(loss_func(model(xb), yb)) 出： tensor(0.0808, grad_fn=) 使用nn.Linear重构 我们继续重构我们的代码。 代替手动定义和初始化self.weights和self.bias并计算xb @ self.weights + self.bias，我们将对线性层使用 Pytorch 类nn.Linear，这将为我们完成所有工作。 Pytorch 具有许多类型的预定义层，可以大大简化我们的代码，并且通常也可以使其速度更快。 class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.lin = nn.Linear(784, 10) def forward(self, xb): return self.lin(xb) 我们以与以前相同的方式实例化模型并计算损失： model = Mnist_Logistic() print(loss_func(model(xb), yb)) 出： tensor(2.4215, grad_fn=) 我们仍然可以使用与以前相同的fit方法。 fit() print(loss_func(model(xb), yb)) 出： tensor(0.0824, grad_fn=) 使用optim重构 Pytorch 还提供了一个包含各种优化算法的包torch.optim。 我们可以使用优化器中的step方法采取向前的步骤，而不是手动更新每个参数。 这将使我们替换之前的手动编码优化步骤： with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() 而是只使用： opt.step() opt.zero_grad() （optim.zero_grad()将梯度重置为 0，我们需要在计算下一个小批量的梯度之前调用它。） from torch import optim 我们将定义一个小函数来创建模型和优化器，以便将来重用。 def get_model(): model = Mnist_Logistic() return model, optim.SGD(model.parameters(), lr=lr) model, opt = get_model() print(loss_func(model(xb), yb)) for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 出： tensor(2.2999, grad_fn=) tensor(0.0823, grad_fn=) 使用Dataset重构 PyTorch 有一个抽象的Dataset类。 数据集可以是具有__len__函数（由 Python 的标准len函数调用）和具有__getitem__函数作为对其进行索引的一种方法。 本教程演示了一个不错的示例，该示例创建一个自定义FacialLandmarkDataset类作为Dataset的子类。 PyTorch 的TensorDataset是一个数据集包装张量。 通过定义索引的长度和方式，这也为我们提供了沿张量的第一维进行迭代，索引和切片的方法。 这将使我们在训练的同一行中更容易访问自变量和因变量。 from torch.utils.data import TensorDataset x_train和y_train都可以合并为一个TensorDataset，这将更易于迭代和切片。 train_ds = TensorDataset(x_train, y_train) 以前，我们不得不分别遍历x和y值的小批量： xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] 现在，我们可以一起执行以下两个步骤： xb,yb = train_ds[i*bs : i*bs+bs] model, opt = get_model() for epoch in range(epochs): for i in range((n - 1) // bs + 1): xb, yb = train_ds[i * bs: i * bs + bs] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 出： tensor(0.0819, grad_fn=) 使用DataLoader重构 Pytorch 的DataLoader负责批量管理。 您可以从任何Dataset创建一个DataLoader。 DataLoader使迭代迭代变得更加容易。 不必使用train_ds[i*bs : i*bs+bs]，DataLoader会自动为我们提供每个小批量。 from torch.utils.data import DataLoader train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs) 以前，我们的循环遍历如下批量(xb, yb)： for i in range((n-1)//bs + 1): xb,yb = train_ds[i*bs : i*bs+bs] pred = model(xb) 现在，我们的循环更加简洁了，因为(xb, yb)是从数据加载器自动加载的： for xb,yb in train_dl: pred = model(xb) model, opt = get_model() for epoch in range(epochs): for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 出： tensor(0.0821, grad_fn=) 得益于 Pytorch 的nn.Module，nn.Parameter，Dataset和DataLoader，我们的训练循环现在变得更小，更容易理解。 现在，让我们尝试添加在实践中创建有效模型所需的基本功能。 添加验证 在第 1 节中，我们只是试图建立一个合理的训练循环以用于我们的训练数据。 实际上，您也应该始终具有验证集，以便识别您是否过拟合。 对训练数据进行打乱对于防止批量与过拟合之间的相关性很重要。 另一方面，无论我们是否打乱验证集，验证损失都是相同的。 由于打乱需要花费更多时间，因此打乱验证数据没有任何意义。 我们将验证集的批量大小设为训练集的两倍。 这是因为验证集不需要反向传播，因此占用的内存更少（不需要存储梯度）。 我们利用这一优势来使用更大的批量，并更快地计算损失。 train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) valid_ds = TensorDataset(x_valid, y_valid) valid_dl = DataLoader(valid_ds, batch_size=bs * 2) 我们将在每个周期结束时计算并打印验证损失。 （请注意，我们总是在训练之前调用model.train()，并在推理之前调用model.eval()，因为诸如nn.BatchNorm2d和nn.Dropout之类的层会使用它们，以确保这些不同阶段的行为正确。） model, opt = get_model() for epoch in range(epochs): model.train() for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() model.eval() with torch.no_grad(): valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) print(epoch, valid_loss / len(valid_dl)) 出： 0 tensor(0.3743) 1 tensor(0.3316) 创建fit()和get_data() 现在，我们将自己进行一些重构。 由于我们经历了两次相似的过程来计算训练集和验证集的损失，因此我们将其设为自己的函数loss_batch，该函数可计算一批损失。 我们将优化器传入训练集中，然后使用它执行反向传播。 对于验证集，我们没有通过优化程序，因此该方法不会执行反向传播。 def loss_batch(model, loss_func, xb, yb, opt=None): loss = loss_func(model(xb), yb) if opt is not None: loss.backward() opt.step() opt.zero_grad() return loss.item(), len(xb) fit运行必要的操作来训练我们的模型，并计算每个周期的训练和验证损失。 import numpy as np def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): model.train() for xb, yb in train_dl: loss_batch(model, loss_func, xb, yb, opt) model.eval() with torch.no_grad(): losses, nums = zip( *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl] ) val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums) print(epoch, val_loss) get_data返回训练和验证集的数据加载器。 def get_data(train_ds, valid_ds, bs): return ( DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(valid_ds, batch_size=bs * 2), ) 现在，我们获取数据加载器和拟合模型的整个过程可以在 3 行代码中运行： train_dl, valid_dl = get_data(train_ds, valid_ds, bs) model, opt = get_model() fit(epochs, model, loss_func, opt, train_dl, valid_dl) 出： 0 0.3120644524335861 1 0.28915613491535186 您可以使用这些基本的 3 行代码来训练各种各样的模型。 让我们看看是否可以使用它们来训练卷积神经网络（CNN）！ 切换到 CNN 现在，我们将构建具有三个卷积层的神经网络。 由于上一节中的任何功能都不假设任何有关模型形式的信息，因此我们将能够使用它们来训练 CNN，而无需进行任何修改。 我们将使用 Pytorch 的预定义Conv2d类作为我们的卷积层。 我们定义了具有 3 个卷积层的 CNN。 每个卷积后跟一个 ReLU。 最后，我们执行平均池化。 （请注意，view是 numpy 的reshape的 PyTorch 版本） class Mnist_CNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1) self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1) self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1) def forward(self, xb): xb = xb.view(-1, 1, 28, 28) xb = F.relu(self.conv1(xb)) xb = F.relu(self.conv2(xb)) xb = F.relu(self.conv3(xb)) xb = F.avg_pool2d(xb, 4) return xb.view(-1, xb.size(1)) lr = 0.1 动量是随机梯度下降的一种变体，它也考虑了以前的更新，通常可以加快训练速度。 model = Mnist_CNN() opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 出： 0 0.32337012240886687 1 0.25021172934770586 nn.Sequential torch.nn还有另一个方便的类，可以用来简化我们的代码：Sequential。 Sequential对象以顺序方式运行其中包含的每个模块。 这是编写神经网络的一种简单方法。 为了利用这一点，我们需要能够从给定的函数轻松定义自定义层。 例如，PyTorch 没有视层，我们需要为我们的网络创建一个层。 Lambda将创建一个层，然后在使用Sequential定义网络时可以使用该层。 class Lambda(nn.Module): def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) def preprocess(x): return x.view(-1, 1, 28, 28) 用Sequential创建的模型很简单： model = nn.Sequential( Lambda(preprocess), nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AvgPool2d(4), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 出： 0 0.30119081069231035 1 0.25335356528759 包装DataLoader Our CNN is fairly concise, but it only works with MNIST, because: 假设输入为28 * 28长向量 假设 CNN 的最终网格尺寸为4 * 4（因为这是平均值 我们使用的合并核大小） 让我们摆脱这两个假设，因此我们的模型适用于任何 2d 单通道图像。 首先，我们可以删除初始的 Lambda 层，但将数据预处理移至生成器中： def preprocess(x, y): return x.view(-1, 1, 28, 28), y class WrappedDataLoader: def __init__(self, dl, func): self.dl = dl self.func = func def __len__(self): return len(self.dl) def __iter__(self): batches = iter(self.dl) for b in batches: yield (self.func(*b)) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) 接下来，我们可以将nn.AvgPool2d替换为nn.AdaptiveAvgPool2d，这使我们能够定义所需的输出张量的大小，而不是所需的输入张量的大小。 结果，我们的模型将适用于任何大小的输入。 model = nn.Sequential( nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) 试试看： fit(epochs, model, loss_func, opt, train_dl, valid_dl) 出： 0 0.327303307390213 1 0.2181092014491558 使用您的 GPU 如果您足够幸运地能够使用具有 CUDA 功能的 GPU（可以从大多数云提供商处以每小时 0.50 美元的价格租用一个），则可以使用它来加速代码。 首先检查您的 GPU 是否在 Pytorch 中正常工作： print(torch.cuda.is_available()) 出： True 然后为其创建一个设备对象： dev = torch.device( \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") 让我们更新preprocess，将批量移至 GPU： def preprocess(x, y): return x.view(-1, 1, 28, 28).to(dev), y.to(dev) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) 最后，我们可以将模型移至 GPU。 model.to(dev) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) 您应该发现它现在运行得更快： fit(epochs, model, loss_func, opt, train_dl, valid_dl) 出： 0 0.1833980613708496 1 0.17365939717292786 总结 现在，我们有了一个通用的数据管道和训练循环，您可以将其用于使用 Pytorch 训练许多类型的模型。 要了解现在可以轻松进行模型训练，请查看mnist_sample示例笔记本。 当然，您需要添加很多内容，例如数据扩充，超参数调整，监控训练，迁移学习等。 这些功能可在 fastai 库中使用，该库是使用本教程中所示的相同设计方法开发的，为希望进一步推广其模型的从业人员提供了自然的下一步。 我们承诺在本教程开始时将通过示例分别说明torch.nn，torch.optim，Dataset和DataLoader。 因此，让我们总结一下我们所看到的： torch.nn Module：创建一个行为类似于函数的可调用对象，但也可以包含状态（例如神经网络层权重）。 它知道其中包含的 Parameter ，并且可以将其所有坡度归零，遍历它们以进行权重更新等。 Parameter：张量的包装器，用于告知 Module 具有在反向传播期间需要更新的权重。 仅更新具有require_grad属性集的张量 functional：一个模块（通常按照惯例导入到 F 名称空间中），其中包含激活函数，损失函数等。 以及卷积和线性层等层的无状态版本。 torch.optim：包含诸如 SGD 的优化程序，这些优化程序在后退步骤 Dataset 中更新 Parameter 的权重。 具有 __len__ 和 __getitem__ 的对象，包括 Pytorch 提供的类，例如 TensorDataset DataLoader：获取任何 Dataset 并创建一个迭代器，该迭代器返回批量数据。 脚本的总运行时间：（0 分钟 57.062 秒） 下载 Python 源码：nn_tutorial.py 下载 Jupyter 笔记本：nn_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"17.html":{"url":"17.html","title":"使用 TensorBoard 可视化模型，数据和训练","keywords":"","body":"使用 TensorBoard 可视化模型，数据和训练 原文：https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html 在 60 分钟突击中，我们向您展示了如何加载数据，如何通过定义为nn.Module子类的模型提供数据，如何在训练数据上训练该模型以及在测试数据上对其进行测试。 为了了解发生的情况，我们在模型训练期间打印一些统计数据，以了解训练是否在进行中。 但是，我们可以做得更好：PyTorch 与 TensorBoard 集成在一起，TensorBoard 是一种工具，用于可视化神经网络训练运行的结果。 本教程使用 Fashion-MNIST 数据集说明了其某些功能，可以使用torchvision.datasets将其读入 PyTorch。 在本教程中，我们将学习如何： 读取数据并进行适当的转换（与先前的教程几乎相同）。 设置 TensorBoard。 写入 TensorBoard。 使用 TensorBoard 检查模型架构。 使用 TensorBoard 来创建我们在上一个教程中创建的可视化的交互式版本，并使用较少的代码 具体来说，在第 5 点，我们将看到： 有两种方法可以检查我们的训练数据 在训练模型时如何跟踪其表现 在训练后如何评估模型的表现。 我们将从 CIFAR-10 教程中类似的样板代码开始： # imports import matplotlib.pyplot as plt import numpy as np import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.FashionMNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.FashionMNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) # constant for classes classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot') # helper function to show an image # (used in the `plot_classes_preds` function below) def matplotlib_imshow(img, one_channel=False): if one_channel: img = img.mean(dim=0) img = img / 2 + 0.5 # unnormalize npimg = img.numpy() if one_channel: plt.imshow(npimg, cmap=\"Greys\") else: plt.imshow(np.transpose(npimg, (1, 2, 0))) 我们将在该教程中定义一个类似的模型架构，仅需进行少量修改即可解决以下事实：图像现在是一个通道而不是三个通道，而图像是28x28而不是32x32： class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 4 * 4, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 我们将在之前定义相同的optimizer和criterion： criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 1. TensorBoard 设置 现在，我们将设置 TensorBoard，从torch.utils导入tensorboard并定义SummaryWriter，这是将信息写入 TensorBoard 的关键对象。 from torch.utils.tensorboard import SummaryWriter # default `log_dir` is \"runs\" - we'll be more specific here writer = SummaryWriter('runs/fashion_mnist_experiment_1') 请注意，仅此行会创建一个runs/fashion_mnist_experiment_1文件夹。 2. 写入 TensorBoard 现在，使用make_grid将图像写入到 TensorBoard 中，具体来说就是网格。 # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # create grid of images img_grid = torchvision.utils.make_grid(images) # show images matplotlib_imshow(img_grid, one_channel=True) # write to tensorboard writer.add_image('four_fashion_mnist_images', img_grid) 正在运行 tensorboard --logdir=runs 从命令行，然后导航到https://localhost:6006应该显示以下内容。 现在您知道如何使用 TensorBoard 了！ 但是，此示例可以在 Jupyter 笔记本中完成-TensorBoard 真正擅长的地方是创建交互式可视化。 接下来，我们将介绍其中之一，并在本教程结束时介绍更多内容。 3. 使用 TensorBoard 检查模型 TensorBoard 的优势之一是其可视化复杂模型结构的能力。 让我们可视化我们构建的模型。 writer.add_graph(net, images) writer.close() 现在刷新 TensorBoard 后，您应该会看到一个Graphs标签，如下所示： 继续并双击Net以展开它，查看构成模型的各个操作的详细视图。 TensorBoard 具有非常方便的功能，可在低维空间中可视化高维数据，例如图像数据。 接下来我们将介绍这一点。 4. 在 TensorBoard 中添加“投影仪” 我们可以通过add_embedding方法可视化高维数据的低维表示 # helper function def select_n_random(data, labels, n=100): ''' Selects n random datapoints and their corresponding labels from a dataset ''' assert len(data) == len(labels) perm = torch.randperm(len(data)) return data[perm][:n], labels[perm][:n] # select random images and their target indices images, labels = select_n_random(trainset.data, trainset.targets) # get the class labels for each image class_labels = [classes[lab] for lab in labels] # log embeddings features = images.view(-1, 28 * 28) writer.add_embedding(features, metadata=class_labels, label_img=images.unsqueeze(1)) writer.close() 现在，在 TensorBoard 的“投影仪”选项卡中，您可以看到这 100 张图像-每个图像 784 维-向下投影到三维空间中。 此外，这是交互式的：您可以单击并拖动以旋转三维投影。 最后，一些技巧可以使可视化效果更容易看到：选择左上方的“颜色：标签”，以及启用“夜间模式”，这将使图像更容易看到，因为它们的背景是白色的： 现在我们已经彻底检查了我们的数据，让我们展示了 TensorBoard 如何从训练开始就可以使跟踪模型的训练和评估更加清晰。 5. 使用 TensorBoard 跟踪模型训练 在前面的示例中，我们仅每 2000 次迭代打印该模型的运行损失。 现在，我们将运行损失记录到 TensorBoard 中，并通过plot_classes_preds函数查看模型所做的预测。 # helper functions def images_to_probs(net, images): ''' Generates predictions and corresponding probabilities from a trained network and a list of images ''' output = net(images) # convert output probabilities to predicted class _, preds_tensor = torch.max(output, 1) preds = np.squeeze(preds_tensor.numpy()) return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)] def plot_classes_preds(net, images, labels): ''' Generates matplotlib Figure using a trained network, along with images and labels from a batch, that shows the network's top prediction along with its probability, alongside the actual label, coloring this information based on whether the prediction was correct or not. Uses the \"images_to_probs\" function. ''' preds, probs = images_to_probs(net, images) # plot the images in the batch, along with predicted and true labels fig = plt.figure(figsize=(12, 48)) for idx in np.arange(4): ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[]) matplotlib_imshow(images[idx], one_channel=True) ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format( classes[preds[idx]], probs[idx] * 100.0, classes[labels[idx]]), color=(\"green\" if preds[idx]==labels[idx].item() else \"red\")) return fig 最后，让我们使用与之前教程相同的模型训练代码来训练模型，但是每 1000 批将结果写入 TensorBoard，而不是打印到控制台。 这是通过add_scalar函数完成的。 此外，在训练过程中，我们将生成一幅图像，显示该批量中包含的四幅图像的模型预测与实际结果。 running_loss = 0.0 for epoch in range(1): # loop over the dataset multiple times for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 1000 == 999: # every 1000 mini-batches... # ...log the running loss writer.add_scalar('training loss', running_loss / 1000, epoch * len(trainloader) + i) # ...log a Matplotlib Figure showing the model's predictions on a # random mini-batch writer.add_figure('predictions vs. actuals', plot_classes_preds(net, inputs, labels), global_step=epoch * len(trainloader) + i) running_loss = 0.0 print('Finished Training') 现在，您可以查看“标量”选项卡，以查看在 15,000 次训练迭代中绘制的运行损失： 此外，我们可以查看整个学习过程中模型在任意批量上所做的预测。 查看“图像”选项卡，然后在“预测与实际”可视化条件下向下滚动以查看此内容； 这表明，例如，仅经过 3000 次训练迭代，该模型就已经能够区分出视觉上截然不同的类，例如衬衫，运动鞋和外套，尽管它并没有像后来的训练那样有信心： 在之前的教程中，我们研究了模型训练后的每类准确率； 在这里，我们将使用 TensorBoard 绘制每个类别的精确调用曲线（在这里解释）。 6. 使用 TensorBoard 评估经过训练的模型 # 1\\. gets the probability predictions in a test_size x num_classes Tensor # 2\\. gets the preds in a test_size Tensor # takes ~10 seconds to run class_probs = [] class_preds = [] with torch.no_grad(): for data in testloader: images, labels = data output = net(images) class_probs_batch = [F.softmax(el, dim=0) for el in output] _, class_preds_batch = torch.max(output, 1) class_probs.append(class_probs_batch) class_preds.append(class_preds_batch) test_probs = torch.cat([torch.stack(batch) for batch in class_probs]) test_preds = torch.cat(class_preds) # helper function def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0): ''' Takes in a \"class_index\" from 0 to 9 and plots the corresponding precision-recall curve ''' tensorboard_preds = test_preds == class_index tensorboard_probs = test_probs[:, class_index] writer.add_pr_curve(classes[class_index], tensorboard_preds, tensorboard_probs, global_step=global_step) writer.close() # plot all the pr curves for i in range(len(classes)): add_pr_curve_tensorboard(i, test_probs, test_preds) 现在，您将看到一个PR Curves选项卡，其中包含每个类别的精确调用曲线。 继续四处戳； 您会发现在某些类别中，模型的“曲线下面积”接近 100%，而在另一些类别中，该面积更低： 这是 TensorBoard 和 PyTorch 与之集成的介绍。 当然，您可以在 Jupyter 笔记本中完成 TensorBoard 的所有操作，但是使用 TensorBoard 时，默认情况下会获得交互式的视觉效果。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"18.html":{"url":"18.html","title":"图片/视频","keywords":"","body":"图片/视频 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"19.html":{"url":"19.html","title":"torchvision对象检测微调教程","keywords":"","body":"torchvision对象检测微调教程 原文：https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html 小费 为了充分利用本教程，我们建议使用此 Colab 版本。 这将使您可以尝试以下信息。 在本教程中，我们将对 Penn-Fudan 数据库中的行人检测和分割，使用预训练的 Mask R-CNN 模型进行微调。 它包含 170 个图像和 345 个行人实例，我们将用它来说明如何在torchvision中使用新功能，以便在自定义数据集上训练实例细分模型。 定义数据集 用于训练对象检测，实例细分和人员关键点检测的参考脚本可轻松支持添加新的自定义数据集。 数据集应继承自标准torch.utils.data.Dataset类，并实现__len__和__getitem__。 我们唯一需要的特异性是数据集__getitem__应该返回： 图像：大小为(H, W)的 PIL 图像 目标：包含以下字段的字典 boxes (FloatTensor[N, 4])：[x0, y0, x1, y1]格式的N边界框的坐标，范围从0至W，从0至H labels (Int64Tensor[N])：每个边界框的标签。 0始终代表背景类。 image_id (Int64Tensor[1])：图像标识符。 它在数据集中的所有图像之间应该是唯一的，并在评估过程中使用 area (Tensor[N])：边界框的区域。 在使用 COCO 度量进行评估时，可使用此值来区分小盒子，中盒子和大盒子之间的度量得分。 iscrowd (UInt8Tensor[N])：iscrowd = True的实例在评估期间将被忽略。 （可选）masks (UInt8Tensor[N, H, W])：每个对象的分割蒙版 （可选）keypoints (FloatTensor[N, K, 3])：对于 N 个对象中的每一个，它包含[x, y, visibility]格式的 K 个关键点，以定义对象。 可见性为 0 表示关键点不可见。 请注意，对于数据扩充，翻转关键点的概念取决于数据表示形式，您可能应该将references/detection/transforms.py修改为新的关键点表示形式 如果您的模型返回上述方法，则它们将使其适用于训练和评估，并将使用pycocotools中的评估脚本。 注意 对于 Windows，请使用命令从gautamchitnis安装pycocotools pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI 关于labels的注解。 该模型将0类作为背景。 如果您的数据集不包含背景类，则labels中不应包含0。 例如，假设您只有猫和狗两类，则可以定义1来表示猫和0代表狗。 因此，例如，如果其中一个图像同时具有两个类，则您的labels张量应类似于[1,2]。 此外，如果要在训练过程中使用宽高比分组（以便每个批量仅包含具有相似长宽比的图像），则建议您还实现get_height_and_width方法，该方法返回图像的高度和宽度。 如果未提供此方法，我们将通过__getitem__查询数据集的所有元素，这会将图像加载到内存中，并且比提供自定义方法慢。 为 PennFudan 编写自定义数据集 让我们为 PennFudan 数据集编写一个数据集。 在下载并解压缩 zip 文件之后，我们具有以下文件夹结构： PennFudanPed/ PedMasks/ FudanPed00001_mask.png FudanPed00002_mask.png FudanPed00003_mask.png FudanPed00004_mask.png ... PNGimg/ FudanPed00001.png FudanPed00002.png FudanPed00003.png FudanPed00004.png 这是一对图像和分割蒙版的一个示例 因此，每个图像都有一个对应的分割蒙版，其中每个颜色对应一个不同的实例。 让我们为此数据集编写一个torch.utils.data.Dataset类。 import os import numpy as np import torch from PIL import Image class PennFudanDataset(object): def __init__(self, root, transforms): self.root = root self.transforms = transforms # load all image files, sorting them to # ensure that they are aligned self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\")))) self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\")))) def __getitem__(self, idx): # load images ad masks img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx]) mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx]) img = Image.open(img_path).convert(\"RGB\") # note that we haven't converted the mask to RGB, # because each color corresponds to a different instance # with 0 being background mask = Image.open(mask_path) # convert the PIL Image into a numpy array mask = np.array(mask) # instances are encoded as different colors obj_ids = np.unique(mask) # first id is the background, so remove it obj_ids = obj_ids[1:] # split the color-encoded mask into a set # of binary masks masks = mask == obj_ids[:, None, None] # get bounding box coordinates for each mask num_objs = len(obj_ids) boxes = [] for i in range(num_objs): pos = np.where(masks[i]) xmin = np.min(pos[1]) xmax = np.max(pos[1]) ymin = np.min(pos[0]) ymax = np.max(pos[0]) boxes.append([xmin, ymin, xmax, ymax]) # convert everything into a torch.Tensor boxes = torch.as_tensor(boxes, dtype=torch.float32) # there is only one class labels = torch.ones((num_objs,), dtype=torch.int64) masks = torch.as_tensor(masks, dtype=torch.uint8) image_id = torch.tensor([idx]) area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # suppose all instances are not crowd iscrowd = torch.zeros((num_objs,), dtype=torch.int64) target = {} target[\"boxes\"] = boxes target[\"labels\"] = labels target[\"masks\"] = masks target[\"image_id\"] = image_id target[\"area\"] = area target[\"iscrowd\"] = iscrowd if self.transforms is not None: img, target = self.transforms(img, target) return img, target def __len__(self): return len(self.imgs) 这就是数据集的全部内容。 现在，我们定义一个可以对该数据集执行预测的模型。 定义模型 在本教程中，我们将基于 Faster R-CNN 使用 Mask R-CNN 。 Faster R-CNN 是可预测图像中潜在对象的边界框和类分数的模型。 Mask R-CNN 在 Faster R-CNN 中增加了一个分支，该分支还可以预测每个实例的分割掩码。 在两种常见情况下，可能要修改torchvision模型动物园中的可用模型之一。 首先是当我们想从预先训练的模型开始，然后微调最后一层时。 另一个是当我们想要用另一个模型替换主干时（例如，为了更快的预测）。 在以下各节中，让我们看看如何做一个或另一个。 1-将预训练模型用于微调 假设您要从在 COCO 上经过预训练的模型开始，并希望针对您的特定类对其进行微调。 这是一种可行的方法： import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # load a model pre-trained pre-trained on COCO model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # replace the classifier with a new one, that has # num_classes which is user-defined num_classes = 2 # 1 class (person) + background # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) 2-修改模型以添加其他主干 import torchvision from torchvision.models.detection import FasterRCNN from torchvision.models.detection.rpn import AnchorGenerator # load a pre-trained model for classification and return # only the features backbone = torchvision.models.mobilenet_v2(pretrained=True).features # FasterRCNN needs to know the number of # output channels in a backbone. For mobilenet_v2, it's 1280 # so we need to add it here backbone.out_channels = 1280 # let's make the RPN generate 5 x 3 anchors per spatial # location, with 5 different sizes and 3 different aspect # ratios. We have a Tuple[Tuple[int]] because each feature # map could potentially have different sizes and # aspect ratios anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)) # let's define what are the feature maps that we will # use to perform the region of interest cropping, as well as # the size of the crop after rescaling. # if your backbone returns a Tensor, featmap_names is expected to # be [0]. More generally, the backbone should return an # OrderedDict[Tensor], and in featmap_names you can choose which # feature maps to use. roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0], output_size=7, sampling_ratio=2) # put the pieces together inside a FasterRCNN model model = FasterRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler) PennFudan 数据集的实例细分模型 在我们的案例中，由于我们的数据集非常小，我们希望从预训练模型中进行微调，因此我们将遵循方法 1。 这里我们还想计算实例分割掩码，因此我们将使用 Mask R-CNN： import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor def get_model_instance_segmentation(num_classes): # load an instance segmentation model pre-trained pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # now get the number of input features for the mask classifier in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 # and replace the mask predictor with a new one model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes) return model 就是这样，这将使model随时可以在您的自定义数据集上进行训练和评估。 将所有内容放在一起 在references/detection/中，我们提供了许多帮助程序功能来简化训练和评估检测模型。 在这里，我们将使用references/detection/engine.py，references/detection/utils.py和references/detection/transforms.py。 只需将它们复制到您的文件夹中，然后在此处使用它们即可。 让我们写一些辅助函数来进行数据扩充/转换： import transforms as T def get_transform(train): transforms = [] transforms.append(T.ToTensor()) if train: transforms.append(T.RandomHorizontalFlip(0.5)) return T.Compose(transforms) 测试forward()方法（可选） 在遍历数据集之前，最好先查看模型在训练过程中的期望值以及对样本数据的推断时间。 model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) dataset = PennFudanDataset('PennFudanPed', get_transform(train=True)) data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn) # For Training images,targets = next(iter(data_loader)) images = list(image for image in images) targets = [{k: v for k, v in t.items()} for t in targets] output = model(images,targets) # Returns losses and detections # For inference model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) # Returns predictions 现在，我们编写执行训练和验证的main函数： from engine import train_one_epoch, evaluate import utils def main(): # train on the GPU or on the CPU, if a GPU is not available device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # our dataset has two classes only - background and person num_classes = 2 # use our dataset and defined transformations dataset = PennFudanDataset('PennFudanPed', get_transform(train=True)) dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False)) # split the dataset in train and test set indices = torch.randperm(len(dataset)).tolist() dataset = torch.utils.data.Subset(dataset, indices[:-50]) dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:]) # define training and validation data loaders data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn) data_loader_test = torch.utils.data.DataLoader( dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=utils.collate_fn) # get the model using our helper function model = get_model_instance_segmentation(num_classes) # move model to the right device model.to(device) # construct an optimizer params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # let's train it for 10 epochs num_epochs = 10 for epoch in range(num_epochs): # train for one epoch, printing every 10 iterations train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) # update the learning rate lr_scheduler.step() # evaluate on the test dataset evaluate(model, data_loader_test, device=device) print(\"That's it!\") 您应该获得第一个周期的输出： Epoch: [0] [ 0/60] eta: 0:01:18 lr: 0.000090 loss: 2.5213 (2.5213) loss_classifier: 0.8025 (0.8025) loss_box_reg: 0.2634 (0.2634) loss_mask: 1.4265 (1.4265) loss_objectness: 0.0190 (0.0190) loss_rpn_box_reg: 0.0099 (0.0099) time: 1.3121 data: 0.3024 max mem: 3485 Epoch: [0] [10/60] eta: 0:00:20 lr: 0.000936 loss: 1.3007 (1.5313) loss_classifier: 0.3979 (0.4719) loss_box_reg: 0.2454 (0.2272) loss_mask: 0.6089 (0.7953) loss_objectness: 0.0197 (0.0228) loss_rpn_box_reg: 0.0121 (0.0141) time: 0.4198 data: 0.0298 max mem: 5081 Epoch: [0] [20/60] eta: 0:00:15 lr: 0.001783 loss: 0.7567 (1.1056) loss_classifier: 0.2221 (0.3319) loss_box_reg: 0.2002 (0.2106) loss_mask: 0.2904 (0.5332) loss_objectness: 0.0146 (0.0176) loss_rpn_box_reg: 0.0094 (0.0123) time: 0.3293 data: 0.0035 max mem: 5081 Epoch: [0] [30/60] eta: 0:00:11 lr: 0.002629 loss: 0.4705 (0.8935) loss_classifier: 0.0991 (0.2517) loss_box_reg: 0.1578 (0.1957) loss_mask: 0.1970 (0.4204) loss_objectness: 0.0061 (0.0140) loss_rpn_box_reg: 0.0075 (0.0118) time: 0.3403 data: 0.0044 max mem: 5081 Epoch: [0] [40/60] eta: 0:00:07 lr: 0.003476 loss: 0.3901 (0.7568) loss_classifier: 0.0648 (0.2022) loss_box_reg: 0.1207 (0.1736) loss_mask: 0.1705 (0.3585) loss_objectness: 0.0018 (0.0113) loss_rpn_box_reg: 0.0075 (0.0112) time: 0.3407 data: 0.0044 max mem: 5081 Epoch: [0] [50/60] eta: 0:00:03 lr: 0.004323 loss: 0.3237 (0.6703) loss_classifier: 0.0474 (0.1731) loss_box_reg: 0.1109 (0.1561) loss_mask: 0.1658 (0.3201) loss_objectness: 0.0015 (0.0093) loss_rpn_box_reg: 0.0093 (0.0116) time: 0.3379 data: 0.0043 max mem: 5081 Epoch: [0] [59/60] eta: 0:00:00 lr: 0.005000 loss: 0.2540 (0.6082) loss_classifier: 0.0309 (0.1526) loss_box_reg: 0.0463 (0.1405) loss_mask: 0.1568 (0.2945) loss_objectness: 0.0012 (0.0083) loss_rpn_box_reg: 0.0093 (0.0123) time: 0.3489 data: 0.0042 max mem: 5081 Epoch: [0] Total time: 0:00:21 (0.3570 s / it) creating index... index created! Test: [ 0/50] eta: 0:00:19 model_time: 0.2152 (0.2152) evaluator_time: 0.0133 (0.0133) time: 0.4000 data: 0.1701 max mem: 5081 Test: [49/50] eta: 0:00:00 model_time: 0.0628 (0.0687) evaluator_time: 0.0039 (0.0064) time: 0.0735 data: 0.0022 max mem: 5081 Test: Total time: 0:00:04 (0.0828 s / it) Averaged stats: model_time: 0.0628 (0.0687) evaluator_time: 0.0039 (0.0064) Accumulating evaluation results... DONE (t=0.01s). Accumulating evaluation results... DONE (t=0.01s). IoU metric: bbox Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.606 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.984 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.780 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.612 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.270 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.672 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.672 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.755 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.664 IoU metric: segm Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.704 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.979 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.871 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.488 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.727 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.316 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.748 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.749 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.758 因此，经过一个周期的训练，我们获得了 60.6 的 COCO 风格 mAP 和 70.4 的遮罩 mAP。 经过 10 个周期的训练，我得到了以下指标 IoU metric: bbox Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.799 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.969 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.935 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.324 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.844 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.844 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.777 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.870 IoU metric: segm Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.761 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.969 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.919 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.464 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.303 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.799 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.799 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.769 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.818 但是这些预测是什么样的？ 让我们在数据集中拍摄一张图像并进行验证 经过训练的模型会在此图片中预测 9 个人物实例，让我们看看其中的几个： 结果看起来还不错！ 总结 在本教程中，您学习了如何在自定义数据集上为实例细分模型创建自己的训练管道。 为此，您编写了一个torch.utils.data.Dataset类，该类返回图像以及真实情况框和分段蒙版。 您还利用了在 COCO train2017 上预先训练的 Mask R-CNN 模型，以便对该新数据集执行迁移学习。 对于更完整的示例（包括多机/多 GPU 训练），请检查在torchvision存储库中存在的references/detection/train.py。 您可以在此处下载本教程的完整源文件。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"20.html":{"url":"20.html","title":"计算机视觉的迁移学习教程","keywords":"","body":"计算机视觉的迁移学习教程 原文：https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html 作者： Sasank Chilamkurthy 在本教程中，您将学习如何使用迁移学习训练卷积神经网络进行图像分类。 您可以在 cs231n 笔记中阅读有关转学的更多信息。 引用这些注解， 实际上，很少有人从头开始训练整个卷积网络（使用随机初始化），因为拥有足够大小的数据集相对很少。 相反，通常在非常大的数据集上对 ConvNet 进行预训练（例如 ImageNet，其中包含 120 万个具有 1000 个类别的图像），然后将 ConvNet 用作初始化或固定特征提取器以完成感兴趣的任务。 这两个主要的迁移学习方案如下所示： 卷积网络的微调：代替随机初始化，我们使用经过预训练的网络初始化网络，例如在 imagenet 1000 数据集上进行训练的网络。 其余的训练照常进行。 作为固定特征提取器的 ConvNet：在这里，我们将冻结除最终全连接层之外的所有网络的权重。 最后一个全连接层将替换为具有随机权重的新层，并且仅训练该层。 # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function, division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy plt.ion() # interactive mode 加载数据 我们将使用torchvision和torch.utils.data包来加载数据。 我们今天要解决的问题是训练一个模型来对蚂蚁和蜜蜂进行分类。 我们为蚂蚁和蜜蜂提供了大约 120 张训练图像。 每个类别有 75 个验证图像。 通常，如果从头开始训练的话，这是一个非常小的数据集。 由于我们正在使用迁移学习，因此我们应该能够很好地概括。 该数据集是 imagenet 的很小一部分。 注意 从的下载数据，并将其提取到当前目录。 # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = 'data/hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") 可视化一些图像 让我们可视化一些训练图像，以了解数据扩充。 def imshow(inp, title=None): \"\"\"Imshow for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders['train'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) 训练模型 现在，让我们编写一个通用函数来训练模型。 在这里，我们将说明： 安排学习率 保存最佳模型 以下，参数scheduler是来自torch.optim.lr_scheduler的 LR 调度器对象。 def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == 'train': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model 可视化模型预测 通用函数，显示一些图像的预测 def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders['val']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_img//2, 2, images_so_far) ax.axis('off') ax.set_title('predicted: {}'.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) 微调 ConvNet 加载预训练的模型并重置最终的全连接层。 model_ft = models.resnet18(pretrained=True) num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) 训练和评估 在 CPU 上大约需要 15-25 分钟。 但是在 GPU 上，此过程不到一分钟。 model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) 出： Epoch 0/24 ---------- train Loss: 0.6303 Acc: 0.6926 val Loss: 0.1492 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.5511 Acc: 0.7869 val Loss: 0.2577 Acc: 0.8889 Epoch 2/24 ---------- train Loss: 0.4885 Acc: 0.8115 val Loss: 0.3390 Acc: 0.8758 Epoch 3/24 ---------- train Loss: 0.5158 Acc: 0.7992 val Loss: 0.5070 Acc: 0.8366 Epoch 4/24 ---------- train Loss: 0.5878 Acc: 0.7992 val Loss: 0.2706 Acc: 0.8758 Epoch 5/24 ---------- train Loss: 0.4396 Acc: 0.8279 val Loss: 0.2870 Acc: 0.8954 Epoch 6/24 ---------- train Loss: 0.4612 Acc: 0.8238 val Loss: 0.2809 Acc: 0.9150 Epoch 7/24 ---------- train Loss: 0.4387 Acc: 0.8402 val Loss: 0.1853 Acc: 0.9281 Epoch 8/24 ---------- train Loss: 0.2998 Acc: 0.8648 val Loss: 0.1926 Acc: 0.9085 Epoch 9/24 ---------- train Loss: 0.3383 Acc: 0.9016 val Loss: 0.1762 Acc: 0.9281 Epoch 10/24 ---------- train Loss: 0.2969 Acc: 0.8730 val Loss: 0.1872 Acc: 0.8954 Epoch 11/24 ---------- train Loss: 0.3117 Acc: 0.8811 val Loss: 0.1807 Acc: 0.9150 Epoch 12/24 ---------- train Loss: 0.3005 Acc: 0.8770 val Loss: 0.1930 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.3129 Acc: 0.8689 val Loss: 0.2184 Acc: 0.9150 Epoch 14/24 ---------- train Loss: 0.3776 Acc: 0.8607 val Loss: 0.1869 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.2245 Acc: 0.9016 val Loss: 0.1742 Acc: 0.9346 Epoch 16/24 ---------- train Loss: 0.3105 Acc: 0.8607 val Loss: 0.2056 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2729 Acc: 0.8893 val Loss: 0.1722 Acc: 0.9085 Epoch 18/24 ---------- train Loss: 0.3210 Acc: 0.8730 val Loss: 0.1977 Acc: 0.9281 Epoch 19/24 ---------- train Loss: 0.3231 Acc: 0.8566 val Loss: 0.1811 Acc: 0.9216 Epoch 20/24 ---------- train Loss: 0.3206 Acc: 0.8648 val Loss: 0.2033 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2917 Acc: 0.8648 val Loss: 0.1694 Acc: 0.9150 Epoch 22/24 ---------- train Loss: 0.2412 Acc: 0.8852 val Loss: 0.1757 Acc: 0.9216 Epoch 23/24 ---------- train Loss: 0.2508 Acc: 0.8975 val Loss: 0.1662 Acc: 0.9281 Epoch 24/24 ---------- train Loss: 0.3283 Acc: 0.8566 val Loss: 0.1761 Acc: 0.9281 Training complete in 1m 10s Best val Acc: 0.934641 visualize_model(model_ft) 作为固定特征提取器的 ConvNet 在这里，我们需要冻结除最后一层之外的所有网络。 我们需要设置requires_grad == False冻结参数，以便不在backward()中计算梯度。 您可以在文档中阅读有关此内容的更多信息。 model_conv = torchvision.models.resnet18(pretrained=True) for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) 训练和评估 与以前的方案相比，在 CPU 上将花费大约一半的时间。 这是可以预期的，因为不需要为大多数网络计算梯度。 但是，确实需要计算正向。 model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) 出： Epoch 0/24 ---------- train Loss: 0.7258 Acc: 0.6148 val Loss: 0.2690 Acc: 0.9020 Epoch 1/24 ---------- train Loss: 0.5342 Acc: 0.7500 val Loss: 0.1905 Acc: 0.9412 Epoch 2/24 ---------- train Loss: 0.4262 Acc: 0.8320 val Loss: 0.1903 Acc: 0.9412 Epoch 3/24 ---------- train Loss: 0.4103 Acc: 0.8197 val Loss: 0.2658 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.3938 Acc: 0.8115 val Loss: 0.2871 Acc: 0.8954 Epoch 5/24 ---------- train Loss: 0.4623 Acc: 0.8361 val Loss: 0.1651 Acc: 0.9346 Epoch 6/24 ---------- train Loss: 0.5348 Acc: 0.7869 val Loss: 0.1944 Acc: 0.9477 Epoch 7/24 ---------- train Loss: 0.3827 Acc: 0.8402 val Loss: 0.1846 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.3655 Acc: 0.8443 val Loss: 0.1873 Acc: 0.9412 Epoch 9/24 ---------- train Loss: 0.3275 Acc: 0.8525 val Loss: 0.2091 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.3375 Acc: 0.8320 val Loss: 0.1798 Acc: 0.9412 Epoch 11/24 ---------- train Loss: 0.3077 Acc: 0.8648 val Loss: 0.1942 Acc: 0.9346 Epoch 12/24 ---------- train Loss: 0.4336 Acc: 0.7787 val Loss: 0.1934 Acc: 0.9346 Epoch 13/24 ---------- train Loss: 0.3149 Acc: 0.8566 val Loss: 0.2062 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.3617 Acc: 0.8320 val Loss: 0.1761 Acc: 0.9412 Epoch 15/24 ---------- train Loss: 0.3066 Acc: 0.8361 val Loss: 0.1799 Acc: 0.9281 Epoch 16/24 ---------- train Loss: 0.3952 Acc: 0.8443 val Loss: 0.1666 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3552 Acc: 0.8443 val Loss: 0.1928 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.3106 Acc: 0.8648 val Loss: 0.1964 Acc: 0.9346 Epoch 19/24 ---------- train Loss: 0.3675 Acc: 0.8566 val Loss: 0.1813 Acc: 0.9346 Epoch 20/24 ---------- train Loss: 0.3565 Acc: 0.8320 val Loss: 0.1758 Acc: 0.9346 Epoch 21/24 ---------- train Loss: 0.2922 Acc: 0.8566 val Loss: 0.2295 Acc: 0.9216 Epoch 22/24 ---------- train Loss: 0.3283 Acc: 0.8402 val Loss: 0.2267 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.2875 Acc: 0.8770 val Loss: 0.1878 Acc: 0.9346 Epoch 24/24 ---------- train Loss: 0.3172 Acc: 0.8689 val Loss: 0.1849 Acc: 0.9412 Training complete in 0m 34s Best val Acc: 0.947712 visualize_model(model_conv) plt.ioff() plt.show() 进一步学习 如果您想了解有关迁移学习的更多信息，请查看我们的计算机视觉教程的量化迁移学习。 脚本的总运行时间：（1 分钟 56.157 秒） 下载 Python 源码：transfer_learning_tutorial.py 下载 Jupyter 笔记本：transfer_learning_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"21.html":{"url":"21.html","title":"对抗示例生成","keywords":"","body":"对抗示例生成 原文：https://pytorch.org/tutorials/beginner/fgsm_tutorial.html 作者： Nathan Inkawhich 如果您正在阅读本文，希望您能体会到某些机器学习模型的有效性。 研究不断推动 ML 模型更快，更准确和更高效。 但是，设计和训练模型的一个经常被忽略的方面是安全性和鲁棒性，尤其是在面对想要欺骗模型的对手的情况下。 本教程将提高您对 ML 模型的安全漏洞的认识，并深入了解对抗性机器学习的热门话题。 您可能会惊讶地发现，在图像上添加无法察觉的扰动会导致完全不同的模型表现。 鉴于这是一个教程，我们将通过图像分类器上的示例来探讨该主题。 具体而言，我们将使用最流行的一种攻击方法，即快速梯度符号攻击（FGSM）来欺骗 MNIST 分类器。 威胁模型 就上下文而言，有多种类型的对抗性攻击，每种攻击者的目标和假设都不同。 但是，总的来说，总体目标是向输入数据添加最少的扰动，以引起所需的错误分类。 攻击者的知识有几种假设，其中两种是：白盒和黑盒。 白盒攻击假定攻击者具有完全的知识并可以访问模型，包括架构，输入，输出和权重。 黑盒攻击假定攻击者只能访问模型的输入和输出，并且对底层架构或权重一无所知。 目标也有几种类型，包括错误分类和源/目标错误分类。 错误分类意味着对手只希望输出分类错误，而不在乎新分类是什么。 源/目标错误分类意味着对手想要更改最初属于特定源类别的图像，以便将其分类为特定目标类别。 在这种情况下，FGSM 攻击是白盒攻击，目标是错误分类。 有了这些背景信息，我们现在可以详细讨论攻击了。 快速梯度符号攻击 迄今为止，最早的也是最流行的对抗性攻击之一被称为快速梯度符号攻击（FGSM），由《解释和利用对抗性示例》（Goodfellow 等）描述。 攻击非常强大，而且直观。 它旨在利用神经网络学习梯度的方式来攻击神经网络。 这个想法很简单，不是通过基于反向传播的梯度来调整权重来使损失最小化，攻击会基于相同的反向传播的梯度来调整输入数据，以使损失最大化。 换句话说，攻击使用损失相对于输入数据的梯度，然后调整输入数据以使损失最大化。 在进入代码之前，让我们看一下著名的 FGSM Pandas 示例，并提取一些符号。 从图中，x是正确分类为“Pandas”的原始输入图像，y是x的输出，θ表示模型参数，而J(θ, x, y)是用于训练网络的损失。 攻击会将梯度反向传播回输入数据，以计算ᐁ[x] J(θ, x, y)。 然后，它会沿方向（即ᐁ[x] J(θ)）沿一小步（图片中的ε或0.007）调整输入数据，(x, y)，这将使损失最大化。 然后，当目标图像仍明显是“Pandas”时，目标网络将它们误分类为“长臂猿”。 希望本教程的动机已经明确，所以让我们跳入实现过程。 from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms import numpy as np import matplotlib.pyplot as plt 实现 在本节中，我们将讨论本教程的输入参数，定义受到攻击的模型，然后编写攻击代码并运行一些测试。 输入 本教程只有三个输入，定义如下： epsilons-用于运行的ε值列表。 在列表中保留 0 很重要，因为它表示原始测试集上的模型表现。 同样，从直觉上讲，我们期望ε越大，扰动越明显，但是从降低模型准确率的角度来看，攻击越有效。 由于此处的数据范围为[0,1]，因此ε值不得超过 1。 pretrained_model-使用pytorch/examples/mnist训练的 MNIST 模型的路径。 为简单起见，请在此处下载预训练模型。 use_cuda-布尔标志，如果需要和可用，则使用 CUDA。 请注意，具有 CUDA 的 GPU 在本教程中并不重要，因为 CPU 不会花费很多时间。 epsilons = [0, .05, .1, .15, .2, .25, .3] pretrained_model = \"data/lenet_mnist_model.pth\" use_cuda=True 受到攻击的模型 如前所述，受到攻击的模型与pytorch/examples/mnist中的 MNIST 模型相同。 您可以训练并保存自己的 MNIST 模型，也可以下载并使用提供的模型。 这里的网络定义和测试数据加载器已从 MNIST 示例中复制而来。 本部分的目的是定义模型和数据加载器，然后初始化模型并加载预训练的权重。 # LeNet Model definition class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) # MNIST Test dataset and dataloader declaration test_loader = torch.utils.data.DataLoader( datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([ transforms.ToTensor(), ])), batch_size=1, shuffle=True) # Define what device we are using print(\"CUDA Available: \",torch.cuda.is_available()) device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\") # Initialize the network model = Net().to(device) # Load the pretrained model model.load_state_dict(torch.load(pretrained_model, map_location='cpu')) # Set the model in evaluation mode. In this case this is for the Dropout layers model.eval() 出： Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw Processing... Done! CUDA Available: True FGSM 攻击 现在，我们可以通过干扰原始输入来定义创建对抗示例的函数。 fgsm_attack函数接受三个输入，image是原始的干净图像（x），epsilon是像素级扰动量ε，data_grad是输入图像损失的梯度（ᐁ[x] J(θ, x, y)）。 该函数然后创建扰动图像为 最后，为了维持数据的原始范围，将被扰动的图像裁剪到范围[0,1]。 # FGSM attack code def fgsm_attack(image, epsilon, data_grad): # Collect the element-wise sign of the data gradient sign_data_grad = data_grad.sign() # Create the perturbed image by adjusting each pixel of the input image perturbed_image = image + epsilon*sign_data_grad # Adding clipping to maintain [0,1] range perturbed_image = torch.clamp(perturbed_image, 0, 1) # Return the perturbed image return perturbed_image 测试函数 最后，本教程的主要结果来自test函数。 每次调用此测试函数都会在 MNIST 测试集上执行完整的测试步骤，并报告最终精度。 但是，请注意，此函数还需要epsilon输入。 这是因为test函数报告实力为ε的来自对手的攻击模型的准确率。 更具体地说，对于测试集中的每个样本，函数都会计算输入数据data_grad的损失梯度，并使用fgsm_attack创建一个扰动图像perturbed_data，然后检查受干扰的示例是否具有对抗性。 除了测试模型的准确率外，该函数还保存并返回了一些成功的对抗示例，以供以后可视化。 def test( model, device, test_loader, epsilon ): # Accuracy counter correct = 0 adv_examples = [] # Loop over all examples in test set for data, target in test_loader: # Send the data and label to the device data, target = data.to(device), target.to(device) # Set requires_grad attribute of tensor. Important for Attack data.requires_grad = True # Forward pass the data through the model output = model(data) init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability # If the initial prediction is wrong, dont bother attacking, just move on if init_pred.item() != target.item(): continue # Calculate the loss loss = F.nll_loss(output, target) # Zero all existing gradients model.zero_grad() # Calculate gradients of model in backward pass loss.backward() # Collect datagrad data_grad = data.grad.data # Call FGSM Attack perturbed_data = fgsm_attack(data, epsilon, data_grad) # Re-classify the perturbed image output = model(perturbed_data) # Check for success final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability if final_pred.item() == target.item(): correct += 1 # Special case for saving 0 epsilon examples if (epsilon == 0) and (len(adv_examples) 运行攻击 实现的最后一部分是实际运行攻击。 在这里，我们为epsilon输入中的每个ε值运行完整的测试步骤。 对于每个ε，我们还保存最终精度，并在接下来的部分中绘制一些成功的对抗示例。 请注意，随着ε值的增加，打印的精度如何降低。 另外，请注意ε = 0表示原始测试准确率，没有受到攻击。 accuracies = [] examples = [] # Run test for each epsilon for eps in epsilons: acc, ex = test(model, device, test_loader, eps) accuracies.append(acc) examples.append(ex) 出： Epsilon: 0 Test Accuracy = 9810 / 10000 = 0.981 Epsilon: 0.05 Test Accuracy = 9426 / 10000 = 0.9426 Epsilon: 0.1 Test Accuracy = 8510 / 10000 = 0.851 Epsilon: 0.15 Test Accuracy = 6826 / 10000 = 0.6826 Epsilon: 0.2 Test Accuracy = 4301 / 10000 = 0.4301 Epsilon: 0.25 Test Accuracy = 2082 / 10000 = 0.2082 Epsilon: 0.3 Test Accuracy = 869 / 10000 = 0.0869 结果 准确率与ε 第一个结果是精度与ε曲线的关系。 如前所述，随着ε的增加，我们预计测试精度会降低。 这是因为更大的ε意味着我们朝着将损失最大化的方向迈出了更大的一步。 请注意，即使ε值是线性间隔的，曲线中的趋势也不是线性的。 例如，ε = 0.05处的精度仅比ε = 0低约 4%，但ε = 0.2处的精度比ε = 0.15。 另外，请注意，模型的准确率在ε = 0.25和ε = 0.3之间达到 10 类分类器的随机准确率。 plt.figure(figsize=(5,5)) plt.plot(epsilons, accuracies, \"*-\") plt.yticks(np.arange(0, 1.1, step=0.1)) plt.xticks(np.arange(0, .35, step=0.05)) plt.title(\"Accuracy vs Epsilon\") plt.xlabel(\"Epsilon\") plt.ylabel(\"Accuracy\") plt.show() 对抗示例样本 还记得没有免费午餐的想法吗？ 在这种情况下，随着ε的增加，测试精度降低，但扰动变得更容易察觉。 实际上，在攻击者必须考虑的准确率下降和可感知性之间要进行权衡。 在这里，我们展示了每个ε值下成功对抗示例的一些示例。 绘图的每一行显示不同的ε值。 第一行是ε = 0示例，这些示例表示没有干扰的原始“干净”图像。 每张图片的标题均显示“原始分类->对抗分类”。 注意，扰动在ε = 0.15处开始变得明显，而在ε = 0.3处则非常明显。 但是，在所有情况下，尽管增加了噪音，人类仍然能够识别正确的类别。 # Plot several examples of adversarial samples at each epsilon cnt = 0 plt.figure(figsize=(8,10)) for i in range(len(epsilons)): for j in range(len(examples[i])): cnt += 1 plt.subplot(len(epsilons),len(examples[0]),cnt) plt.xticks([], []) plt.yticks([], []) if j == 0: plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14) orig,adv,ex = examples[i][j] plt.title(\"{} -> {}\".format(orig, adv)) plt.imshow(ex, cmap=\"gray\") plt.tight_layout() plt.show() 接下来要去哪里？ 希望本教程对对抗性机器学习主题有所了解。 从这里可以找到许多潜在的方向。 这种攻击代表了对抗性攻击研究的最开始，并且由于随后有许多关于如何攻击和防御来自对手的 ML 模型的想法。 实际上，在 NIPS 2017 上有一个对抗性的攻击和防御竞赛，并且本文描述了该竞赛中使用的许多方法：《对抗性的攻击与防御竞赛》。 防御方面的工作还引发了使机器学习模型总体上更健壮的想法，以适应自然扰动和对抗性输入。 另一个方向是不同领域的对抗性攻击和防御。 对抗性研究不仅限于图像领域，请查看对语音到文本模型的这种攻击。 但是，也许更多地了解对抗性机器学习的最好方法是动手。 尝试实现与 NIPS 2017 竞赛不同的攻击，并查看它与 FGSM 有何不同。 然后，尝试保护模型免受自己的攻击。 脚本的总运行时间：（4 分钟 22.519 秒） 下载 Python 源码：fgsm_tutorial.py 下载 Jupyter 笔记本：fgsm_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"22.html":{"url":"22.html","title":"DCGAN 教程","keywords":"","body":"DCGAN 教程 原文：https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html 作者： Nathan Inkawhich 简介 本教程将通过一个示例对 DCGAN 进行介绍。 在向其展示许多真实名人的照片后，我们将训练一个生成对抗网络（GAN）来产生新名人。 此处的大多数代码来自pytorch/examples中的 dcgan 实现，并且本文档将对该实现进行详尽的解释，并阐明此模型的工作方式和原因。 但请放心，不需要 GAN 的先验知识，但这可能需要新手花一些时间来推理幕后实际发生的事情。 同样，为了节省时间，拥有一两个 GPU 也将有所帮助。 让我们从头开始。 生成对抗网络 什么是 GAN？ GAN 是用于教授 DL 模型以捕获训练数据分布的框架，因此我们可以从同一分布中生成新数据。 GAN 由 Ian Goodfellow 于 2014 年发明，并在论文《生成对抗网络》中首次进行了描述。 它们由两个不同的模型组成：生成器和判别器。 生成器的工作是生成看起来像训练图像的“假”图像。 判别器的工作是查看图像并从生成器输出它是真实的训练图像还是伪图像。 在训练过程中，生成器不断尝试通过生成越来越好的伪造品而使判别器的表现超过智者，而判别器正在努力成为更好的侦探并正确地对真实和伪造图像进行分类。 博弈的平衡点是当生成器生成的伪造品看起来像直接来自训练数据时，而判别器则总是猜测生成器输出是真实还是伪造品的 50% 置信度。 现在，让我们从判别器开始定义一些在整个教程中使用的符号。 令x为代表图像的数据。 D(x)是判别器网络，其输出x来自训练数据而不是生成器的（标量）概率。 在这里，由于我们要处理图像，因此D(x)的输入是 CHW 大小为3x64x64的图像。 直观地，当x来自训练数据时，D(x)应该为高，而当x来自生成器时，它应该为低。 D(x)也可以被认为是传统的二分类器。 对于生成器的表示法，令z是从标准正态分布中采样的潜在空间向量。 G(z)表示将隐向量z映射到数据空间的生成器函数。 G的目标是估计训练数据来自p_data的分布，以便它可以从该估计分布（p_g）生成假样本。 因此，D(G(z))是生成器G的输出是真实图像的概率（标量）。 如 Goodfellow 的论文中所述，D和G玩一个 minimax 游戏，其中D试图最大化其正确分类实物和假物log D(x)，并且G尝试最小化D预测其输出为假的概率log(1 - D(G(g(x))))。 从本文来看，GAN 损失函数为 从理论上讲，此极小极大游戏的解决方案是p_g = p_data，判别器会随机猜测输入是真实的还是假的。 但是，GAN 的收敛理论仍在积极研究中，实际上，模型并不总是能达到这一目的。 什么是 DCGAN？ DCGAN 是上述 GAN 的直接扩展，不同之处在于，DCGAN 分别在判别器和生成器中分别使用卷积和卷积转置层。 它最早由 Radford 等人，在论文《使用深度卷积生成对抗网络的无监督表示学习》中描述。 判别器由分层的卷积层，批量规范层和 LeakyReLU 激活组成。 输入是3x64x64的输入图像，输出是输入来自真实数据分布的标量概率。 生成器由转置卷积层，批量规范层和 ReLU 激活组成。 输入是从标准正态分布中提取的潜向量z，输出是3x64x64 RGB 图像。 跨步的转置层使潜向量可以转换为具有与图像相同形状的体积。 在本文中，作者还提供了一些有关如何设置优化器，如何计算损失函数以及如何初始化模型权重的提示，所有这些都将在接下来的部分中进行解释。 from __future__ import print_function #%matplotlib inline import argparse import os import random import torch import torch.nn as nn import torch.nn.parallel import torch.backends.cudnn as cudnn import torch.optim as optim import torch.utils.data import torchvision.datasets as dset import torchvision.transforms as transforms import torchvision.utils as vutils import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from IPython.display import HTML # Set random seed for reproducibility manualSeed = 999 #manualSeed = random.randint(1, 10000) # use if you want new results print(\"Random Seed: \", manualSeed) random.seed(manualSeed) torch.manual_seed(manualSeed) 出： Random Seed: 999 输入 让我们为跑步定义一些输入： dataroot-数据集文件夹根目录的路径。 我们将在下一节中进一步讨论数据集 worker-使用DataLoader加载数据的工作线程数 batch_size-训练中使用的批量大小。 DCGAN 纸使用的批量大小为 128 image_size-用于训练的图像的空间大小。 此实现默认为64x64。 如果需要其他尺寸，则必须更改D和G的结构。 有关更多详细信息，请参见此处。 nc-输入图像中的彩色通道数。 对于彩色图像，这是 3 nz-潜向量的长度 ngf-与通过生成器传送的特征映射的深度有关 ndf-设置通过判别器传播的特征映射的深度 num_epochs-要运行的训练周期数。 训练更长的时间可能会导致更好的结果，但也会花费更长的时间 lr-训练的学习率。 如 DCGAN 文件中所述，此数字应为 0.0002 beta1-Adam 优化器的beta1超参数。 如论文所述，该数字应为 0.5 ngpu-可用的 GPU 数量。 如果为 0，则代码将在 CPU 模式下运行。 如果此数字大于 0，它将在该数量的 GPU 上运行 # Root directory for dataset dataroot = \"data/celeba\" # Number of workers for dataloader workers = 2 # Batch size during training batch_size = 128 # Spatial size of training images. All images will be resized to this # size using a transformer. image_size = 64 # Number of channels in the training images. For color images this is 3 nc = 3 # Size of z latent vector (i.e. size of generator input) nz = 100 # Size of feature maps in generator ngf = 64 # Size of feature maps in discriminator ndf = 64 # Number of training epochs num_epochs = 5 # Learning rate for optimizers lr = 0.0002 # Beta1 hyperparam for Adam optimizers beta1 = 0.5 # Number of GPUs available. Use 0 for CPU mode. ngpu = 1 数据 在本教程中，我们将使用 Celeb-A Faces 数据集，该数据集可在链接的站点或 Google 云端硬盘中下载。 数据集将下载为名为img_align_celeba.zip的文件。 下载完成后，创建一个名为celeba的目录，并将 zip 文件解压缩到该目录中。 然后，将此笔记本的dataroot输入设置为刚创建的celeba目录。 结果目录结构应为： /path/to/celeba -> img_align_celeba -> 188242.jpg -> 173822.jpg -> 284702.jpg -> 537394.jpg ... 这是重要的一步，因为我们将使用ImageFolder数据集类，该类要求数据集的根文件夹中有子目录。 现在，我们可以创建数据集，创建数据加载器，将设备设置为可以运行，并最终可视化一些训练数据。 # We can use an image folder dataset the way we have it setup. # Create the dataset dataset = dset.ImageFolder(root=dataroot, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])) # Create the dataloader dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers) # Decide which device we want to run on device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\") # Plot some training images real_batch = next(iter(dataloader)) plt.figure(figsize=(8,8)) plt.axis(\"off\") plt.title(\"Training Images\") plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) 实现 设置好输入参数并准备好数据集后，我们现在可以进入实现了。 我们将从权重初始化策略开始，然后详细讨论生成器，判别器，损失函数和训练循环。 权重初始化 在 DCGAN 论文中，作者指定所有模型权重均应从均值为 0，stdev = 0.02的正态分布中随机初始化。 weights_init函数采用已初始化的模型作为输入，并重新初始化所有卷积，卷积转置和批量归一化层以满足此标准。 初始化后立即将此函数应用于模型。 # custom weights initialization called on netG and netD def weights_init(m): classname = m.__class__.__name__ if classname.find('Conv') != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find('BatchNorm') != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) 生成器 生成器G用于将潜在空间向量（z）映射到数据空间。 由于我们的数据是图像，因此将z转换为数据空间意味着最终创建与训练图像大小相同的 RGB 图像（即3x64x64）。 在实践中，这是通过一系列跨步的二维卷积转置层来完成的，每个层都与 2d 批量规范层和 relu 激活配对。 生成器的输出通过 tanh 函数馈送，以使其返回到输入数据范围[-1,1]。 值得注意的是，在卷积转置层之后存在批量规范函数，因为这是 DCGAN 论文的关键贡献。 这些层有助于训练过程中的梯度流动。 DCGAN 纸生成的图像如下所示。 请注意，我们在输入部分中设置的输入（nz，ngf和nc）如何影响代码中的生成器架构。 nz是z输入向量的长度，ngf与通过生成器传播的特征映射的大小有关， nc是输出图像中的通道（对于 RGB 图像设置为 3）。 下面是生成器的代码。 # Generator Code class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is Z, going into a convolution nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # state size. (ngf*8) x 4 x 4 nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # state size. (ngf*4) x 8 x 8 nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # state size. (ngf*2) x 16 x 16 nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # state size. (ngf) x 32 x 32 nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), nn.Tanh() # state size. (nc) x 64 x 64 ) def forward(self, input): return self.main(input) 现在，我们可以实例化生成器并应用weights_init函数。 签出打印的模型以查看生成器对象的结构。 # Create the generator netG = Generator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == 'cuda') and (ngpu > 1): netG = nn.DataParallel(netG, list(range(ngpu))) # Apply the weights_init function to randomly initialize all weights # to mean=0, stdev=0.2. netG.apply(weights_init) # Print the model print(netG) 出： Generator( (main): Sequential( (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (11): ReLU(inplace=True) (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (13): Tanh() ) ) 判别器 如前所述，判别器D是一个二分类网络，将图像作为输入并输出标量概率，即输入图像是真实的（与假的相对）。 在这里，D拍摄3x64x64的输入图像，通过一系列的Conv2d，BatchNorm2d和LeakyReLU层对其进行处理，然后通过 Sigmoid 激活函数输出最终概率。 如果需要解决此问题，可以用更多层扩展此架构，但是使用跨步卷积，BatchNorm和LeakyReLU仍然很重要。 DCGAN 论文提到，使用跨步卷积而不是通过池化来进行下采样是一个好习惯，因为它可以让网络学习自己的池化特征。 批量规范和泄漏 ReLU 函数还可以促进健康的梯度流，这对于G和D的学习过程都是至关重要的。 鉴别码 class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is (nc) x 64 x 64 nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf) x 32 x 32 nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*2) x 16 x 16 nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*4) x 8 x 8 nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 8), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*8) x 4 x 4 nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, input): return self.main(input) 现在，与生成器一样，我们可以创建判别器，应用weights_init函数，并打印模型的结构。 # Create the Discriminator netD = Discriminator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == 'cuda') and (ngpu > 1): netD = nn.DataParallel(netD, list(range(ngpu))) # Apply the weights_init function to randomly initialize all weights # to mean=0, stdev=0.2. netD.apply(weights_init) # Print the model print(netD) 出： Discriminator( (main): Sequential( (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): LeakyReLU(negative_slope=0.2, inplace=True) (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (4): LeakyReLU(negative_slope=0.2, inplace=True) (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): LeakyReLU(negative_slope=0.2, inplace=True) (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (10): LeakyReLU(negative_slope=0.2, inplace=True) (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False) (12): Sigmoid() ) ) 损失函数和优化器 使用D和G设置，我们可以指定它们如何通过损失函数和优化器学习。 我们将使用在 PyTorch 中定义的二进制交叉熵损失（BCELoss）函数： 请注意，此函数如何提供目标函数中两个对数分量的计算（即log D(x)和log(1 - D(G(z)))）。 我们可以指定y输入使用 BCE 方程的哪一部分。 这是在即将到来的训练循环中完成的，但重要的是要了解我们如何仅通过更改y（即GT标签）即可选择希望计算的分量。 接下来，我们将实际标签定义为 1，将假标签定义为 0。这些标签将在计算D和G的损失时使用，这也是 GAN 原始论文中使用的惯例 。 最后，我们设置了两个单独的优化器，一个用于D，另一个用于G。 如 DCGAN 论文中所指定，这两个都是学习速度为 0.0002 和Beta1 = 0.5的 Adam 优化器。 为了跟踪生成器的学习进度，我们将生成一批固定的潜在向量，这些向量是从高斯分布（即fixed_noise）中提取的。 在训练循环中，我们将定期将此fixed_noise输入到G中，并且在迭代过程中，我们将看到图像形成于噪声之外。 # Initialize BCELoss function criterion = nn.BCELoss() # Create batch of latent vectors that we will use to visualize # the progression of the generator fixed_noise = torch.randn(64, nz, 1, 1, device=device) # Establish convention for real and fake labels during training real_label = 1. fake_label = 0. # Setup Adam optimizers for both G and D optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)) 训练 最后，既然我们已经定义了 GAN 框架的所有部分，我们就可以对其进行训练。 请注意，训练 GAN 某种程度上是一种艺术形式，因为不正确的超参数设置会导致模式崩溃，而对失败的原因几乎没有解释。 在这里，我们将严格遵循 Goodfellow 论文中的算法 1，同时遵守ganhacks中显示的一些最佳做法。 即，我们将“为真实和伪造构建不同的小批量”图像，并调整G的目标函数以最大化log D(G(z))。 训练分为两个主要部分。 第 1 部分更新了判别器，第 2 部分更新了生成器。 第 1 部分-训练判别器 回想一下，训练判别器的目的是最大程度地提高将给定输入正确分类为真实或伪造的可能性。 就古德费罗而言，我们希望“通过提高其随机梯度来更新判别器”。 实际上，我们要最大化log D(x) + log(1 - D(G(z))。 由于 ganhacks 提出了单独的小批量建议，因此我们将分两步进行计算。 首先，我们将从训练集中构造一批真实样本，向前通过D，计算损失（log D(x)），然后在向后通过中计算梯度。 其次，我们将使用当前生成器构造一批假样本，将这批伪造通过D，计算损失（log(1 - D(G(z)))），然后反向累积梯度。 现在，利用全批量和全批量的累积梯度，我们称之为判别器优化程序的一个步骤。 第 2 部分-训练生成器 如原始论文所述，我们希望通过最小化log(1 - D(G(z)))来训练生成器，以产生更好的假货。 如前所述，Goodfellow 证明这不能提供足够的梯度，尤其是在学习过程的早期。 作为解决方法，我们希望最大化log D(G(z))。 在代码中，我们通过以下步骤来实现此目的：将第 1 部分的生成器输出与判别器进行分类，使用实数标签GT计算G的损失，反向计算G的梯度，最后使用优化器步骤更新G的参数。 将真实标签用作损失函数的GT标签似乎是违反直觉的，但这使我们可以使用 BCELoss 的log(x)部分（而不是log(1 - x)部分），这正是我们想要的。 最后，我们将进行一些统计报告，并在每个周期结束时，将我们的fixed_noise批量推送到生成器中，以直观地跟踪G的训练进度。 报告的训练统计数据是： Loss_D-判别器损失，计算为所有真实批量和所有假批量的损失总和（log D(x) + log D(G(z))）。 Loss_G-生成器损失计算为log D(G(z)) D(x)-所有真实批量的判别器的平均输出（整个批量）。 这应该从接近 1 开始，然后在G变得更好时理论上收敛到 0.5。 想想这是为什么。 D(G(z))-所有假批量的平均判别器输出。 第一个数字在D更新之前，第二个数字在D更新之后。 这些数字应从 0 开始，并随着G的提高收敛到 0.5。 想想这是为什么。 注意：此步骤可能需要一段时间，具体取决于您运行了多少个周期以及是否从数据集中删除了一些数据。 # Training Loop # Lists to keep track of progress img_list = [] G_losses = [] D_losses = [] iters = 0 print(\"Starting Training Loop...\") # For each epoch for epoch in range(num_epochs): # For each batch in the dataloader for i, data in enumerate(dataloader, 0): ############################ # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z))) ########################### ## Train with all-real batch netD.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, dtype=torch.float, device=device) # Forward pass real batch through D output = netD(real_cpu).view(-1) # Calculate loss on all-real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Train with all-fake batch # Generate batch of latent vectors noise = torch.randn(b_size, nz, 1, 1, device=device) # Generate fake image batch with G fake = netG(noise) label.fill_(fake_label) # Classify all fake batch with D output = netD(fake.detach()).view(-1) # Calculate D's loss on the all-fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real + errD_fake # Update D optimizerD.step() ############################ # (2) Update G network: maximize log(D(G(z))) ########################### netG.zero_grad() label.fill_(real_label) # fake labels are real for generator cost # Since we just updated D, perform another forward pass of all-fake batch through D output = netD(fake).view(-1) # Calculate G's loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step() # Output training stats if i % 50 == 0: print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f' % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) # Save Losses for plotting later G_losses.append(errG.item()) D_losses.append(errD.item()) # Check how the generator is doing by saving G's output on fixed_noise if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)): with torch.no_grad(): fake = netG(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True)) iters += 1 出： Starting Training Loop... [0/5][0/1583] Loss_D: 1.9847 Loss_G: 5.5914 D(x): 0.6004 D(G(z)): 0.6680 / 0.0062 [0/5][50/1583] Loss_D: 0.7168 Loss_G: 35.7954 D(x): 0.7127 D(G(z)): 0.0000 / 0.0000 [0/5][100/1583] Loss_D: 0.0007 Loss_G: 28.2580 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000 [0/5][150/1583] Loss_D: 0.0001 Loss_G: 42.5731 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000 [0/5][200/1583] Loss_D: 0.0138 Loss_G: 42.3603 D(x): 0.9933 D(G(z)): 0.0000 / 0.0000 [0/5][250/1583] Loss_D: 0.0010 Loss_G: 42.2029 D(x): 0.9991 D(G(z)): 0.0000 / 0.0000 [0/5][300/1583] Loss_D: 0.0000 Loss_G: 41.9521 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][350/1583] Loss_D: 0.0000 Loss_G: 41.7962 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][400/1583] Loss_D: 0.0000 Loss_G: 41.6345 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][450/1583] Loss_D: 0.0000 Loss_G: 41.6058 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][500/1583] Loss_D: 0.0001 Loss_G: 41.6208 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000 [0/5][550/1583] Loss_D: 0.0000 Loss_G: 41.3979 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][600/1583] Loss_D: 0.0000 Loss_G: 41.2545 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][650/1583] Loss_D: 0.0000 Loss_G: 41.0200 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][700/1583] Loss_D: 0.0000 Loss_G: 39.6461 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][750/1583] Loss_D: 0.0000 Loss_G: 38.8834 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][800/1583] Loss_D: 0.0000 Loss_G: 38.5914 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][850/1583] Loss_D: 0.0000 Loss_G: 38.8209 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][900/1583] Loss_D: 0.0000 Loss_G: 38.9713 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][950/1583] Loss_D: 0.0000 Loss_G: 38.4995 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1000/1583] Loss_D: 0.0001 Loss_G: 38.5549 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000 [0/5][1050/1583] Loss_D: 0.0000 Loss_G: 39.1773 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1100/1583] Loss_D: 0.0000 Loss_G: 39.0142 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1150/1583] Loss_D: 0.0000 Loss_G: 38.6368 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1200/1583] Loss_D: 0.0000 Loss_G: 38.7159 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1250/1583] Loss_D: 0.0000 Loss_G: 38.7660 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1300/1583] Loss_D: 0.0000 Loss_G: 38.5522 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1350/1583] Loss_D: 0.0001 Loss_G: 38.6703 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000 [0/5][1400/1583] Loss_D: 0.0000 Loss_G: 38.5487 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1450/1583] Loss_D: 0.0000 Loss_G: 38.0378 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1500/1583] Loss_D: 0.0000 Loss_G: 38.1258 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [0/5][1550/1583] Loss_D: 0.0000 Loss_G: 38.3473 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][0/1583] Loss_D: 0.0000 Loss_G: 37.8825 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][50/1583] Loss_D: 0.0000 Loss_G: 38.2248 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][100/1583] Loss_D: 0.0000 Loss_G: 38.2204 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][150/1583] Loss_D: 0.0000 Loss_G: 38.0967 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][200/1583] Loss_D: 0.0000 Loss_G: 38.0669 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][250/1583] Loss_D: 0.0000 Loss_G: 37.4736 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][300/1583] Loss_D: 0.0000 Loss_G: 37.0766 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][350/1583] Loss_D: 0.0000 Loss_G: 36.6055 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 [1/5][400/1583] Loss_D: 2.5403 Loss_G: 12.8251 D(x): 0.8672 D(G(z)): 0.8088 / 0.0000 [1/5][450/1583] Loss_D: 1.3779 Loss_G: 2.0631 D(x): 0.5850 D(G(z)): 0.4734 / 0.1820 [1/5][500/1583] Loss_D: 1.0299 Loss_G: 2.4048 D(x): 0.5165 D(G(z)): 0.1698 / 0.1333 [1/5][550/1583] Loss_D: 1.4922 Loss_G: 3.2383 D(x): 0.5854 D(G(z)): 0.4773 / 0.0888 [1/5][600/1583] Loss_D: 0.9283 Loss_G: 1.8533 D(x): 0.6231 D(G(z)): 0.2962 / 0.2153 [1/5][650/1583] Loss_D: 0.8065 Loss_G: 2.9684 D(x): 0.6684 D(G(z)): 0.2624 / 0.0715 [1/5][700/1583] Loss_D: 0.6909 Loss_G: 2.8746 D(x): 0.7910 D(G(z)): 0.3013 / 0.0819 [1/5][750/1583] Loss_D: 1.3242 Loss_G: 2.5236 D(x): 0.7183 D(G(z)): 0.5300 / 0.1090 [1/5][800/1583] Loss_D: 1.0871 Loss_G: 2.0203 D(x): 0.4993 D(G(z)): 0.1716 / 0.1727 [1/5][850/1583] Loss_D: 1.7561 Loss_G: 4.9674 D(x): 0.8542 D(G(z)): 0.7052 / 0.0133 [1/5][900/1583] Loss_D: 0.8294 Loss_G: 2.5024 D(x): 0.6913 D(G(z)): 0.2910 / 0.1178 [1/5][950/1583] Loss_D: 0.9390 Loss_G: 2.2087 D(x): 0.5508 D(G(z)): 0.1638 / 0.1617 [1/5][1000/1583] Loss_D: 1.8202 Loss_G: 1.2178 D(x): 0.2535 D(G(z)): 0.0684 / 0.3527 [1/5][1050/1583] Loss_D: 0.9816 Loss_G: 3.7976 D(x): 0.7310 D(G(z)): 0.3944 / 0.0343 [1/5][1100/1583] Loss_D: 0.9798 Loss_G: 2.0990 D(x): 0.5963 D(G(z)): 0.2328 / 0.1660 [1/5][1150/1583] Loss_D: 0.7173 Loss_G: 2.7879 D(x): 0.6385 D(G(z)): 0.1424 / 0.1057 [1/5][1200/1583] Loss_D: 0.8903 Loss_G: 2.3547 D(x): 0.7371 D(G(z)): 0.3589 / 0.1251 [1/5][1250/1583] Loss_D: 0.6137 Loss_G: 2.1031 D(x): 0.7491 D(G(z)): 0.2062 / 0.1588 [1/5][1300/1583] Loss_D: 1.0179 Loss_G: 5.0280 D(x): 0.7465 D(G(z)): 0.4325 / 0.0129 [1/5][1350/1583] Loss_D: 0.7131 Loss_G: 3.6670 D(x): 0.7931 D(G(z)): 0.3270 / 0.0398 [1/5][1400/1583] Loss_D: 1.0736 Loss_G: 4.2392 D(x): 0.8172 D(G(z)): 0.4861 / 0.0351 [1/5][1450/1583] Loss_D: 0.6050 Loss_G: 2.6052 D(x): 0.7590 D(G(z)): 0.2240 / 0.1019 [1/5][1500/1583] Loss_D: 1.3370 Loss_G: 1.9105 D(x): 0.3786 D(G(z)): 0.0405 / 0.2013 [1/5][1550/1583] Loss_D: 0.6698 Loss_G: 2.3040 D(x): 0.6444 D(G(z)): 0.1071 / 0.1372 [2/5][0/1583] Loss_D: 1.3043 Loss_G: 2.1213 D(x): 0.4073 D(G(z)): 0.0423 / 0.1682 [2/5][50/1583] Loss_D: 1.3636 Loss_G: 3.4322 D(x): 0.7959 D(G(z)): 0.6129 / 0.0510 [2/5][100/1583] Loss_D: 0.8047 Loss_G: 3.4262 D(x): 0.9067 D(G(z)): 0.4371 / 0.0536 [2/5][150/1583] Loss_D: 0.7103 Loss_G: 2.4974 D(x): 0.6212 D(G(z)): 0.0862 / 0.1273 [2/5][200/1583] Loss_D: 0.8335 Loss_G: 2.9292 D(x): 0.7340 D(G(z)): 0.3396 / 0.0772 [2/5][250/1583] Loss_D: 1.4766 Loss_G: 1.4532 D(x): 0.3469 D(G(z)): 0.0140 / 0.3162 [2/5][300/1583] Loss_D: 0.8063 Loss_G: 2.5363 D(x): 0.6939 D(G(z)): 0.2714 / 0.1160 [2/5][350/1583] Loss_D: 2.4655 Loss_G: 1.7710 D(x): 0.1625 D(G(z)): 0.0049 / 0.2345 [2/5][400/1583] Loss_D: 0.9256 Loss_G: 1.4698 D(x): 0.5101 D(G(z)): 0.1192 / 0.2926 [2/5][450/1583] Loss_D: 0.7932 Loss_G: 3.1267 D(x): 0.8831 D(G(z)): 0.4330 / 0.0657 [2/5][500/1583] Loss_D: 1.0515 Loss_G: 1.8415 D(x): 0.4922 D(G(z)): 0.0817 / 0.2372 [2/5][550/1583] Loss_D: 1.1575 Loss_G: 2.3904 D(x): 0.8286 D(G(z)): 0.5113 / 0.1394 [2/5][600/1583] Loss_D: 0.8667 Loss_G: 4.0253 D(x): 0.8805 D(G(z)): 0.4499 / 0.0329 [2/5][650/1583] Loss_D: 0.9943 Loss_G: 3.0625 D(x): 0.8224 D(G(z)): 0.4700 / 0.0678 [2/5][700/1583] Loss_D: 0.7634 Loss_G: 3.7297 D(x): 0.7855 D(G(z)): 0.3507 / 0.0369 [2/5][750/1583] Loss_D: 0.6280 Loss_G: 2.7439 D(x): 0.7664 D(G(z)): 0.2518 / 0.0897 [2/5][800/1583] Loss_D: 0.9011 Loss_G: 1.3725 D(x): 0.5495 D(G(z)): 0.1341 / 0.3033 [2/5][850/1583] Loss_D: 0.4595 Loss_G: 3.0410 D(x): 0.8186 D(G(z)): 0.1808 / 0.0721 [2/5][900/1583] Loss_D: 0.8331 Loss_G: 1.3725 D(x): 0.5696 D(G(z)): 0.1528 / 0.3128 [2/5][950/1583] Loss_D: 1.2701 Loss_G: 4.4360 D(x): 0.9365 D(G(z)): 0.6218 / 0.0226 [2/5][1000/1583] Loss_D: 0.5165 Loss_G: 3.2817 D(x): 0.7543 D(G(z)): 0.1460 / 0.0651 [2/5][1050/1583] Loss_D: 0.5562 Loss_G: 2.5533 D(x): 0.8034 D(G(z)): 0.2385 / 0.1047 [2/5][1100/1583] Loss_D: 0.9842 Loss_G: 3.5247 D(x): 0.7936 D(G(z)): 0.4511 / 0.0446 [2/5][1150/1583] Loss_D: 0.6793 Loss_G: 3.2208 D(x): 0.8038 D(G(z)): 0.3133 / 0.0571 [2/5][1200/1583] Loss_D: 1.8110 Loss_G: 5.4461 D(x): 0.8337 D(G(z)): 0.7185 / 0.0090 [2/5][1250/1583] Loss_D: 0.6310 Loss_G: 2.8066 D(x): 0.7859 D(G(z)): 0.2644 / 0.0822 [2/5][1300/1583] Loss_D: 0.6009 Loss_G: 1.6727 D(x): 0.6759 D(G(z)): 0.1297 / 0.2422 [2/5][1350/1583] Loss_D: 0.5156 Loss_G: 3.5893 D(x): 0.8552 D(G(z)): 0.2686 / 0.0385 [2/5][1400/1583] Loss_D: 0.7672 Loss_G: 1.0321 D(x): 0.5755 D(G(z)): 0.0938 / 0.4195 [2/5][1450/1583] Loss_D: 0.6583 Loss_G: 2.0611 D(x): 0.6727 D(G(z)): 0.1675 / 0.1591 [2/5][1500/1583] Loss_D: 1.2956 Loss_G: 3.7047 D(x): 0.9324 D(G(z)): 0.6345 / 0.0479 [2/5][1550/1583] Loss_D: 0.8555 Loss_G: 3.0119 D(x): 0.8243 D(G(z)): 0.4237 / 0.0696 [3/5][0/1583] Loss_D: 0.7295 Loss_G: 2.0605 D(x): 0.7051 D(G(z)): 0.2466 / 0.1671 [3/5][50/1583] Loss_D: 0.6551 Loss_G: 3.0267 D(x): 0.8502 D(G(z)): 0.3419 / 0.0676 [3/5][100/1583] Loss_D: 0.9209 Loss_G: 1.3069 D(x): 0.5238 D(G(z)): 0.1032 / 0.3367 [3/5][150/1583] Loss_D: 0.6289 Loss_G: 1.8684 D(x): 0.6835 D(G(z)): 0.1555 / 0.1994 [3/5][200/1583] Loss_D: 1.0600 Loss_G: 1.3343 D(x): 0.4512 D(G(z)): 0.0575 / 0.3259 [3/5][250/1583] Loss_D: 0.7251 Loss_G: 1.7242 D(x): 0.6128 D(G(z)): 0.1340 / 0.2269 [3/5][300/1583] Loss_D: 0.7097 Loss_G: 1.7072 D(x): 0.7143 D(G(z)): 0.2623 / 0.2238 [3/5][350/1583] Loss_D: 0.8045 Loss_G: 2.7455 D(x): 0.7958 D(G(z)): 0.3825 / 0.0901 [3/5][400/1583] Loss_D: 0.8351 Loss_G: 1.6116 D(x): 0.5394 D(G(z)): 0.1106 / 0.2425 [3/5][450/1583] Loss_D: 1.4829 Loss_G: 0.5346 D(x): 0.3523 D(G(z)): 0.0987 / 0.6289 [3/5][500/1583] Loss_D: 0.6972 Loss_G: 2.1915 D(x): 0.7656 D(G(z)): 0.2987 / 0.1450 [3/5][550/1583] Loss_D: 0.7369 Loss_G: 1.7250 D(x): 0.6402 D(G(z)): 0.1899 / 0.2224 [3/5][600/1583] Loss_D: 0.8170 Loss_G: 2.6806 D(x): 0.7843 D(G(z)): 0.3880 / 0.0929 [3/5][650/1583] Loss_D: 1.1531 Loss_G: 0.9077 D(x): 0.4340 D(G(z)): 0.1224 / 0.4550 [3/5][700/1583] Loss_D: 0.8751 Loss_G: 1.0230 D(x): 0.5587 D(G(z)): 0.1808 / 0.4021 [3/5][750/1583] Loss_D: 0.7169 Loss_G: 2.1268 D(x): 0.6690 D(G(z)): 0.2219 / 0.1588 [3/5][800/1583] Loss_D: 0.9772 Loss_G: 3.1279 D(x): 0.8451 D(G(z)): 0.5081 / 0.0632 [3/5][850/1583] Loss_D: 0.6574 Loss_G: 1.9605 D(x): 0.7010 D(G(z)): 0.2120 / 0.1775 [3/5][900/1583] Loss_D: 0.6153 Loss_G: 2.8981 D(x): 0.8399 D(G(z)): 0.3197 / 0.0697 [3/5][950/1583] Loss_D: 0.9155 Loss_G: 1.1091 D(x): 0.5482 D(G(z)): 0.1730 / 0.3799 [3/5][1000/1583] Loss_D: 0.9873 Loss_G: 3.9150 D(x): 0.8838 D(G(z)): 0.5423 / 0.0284 [3/5][1050/1583] Loss_D: 0.8369 Loss_G: 2.1366 D(x): 0.8039 D(G(z)): 0.4067 / 0.1533 [3/5][1100/1583] Loss_D: 0.9522 Loss_G: 3.4744 D(x): 0.8732 D(G(z)): 0.5049 / 0.0412 [3/5][1150/1583] Loss_D: 0.6371 Loss_G: 2.1278 D(x): 0.7648 D(G(z)): 0.2672 / 0.1424 [3/5][1200/1583] Loss_D: 1.0349 Loss_G: 2.7710 D(x): 0.7604 D(G(z)): 0.4512 / 0.0920 [3/5][1250/1583] Loss_D: 0.9350 Loss_G: 2.7946 D(x): 0.8007 D(G(z)): 0.4649 / 0.0805 [3/5][1300/1583] Loss_D: 0.7655 Loss_G: 2.7838 D(x): 0.7965 D(G(z)): 0.3724 / 0.0803 [3/5][1350/1583] Loss_D: 0.7623 Loss_G: 2.2647 D(x): 0.7979 D(G(z)): 0.3641 / 0.1414 [3/5][1400/1583] Loss_D: 0.9361 Loss_G: 3.1341 D(x): 0.8601 D(G(z)): 0.4938 / 0.0628 [3/5][1450/1583] Loss_D: 0.7966 Loss_G: 3.1544 D(x): 0.8568 D(G(z)): 0.4211 / 0.0623 [3/5][1500/1583] Loss_D: 1.0768 Loss_G: 3.8304 D(x): 0.8364 D(G(z)): 0.5348 / 0.0353 [3/5][1550/1583] Loss_D: 0.8528 Loss_G: 3.3978 D(x): 0.8824 D(G(z)): 0.4788 / 0.0491 [4/5][0/1583] Loss_D: 0.8361 Loss_G: 1.9086 D(x): 0.6756 D(G(z)): 0.2975 / 0.1872 [4/5][50/1583] Loss_D: 0.7666 Loss_G: 2.3647 D(x): 0.7698 D(G(z)): 0.3487 / 0.1232 [4/5][100/1583] Loss_D: 0.7536 Loss_G: 1.6556 D(x): 0.6398 D(G(z)): 0.2084 / 0.2423 [4/5][150/1583] Loss_D: 0.8390 Loss_G: 1.7737 D(x): 0.6400 D(G(z)): 0.2714 / 0.2181 [4/5][200/1583] Loss_D: 0.8608 Loss_G: 2.5683 D(x): 0.7898 D(G(z)): 0.4126 / 0.1009 [4/5][250/1583] Loss_D: 0.8651 Loss_G: 1.8416 D(x): 0.6033 D(G(z)): 0.2312 / 0.1954 [4/5][300/1583] Loss_D: 0.8790 Loss_G: 1.2224 D(x): 0.5099 D(G(z)): 0.0960 / 0.3501 [4/5][350/1583] Loss_D: 2.0809 Loss_G: 0.5006 D(x): 0.1907 D(G(z)): 0.0415 / 0.6501 [4/5][400/1583] Loss_D: 1.0178 Loss_G: 2.6912 D(x): 0.7134 D(G(z)): 0.4299 / 0.0977 [4/5][450/1583] Loss_D: 0.7773 Loss_G: 1.5577 D(x): 0.6859 D(G(z)): 0.2705 / 0.2527 [4/5][500/1583] Loss_D: 1.0217 Loss_G: 2.8968 D(x): 0.8227 D(G(z)): 0.5103 / 0.0755 [4/5][550/1583] Loss_D: 0.6428 Loss_G: 2.8346 D(x): 0.8293 D(G(z)): 0.3290 / 0.0793 [4/5][600/1583] Loss_D: 1.7683 Loss_G: 4.1924 D(x): 0.9236 D(G(z)): 0.7656 / 0.0211 [4/5][650/1583] Loss_D: 0.8692 Loss_G: 2.2491 D(x): 0.7046 D(G(z)): 0.3386 / 0.1336 [4/5][700/1583] Loss_D: 0.8933 Loss_G: 1.5814 D(x): 0.6256 D(G(z)): 0.2963 / 0.2476 [4/5][750/1583] Loss_D: 1.2154 Loss_G: 2.6798 D(x): 0.8082 D(G(z)): 0.5792 / 0.0862 [4/5][800/1583] Loss_D: 0.7252 Loss_G: 1.6059 D(x): 0.6257 D(G(z)): 0.1717 / 0.2486 [4/5][850/1583] Loss_D: 0.6888 Loss_G: 2.4141 D(x): 0.7470 D(G(z)): 0.2786 / 0.1207 [4/5][900/1583] Loss_D: 1.0490 Loss_G: 1.1737 D(x): 0.4731 D(G(z)): 0.1746 / 0.3528 [4/5][950/1583] Loss_D: 1.1517 Loss_G: 0.5954 D(x): 0.4083 D(G(z)): 0.0727 / 0.5876 [4/5][1000/1583] Loss_D: 0.7451 Loss_G: 2.1440 D(x): 0.7385 D(G(z)): 0.3118 / 0.1455 [4/5][1050/1583] Loss_D: 1.2439 Loss_G: 0.8178 D(x): 0.3806 D(G(z)): 0.0852 / 0.4825 [4/5][1100/1583] Loss_D: 0.8468 Loss_G: 3.3432 D(x): 0.8220 D(G(z)): 0.4289 / 0.0484 [4/5][1150/1583] Loss_D: 0.9824 Loss_G: 0.8542 D(x): 0.4712 D(G(z)): 0.1120 / 0.4808 [4/5][1200/1583] Loss_D: 1.1658 Loss_G: 3.3930 D(x): 0.8771 D(G(z)): 0.5939 / 0.0450 [4/5][1250/1583] Loss_D: 0.8152 Loss_G: 1.3158 D(x): 0.5988 D(G(z)): 0.1721 / 0.3111 [4/5][1300/1583] Loss_D: 0.7013 Loss_G: 2.0752 D(x): 0.6751 D(G(z)): 0.2173 / 0.1596 [4/5][1350/1583] Loss_D: 0.8809 Loss_G: 3.0340 D(x): 0.8292 D(G(z)): 0.4574 / 0.0636 [4/5][1400/1583] Loss_D: 0.7911 Loss_G: 2.7713 D(x): 0.7982 D(G(z)): 0.3830 / 0.0829 [4/5][1450/1583] Loss_D: 1.0299 Loss_G: 2.8774 D(x): 0.7987 D(G(z)): 0.4941 / 0.0761 [4/5][1500/1583] Loss_D: 0.8572 Loss_G: 2.5340 D(x): 0.7273 D(G(z)): 0.3717 / 0.1009 [4/5][1550/1583] Loss_D: 0.8135 Loss_G: 1.6428 D(x): 0.5799 D(G(z)): 0.1693 / 0.2267 结果 最后，让我们看看我们是如何做到的。 在这里，我们将看三个不同的结果。 首先，我们将了解D和G的损失在训练过程中如何变化。 其次，我们将在每个周期将G的输出显示为fixed_noise批量。 第三，我们将查看一批真实数据以及来自G的一批伪数据。 损失与训练迭代 下面是D&G的损失与训练迭代的关系图。 plt.figure(figsize=(10,5)) plt.title(\"Generator and Discriminator Loss During Training\") plt.plot(G_losses,label=\"G\") plt.plot(D_losses,label=\"D\") plt.xlabel(\"iterations\") plt.ylabel(\"Loss\") plt.legend() plt.show() 可视化G的进度 请记住，在每次训练之后，我们如何将生成器的输出保存为fixed_noise批量。 现在，我们可以用动画形象化G的训练进度。 按下播放按钮开始动画。 #%%capture fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True) HTML(ani.to_jshtml()) 真实图像和伪图像 最后，让我们并排查看一些真实图像和伪图像。 # Grab a batch of real images from the dataloader real_batch = next(iter(dataloader)) # Plot the real images plt.figure(figsize=(15,15)) plt.subplot(1,2,1) plt.axis(\"off\") plt.title(\"Real Images\") plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0))) # Plot the fake images from the last epoch plt.subplot(1,2,2) plt.axis(\"off\") plt.title(\"Fake Images\") plt.imshow(np.transpose(img_list[-1],(1,2,0))) plt.show() 下一步去哪里 我们已经走到了旅程的尽头，但是您可以从这里到达几个地方。 你可以： 训练更长的时间，看看效果如何 修改此模型以采用其他数据集，并可能更改图像的大小和模型架构 查看其他一些不错的 GAN 项目 创建可生成音乐的 GAN 脚本的总运行时间：（29 分钟 17.480 秒） 下载 Python 源码：dcgan_faces_tutorial.py 下载 Jupyter 笔记本：dcgan_faces_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"23.html":{"url":"23.html","title":"音频","keywords":"","body":"音频 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"24.html":{"url":"24.html","title":"音频 I/O 和torchaudio的预处理","keywords":"","body":"音频 I/O 和torchaudio的预处理 原文：https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html PyTorch 是一个开源深度学习平台，提供了从研究原型到具有 GPU 支持的生产部署的无缝路径。 解决机器学习问题的重要工作是准备数据。 torchaudio充分利用了 PyTorch 的 GPU 支持，并提供了许多工具来简化数据加载并使其更具可读性。 在本教程中，我们将看到如何从简单的数据集中加载和预处理数据。 请访问音频 I/O 和torchaudio的预处理，以了解更多信息。 对于本教程，请确保已安装matplotlib包，以便于查看。 # Uncomment the following line to run in Google Colab # !pip install torchaudio import torch import torchaudio import requests import matplotlib.pyplot as plt 打开文件 torchaudio还支持以 wav 和 mp3 格式加载声音文件。 我们将波形称为原始音频信号。 url = \"https://pytorch.org/tutorials/_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\" r = requests.get(url) with open('steam-train-whistle-daniel_simon-converted-from-mp3.wav', 'wb') as f: f.write(r.content) filename = \"steam-train-whistle-daniel_simon-converted-from-mp3.wav\" waveform, sample_rate = torchaudio.load(filename) print(\"Shape of waveform: {}\".format(waveform.size())) print(\"Sample rate of waveform: {}\".format(sample_rate)) plt.figure() plt.plot(waveform.t().numpy()) 出： Shape of waveform: torch.Size([2, 276858]) Sample rate of waveform: 44100 在torchaudio中加载文件时，可以选择指定后端以通过torchaudio.set_audio_backend使用 SoX 或 SoundFile 。 这些后端在需要时会延迟加载。 torchaudio还使 JIT 编译对于函数是可选的，并在可能的情况下使用nn.Module。 转换 torchaudio支持不断增长的转换列表。 Resample：将波形重采样为其他采样率。 Spectrogram：从波形创建频谱图。 GriffinLim：使用 Griffin-Lim 变换从线性比例幅度谱图计算波形。 ComputeDeltas：计算张量（通常是声谱图）的增量系数。 ComplexNorm：计算复数张量的范数。 MelScale：使用转换矩阵将正常 STFT 转换为 Mel 频率 STFT。 AmplitudeToDB：这将频谱图从功率/振幅标度变为分贝标度。 MFCC：从波形创建梅尔频率倒谱系数。 MelSpectrogram：使用 PyTorch 中的 STFT 特征从波形创建 MEL 频谱图。 MuLawEncoding：基于 mu-law 压扩对波形进行编码。 MuLawDecoding：解码 mu-law 编码波形。 TimeStretch：在不更改给定速率的音调的情况下，及时拉伸频谱图。 FrequencyMasking：在频域中对频谱图应用屏蔽。 TimeMasking：在时域中对频谱图应用屏蔽。 每个变换都支持批量：您可以对单个原始音频信号或频谱图或许多相同形状的信号执行变换。 由于所有变换都是nn.Modules或jit.ScriptModules，因此它们可以随时用作神经网络的一部分。 首先，我们可以以对数刻度查看频谱图的对数。 specgram = torchaudio.transforms.Spectrogram()(waveform) print(\"Shape of spectrogram: {}\".format(specgram.size())) plt.figure() plt.imshow(specgram.log2()[0,:,:].numpy(), cmap='gray') 出： Shape of spectrogram: torch.Size([2, 201, 1385]) 或者我们可以以对数刻度查看梅尔光谱图。 specgram = torchaudio.transforms.MelSpectrogram()(waveform) print(\"Shape of spectrogram: {}\".format(specgram.size())) plt.figure() p = plt.imshow(specgram.log2()[0,:,:].detach().numpy(), cmap='gray') 出： Shape of spectrogram: torch.Size([2, 128, 1385]) 我们可以一次对一个通道重新采样波形。 new_sample_rate = sample_rate/10 # Since Resample applies to a single channel, we resample first channel here channel = 0 transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1)) print(\"Shape of transformed waveform: {}\".format(transformed.size())) plt.figure() plt.plot(transformed[0,:].numpy()) 出： Shape of transformed waveform: torch.Size([1, 27686]) 作为变换的另一个示例，我们可以基于 Mu-Law 编码对信号进行编码。 但是要这样做，我们需要信号在 -1 和 1 之间。由于张量只是常规的 PyTorch 张量，因此我们可以在其上应用标准运算符。 # Let's check if the tensor is in the interval [-1,1] print(\"Min of waveform: {}\\nMax of waveform: {}\\nMean of waveform: {}\".format(waveform.min(), waveform.max(), waveform.mean())) 出： Min of waveform: -0.572845458984375 Max of waveform: 0.575958251953125 Mean of waveform: 9.293758921558037e-05 由于波形已经在 -1 和 1 之间，因此我们不需要对其进行归一化。 def normalize(tensor): # Subtract the mean, and scale to the interval [-1,1] tensor_minusmean = tensor - tensor.mean() return tensor_minusmean/tensor_minusmean.abs().max() # Let's normalize to the full interval [-1,1] # waveform = normalize(waveform) 让我们对波形进行编码。 transformed = torchaudio.transforms.MuLawEncoding()(waveform) print(\"Shape of transformed waveform: {}\".format(transformed.size())) plt.figure() plt.plot(transformed[0,:].numpy()) 出： Shape of transformed waveform: torch.Size([2, 276858]) 现在解码。 reconstructed = torchaudio.transforms.MuLawDecoding()(transformed) print(\"Shape of recovered waveform: {}\".format(reconstructed.size())) plt.figure() plt.plot(reconstructed[0,:].numpy()) 出： Shape of recovered waveform: torch.Size([2, 276858]) 我们最终可以将原始波形与其重构版本进行比较。 # Compute median relative difference err = ((waveform-reconstructed).abs() / waveform.abs()).median() print(\"Median relative difference between original and MuLaw reconstucted signals: {:.2%}\".format(err)) 出： Median relative difference between original and MuLaw reconstucted signals: 1.28% 函数 上面看到的转换依赖于较低级别的无状态函数进行计算。 这些函数在torchaudio.functional下可用。 完整列表在此处，包括： istft：短期傅立叶逆变换。 gain：对整个波形进行放大或衰减。 dither：增加以特定位深度存储的音频的动态范围。 compute_deltas：计算张量的增量系数。 equalizer_biquad：设计双二阶峰值均衡器过滤器并执行滤波。 lowpass_biquad：设计双二阶低通过滤器并执行滤波。 highpass_biquad：设计双二阶高通过滤器并执行滤波。 例如，让我们尝试mu_law_encoding函数： mu_law_encoding_waveform = torchaudio.functional.mu_law_encoding(waveform, quantization_channels=256) print(\"Shape of transformed waveform: {}\".format(mu_law_encoding_waveform.size())) plt.figure() plt.plot(mu_law_encoding_waveform[0,:].numpy()) 出： Shape of transformed waveform: torch.Size([2, 276858]) 您可以看到torchaudio.functional.mu_law_encoding的输出与torchaudio.transforms.MuLawEncoding的输出相同。 现在让我们尝试其他一些函数，并可视化其输出。 通过我们的频谱图，我们可以计算出其增量： computed = torchaudio.functional.compute_deltas(specgram.contiguous(), win_length=3) print(\"Shape of computed deltas: {}\".format(computed.shape)) plt.figure() plt.imshow(computed.log2()[0,:,:].detach().numpy(), cmap='gray') 出： Shape of computed deltas: torch.Size([2, 128, 1385]) 我们可以获取原始波形并对其应用不同的效果。 gain_waveform = torchaudio.functional.gain(waveform, gain_db=5.0) print(\"Min of gain_waveform: {}\\nMax of gain_waveform: {}\\nMean of gain_waveform: {}\".format(gain_waveform.min(), gain_waveform.max(), gain_waveform.mean())) dither_waveform = torchaudio.functional.dither(waveform) print(\"Min of dither_waveform: {}\\nMax of dither_waveform: {}\\nMean of dither_waveform: {}\".format(dither_waveform.min(), dither_waveform.max(), dither_waveform.mean())) 出： Min of gain_waveform: -1.0186792612075806 Max of gain_waveform: 1.024214744567871 Mean of gain_waveform: 0.00016526899707969278 Min of dither_waveform: -0.572784423828125 Max of dither_waveform: 0.575927734375 Mean of dither_waveform: 0.00010744280007202178 torchaudio.functional中函数的另一个示例是将过滤器应用于我们的波形。 将低通双二阶过滤器应用于我们的波形，将输出修改了频率信号的新波形。 lowpass_waveform = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=3000) print(\"Min of lowpass_waveform: {}\\nMax of lowpass_waveform: {}\\nMean of lowpass_waveform: {}\".format(lowpass_waveform.min(), lowpass_waveform.max(), lowpass_waveform.mean())) plt.figure() plt.plot(lowpass_waveform.t().numpy()) 出： Min of lowpass_waveform: -0.5595059990882874 Max of lowpass_waveform: 0.5595012307167053 Mean of lowpass_waveform: 9.293757466366515e-05 我们还可以使用高通双二阶过滤器可视化波形。 highpass_waveform = torchaudio.functional.highpass_biquad(waveform, sample_rate, cutoff_freq=2000) print(\"Min of highpass_waveform: {}\\nMax of highpass_waveform: {}\\nMean of highpass_waveform: {}\".format(highpass_waveform.min(), highpass_waveform.max(), highpass_waveform.mean())) plt.figure() plt.plot(highpass_waveform.t().numpy()) 出： Min of highpass_waveform: -0.11269102990627289 Max of highpass_waveform: 0.10451897978782654 Mean of highpass_waveform: 1.8138147234170177e-11 从 Kaldi 迁移到torchaudio 用户可能熟悉 Kaldi （一种用于语音识别的工具包）。 torchaudio提供与torchaudio.kaldi_io中的兼容性。 实际上，它可以通过以下方式从 kaldi scp 或 ark 文件或流中读取： read_vec_int_ark read_vec_flt_scp read_vec_flt_arkfile/流 read_mat_scp read_mat_ark torchaudio为spectrogram，fbank，mfcc和 Kaldi 提供兼容的转换。 resample_waveform受益于 GPU 支持，有关更多信息，请参见此处。 n_fft = 400.0 frame_length = n_fft / sample_rate * 1000.0 frame_shift = frame_length / 2.0 params = { \"channel\": 0, \"dither\": 0.0, \"window_type\": \"hanning\", \"frame_length\": frame_length, \"frame_shift\": frame_shift, \"remove_dc_offset\": False, \"round_to_power_of_two\": False, \"sample_frequency\": sample_rate, } specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params) print(\"Shape of spectrogram: {}\".format(specgram.size())) plt.figure() plt.imshow(specgram.t().numpy(), cmap='gray') 出： Shape of spectrogram: torch.Size([1383, 201]) 我们还支持根据波形计算过滤器组特征，以匹配 Kaldi 的实现。 fbank = torchaudio.compliance.kaldi.fbank(waveform, **params) print(\"Shape of fbank: {}\".format(fbank.size())) plt.figure() plt.imshow(fbank.t().numpy(), cmap='gray') 出： Shape of fbank: torch.Size([1383, 23]) 您可以从原始音频信号创建梅尔频率倒谱系数，这与 Kaldi 的 compute-mfcc-feats 的输入/输出相匹配。 mfcc = torchaudio.compliance.kaldi.mfcc(waveform, **params) print(\"Shape of mfcc: {}\".format(mfcc.size())) plt.figure() plt.imshow(mfcc.t().numpy(), cmap='gray') 出： Shape of mfcc: torch.Size([1383, 13]) 可用数据集 如果您不想创建自己的数据集来训练模型，则torchaudio提供了统一的数据集接口。 该接口支持将文件延迟加载到内存，下载和提取函数以及数据集以构建模型。 当前支持的数据集torchaudio为： VCTK：109 位以英语为母语的母语者说的语音数据，带有各种重音（在此处详细了解）。 Yesno：一个人在希伯来语中说是或否的 60 张唱片； 每个记录长 8 个字（在此处了解更多）。 Common Voice：开源的多语言语音数据集，任何人都可以用来训练启用语音的应用（在此处了解更多）。 LibriSpeech：阅读英语语音的大型语料库（1000 小时）（在此处详细了解）。 yesno_data = torchaudio.datasets.YESNO('./', download=True) # A data point in Yesno is a tuple (waveform, sample_rate, labels) where labels is a list of integers with 1 for yes and 0 for no. # Pick data point number 3 to see an example of the the yesno_data: n = 3 waveform, sample_rate, labels = yesno_data[n] print(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels)) plt.figure() plt.plot(waveform.t().numpy()) 出： Waveform: tensor([[ 3.0518e-05, 6.1035e-05, 3.0518e-05, ..., -1.8311e-04, 4.2725e-04, 6.7139e-04]]) Sample rate: 8000 Labels: [0, 0, 1, 0, 0, 0, 1, 0] 现在，每当您从数据集中请求声音文件时，仅当您请求声音文件时，它才会加载到内存中。 这意味着，数据集仅加载所需的项目并将其保留在内存中，并保存在内存中。 总结 我们使用示例原始音频信号或波形来说明如何使用torchaudio打开音频文件，以及如何对该波形进行预处理，变换和应用函数。 我们还演示了如何使用熟悉的 Kaldi 函数以及如何利用内置数据集构建模型。 鉴于torchaudio是基于 PyTorch 构建的，因此这些技术可以在利用 GPU 的同时，用作语音识别等更高级音频应用的构建块。 脚本的总运行时间：（0 分钟 18.821 秒） 下载 Python 源码：audio_preprocessing_tutorial.py 下载 Jupyter 笔记本：audio_preprocessing_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"25.html":{"url":"25.html","title":"使用torchaudio的语音命令识别","keywords":"","body":"使用torchaudio的语音命令识别 原文：https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html 本教程将向您展示如何正确设置音频数据集的格式，然后在数据集上训练/测试音频分类器网络。 Colab 提供了 GPU 选项。 在菜单选项卡中，选择“运行系统”，然后选择“更改运行系统类型”。 在随后的弹出窗口中，您可以选择 GPU。 更改之后，运行时应自动重新启动（这意味着来自已执行单元的信息会消失）。 首先，让我们导入常见的 Torch 包，例如torchaudio，可以按照网站上的说明进行安装。 # Uncomment the following line to run in Google Colab # CPU: # !pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html # GPU: # !pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html # For interactive demo at the end: # !pip install pydub import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchaudio import matplotlib.pyplot as plt import IPython.display as ipd from tqdm.notebook import tqdm 让我们检查一下 CUDA GPU 是否可用，然后选择我们的设备。 在 GPU 上运行网络将大大减少训练/测试时间。 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(device) 导入数据集 我们使用torchaudio下载并表示数据集。 在这里，我们使用 SpeechCommands，它是由不同人员说出的 35 个命令的数据集。 数据集SPEECHCOMMANDS是数据集的torch.utils.data.Dataset版本。 在此数据集中，所有音频文件的长度约为 1 秒（因此约为 16000 个时间帧）。 实际的加载和格式化步骤是在访问数据点时发生的，torchaudio负责将音频文件转换为张量。 如果想直接加载音频文件，可以使用torchaudio.load()。 它返回一个包含新创建的张量的元组以及音频文件的采样频率（SpeechCommands为 16kHz）。 回到数据集，这里我们创建一个子类，将其分为标准训练，验证和测试子集。 from torchaudio.datasets import SPEECHCOMMANDS import os class SubsetSC(SPEECHCOMMANDS): def __init__(self, subset: str = None): super().__init__(\"./\", download=True) def load_list(filename): filepath = os.path.join(self._path, filename) with open(filepath) as fileobj: return [os.path.join(self._path, line.strip()) for line in fileobj] if subset == \"validation\": self._walker = load_list(\"validation_list.txt\") elif subset == \"testing\": self._walker = load_list(\"testing_list.txt\") elif subset == \"training\": excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\") excludes = set(excludes) self._walker = [w for w in self._walker if w not in excludes] # Create training and testing split of the data. We do not use validation in this tutorial. train_set = SubsetSC(\"training\") test_set = SubsetSC(\"testing\") waveform, sample_rate, label, speaker_id, utterance_number = train_set[0] SPEECHCOMMANDS数据集中的数据点是一个由波形（音频信号），采样率，发声（标签），讲话者的 ID，发声数组成的元组。 print(\"Shape of waveform: {}\".format(waveform.size())) print(\"Sample rate of waveform: {}\".format(sample_rate)) plt.plot(waveform.t().numpy()); 让我们找到数据集中可用的标签列表。 labels = sorted(list(set(datapoint[2] for datapoint in train_set))) labels 35 个音频标签是用户说的命令。 前几个文件是人们所说的marvin。 waveform_first, *_ = train_set[0] ipd.Audio(waveform_first.numpy(), rate=sample_rate) waveform_second, *_ = train_set[1] ipd.Audio(waveform_second.numpy(), rate=sample_rate) 最后一个文件是有人说“视觉”。 waveform_last, *_ = train_set[-1] ipd.Audio(waveform_last.numpy(), rate=sample_rate) 格式化数据 这是将转换应用于数据的好地方。 对于波形，我们对音频进行下采样以进行更快的处理，而不会损失太多的分类能力。 我们无需在此应用其他转换。 对于某些数据集，通常必须通过沿通道维度取平均值或仅保留其中一个通道来减少通道数量（例如，从立体声到单声道）。 由于SpeechCommands使用单个通道进行音频，因此此处不需要。 new_sample_rate = 8000 transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate) transformed = transform(waveform) ipd.Audio(transformed.numpy(), rate=new_sample_rate) 我们使用标签列表中的每个索引对每个单词进行编码。 def label_to_index(word): # Return the position of the word in labels return torch.tensor(labels.index(word)) def index_to_label(index): # Return the word corresponding to the index in labels # This is the inverse of label_to_index return labels[index] word_start = \"yes\" index = label_to_index(word_start) word_recovered = index_to_label(index) print(word_start, \"-->\", index, \"-->\", word_recovered) 为了将由录音和语音构成的数据点列表转换为该模型的两个成批张量，我们实现了整理函数，PyTorch DataLoader使用了该函数，允许我们分批迭代数据集。 有关使用整理函数的更多信息，请参见文档。 在整理函数中，我们还应用了重采样和文本编码。 def pad_sequence(batch): # Make all tensor in a batch the same length by padding with zeros batch = [item.t() for item in batch] batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.) return batch.permute(0, 2, 1) def collate_fn(batch): # A data tuple has the form: # waveform, sample_rate, label, speaker_id, utterance_number tensors, targets = [], [] # Gather in lists, and encode labels as indices for waveform, _, label, *_ in batch: tensors += [waveform] targets += [label_to_index(label)] # Group the list of tensors into a batched tensor tensors = pad_sequence(tensors) targets = torch.stack(targets) return tensors, targets batch_size = 256 if device == \"cuda\": num_workers = 1 pin_memory = True else: num_workers = 0 pin_memory = False train_loader = torch.utils.data.DataLoader( train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory, ) test_loader = torch.utils.data.DataLoader( test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory, ) 定义网络 在本教程中，我们将使用卷积神经网络来处理原始音频数据。 通常，更高级的转换将应用于音频数据，但是 CNN 可以用于准确处理原始数据。 具体架构是根据本文中描述的 M5 网络架构建模的。 模型处理原始音频数据的一个重要方面是其第一层过滤器的接收范围。 我们模型的第一个过滤器长度为 80，因此在处理以 8kHz 采样的音频时，接收场约为 10ms（而在 4kHz 时约为 20ms）。 此大小类似于语音处理应用，该应用通常使用 20ms 到 40ms 的接收域。 class M5(nn.Module): def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32): super().__init__() self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride) self.bn1 = nn.BatchNorm1d(n_channel) self.pool1 = nn.MaxPool1d(4) self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3) self.bn2 = nn.BatchNorm1d(n_channel) self.pool2 = nn.MaxPool1d(4) self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3) self.bn3 = nn.BatchNorm1d(2 * n_channel) self.pool3 = nn.MaxPool1d(4) self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3) self.bn4 = nn.BatchNorm1d(2 * n_channel) self.pool4 = nn.MaxPool1d(4) self.fc1 = nn.Linear(2 * n_channel, n_output) def forward(self, x): x = self.conv1(x) x = F.relu(self.bn1(x)) x = self.pool1(x) x = self.conv2(x) x = F.relu(self.bn2(x)) x = self.pool2(x) x = self.conv3(x) x = F.relu(self.bn3(x)) x = self.pool3(x) x = self.conv4(x) x = F.relu(self.bn4(x)) x = self.pool4(x) x = F.avg_pool1d(x, x.shape[-1]) x = x.permute(0, 2, 1) x = self.fc1(x) return F.log_softmax(x, dim=2) model = M5(n_input=transformed.shape[0], n_output=len(labels)) model.to(device) print(model) def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad) n = count_parameters(model) print(\"Number of parameters: %s\" % n) 我们将使用与本文相同的优化技术，将权重衰减设置为 0.0001 的 Adam 优化器。 首先，我们将以 0.01 的学习率进行训练，但是在 20 个周期后的训练过程中，我们将使用scheduler将其降低到 0.001。 optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) # reduce the learning after 20 epochs by a factor of 10 训练和测试网络 现在，我们定义一个训练函数，它将训练数据输入模型中，并执行反向传播和优化步骤。 对于训练，我们将使用的损失是负对数可能性。 然后，在每个周期之后将对网络进行测试，以查看训练期间准确率如何变化。 def train(model, epoch, log_interval): model.train() for batch_idx, (data, target) in enumerate(train_loader): data = data.to(device) target = target.to(device) # apply transform and model on whole batch directly on device data = transform(data) output = model(data) # negative log-likelihood for a tensor of size (batch x 1 x n_output) loss = F.nll_loss(output.squeeze(), target) optimizer.zero_grad() loss.backward() optimizer.step() # print training stats if batch_idx % log_interval == 0: print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100\\. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\") # update progress bar pbar.update(pbar_update) # record loss losses.append(loss.item()) 现在我们有了训练函数，我们需要制作一个用于测试网络准确率的函数。 我们将模型设置为eval()模式，然后对测试数据集进行推断。 调用eval()将网络中所有模块中的训练变量设置为false。 某些层（例如批量归一化层和丢弃层）在训练期间的行为会有所不同，因此此步骤对于获取正确的结果至关重要。 def number_of_correct(pred, target): # count number of correct predictions return pred.squeeze().eq(target).sum().item() def get_likely_index(tensor): # find most likely label index for each element in the batch return tensor.argmax(dim=-1) def test(model, epoch): model.eval() correct = 0 for data, target in test_loader: data = data.to(device) target = target.to(device) # apply transform and model on whole batch directly on device data = transform(data) output = model(data) pred = get_likely_index(output) correct += number_of_correct(pred, target) # update progress bar pbar.update(pbar_update) print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100\\. * correct / len(test_loader.dataset):.0f}%)\\n\") 最后，我们可以训练和测试网络。 我们将训练网络十个周期，然后降低学习率，再训练十个周期。 在每个周期之后将对网络进行测试，以查看训练过程中准确率如何变化。 log_interval = 20 n_epoch = 2 pbar_update = 1 / (len(train_loader) + len(test_loader)) losses = [] # The transform needs to live on the same device as the model and the data. transform = transform.to(device) with tqdm(total=n_epoch) as pbar: for epoch in range(1, n_epoch + 1): train(model, epoch, log_interval) test(model, epoch) scheduler.step() # Let's plot the training loss versus the number of iteration. # plt.plot(losses); # plt.title(\"training loss\"); 2 个周期后，测试集的网络准确率应超过 65%，而 21 个周期后，网络应达到 85%。 让我们看一下训练集中的最后几句话，看看模型是如何做到的。 def predict(tensor): # Use the model to predict the label of the waveform tensor = tensor.to(device) tensor = transform(tensor) tensor = model(tensor.unsqueeze(0)) tensor = get_likely_index(tensor) tensor = index_to_label(tensor.squeeze()) return tensor waveform, sample_rate, utterance, *_ = train_set[-1] ipd.Audio(waveform.numpy(), rate=sample_rate) print(f\"Expected: {utterance}. Predicted: {predict(waveform)}.\") 如果有一个示例，我们来寻找一个分类错误的示例。 for i, (waveform, sample_rate, utterance, *_) in enumerate(test_set): output = predict(waveform) if output != utterance: ipd.Audio(waveform.numpy(), rate=sample_rate) print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\") break else: print(\"All examples in this dataset were correctly classified!\") print(\"In this case, let's just look at the last data point\") ipd.Audio(waveform.numpy(), rate=sample_rate) print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\") 随意尝试使用其中一个标签的自己的录音！ 例如，使用 Colab，在执行下面的单元格时说“ Go”。 这将录制一秒钟的音频并尝试对其进行分类。 from google.colab import output as colab_output from base64 import b64decode from io import BytesIO from pydub import AudioSegment RECORD = \"\"\" const sleep = time => new Promise(resolve => setTimeout(resolve, time)) const b2text = blob => new Promise(resolve => { const reader = new FileReader() reader.onloadend = e => resolve(e.srcElement.result) reader.readAsDataURL(blob) }) var record = time => new Promise(async resolve => { stream = await navigator.mediaDevices.getUserMedia({ audio: true }) recorder = new MediaRecorder(stream) chunks = [] recorder.ondataavailable = e => chunks.push(e.data) recorder.start() await sleep(time) recorder.onstop = async ()=>{ blob = new Blob(chunks) text = await b2text(blob) resolve(text) } recorder.stop() }) \"\"\" def record(seconds=1): display(ipd.Javascript(RECORD)) print(f\"Recording started for {seconds} seconds.\") s = colab_output.eval_js(\"record(%d)\" % (seconds * 1000)) print(\"Recording ended.\") b = b64decode(s.split(\",\")[1]) fileformat = \"wav\" filename = f\"_audio.{fileformat}\" AudioSegment.from_file(BytesIO(b)).export(filename, format=fileformat) return torchaudio.load(filename) waveform, sample_rate = record() print(f\"Predicted: {predict(waveform)}.\") ipd.Audio(waveform.numpy(), rate=sample_rate) 总结 在本教程中，我们使用了torchaudio来加载数据集并对信号进行重新采样。 然后，我们定义了经过训练的神经网络，以识别给定命令。 还有其他数据预处理方法，例如找到梅尔频率倒谱系数（MFCC），可以减小数据集的大小。 此变换也可以在torchaudio中作为torchaudio.transforms.MFCC使用。 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：speech_command_recognition_with_torchaudio.py 下载 Jupyter 笔记本：speech_command_recognition_with_torchaudio.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"26.html":{"url":"26.html","title":"文本","keywords":"","body":"文本 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"27.html":{"url":"27.html","title":"使用nn.Transformer和torchtext的序列到序列建模","keywords":"","body":"使用nn.Transformer和torchtext的序列到序列建模 原文：https://pytorch.org/tutorials/beginner/transformer_tutorial.html 这是一个有关如何训练使用nn.Transformer模块的序列到序列模型的教程。 PyTorch 1.2 版本包括一个基于论文的标准转换器模块。 事实证明，该转换器模型在许多序列间问题上具有较高的质量，同时具有更高的可并行性。 nn.Transformer模块完全依赖于注意力机制（另一个最近实现为nn.MultiheadAttention的模块）来绘制输入和输出之间的全局依存关系。 nn.Transformer模块现已高度模块化，因此可以轻松地修改/组成单个组件（如本教程中的nn.TransformerEncoder）。 定义模型 在本教程中，我们将在语言建模任务上训练nn.TransformerEncoder模型。 语言建模任务是为给定单词（或单词序列）遵循单词序列的可能性分配概率。 标记序列首先传递到嵌入层，然后传递到位置编码层以说明单词的顺序（有关更多详细信息，请参见下一段）。 nn.TransformerEncoder由多层nn.TransformerEncoderLayer组成。 与输入序列一起，还需要一个正方形的注意掩码，因为nn.TransformerEncoder中的自注意层仅允许出现在该序列中的较早位置。 对于语言建模任务，应屏蔽将来头寸上的所有标记。 为了获得实际的单词，将nn.TransformerEncoder模型的输出发送到最终的Linear层，然后是对数 Softmax 函数。 import math import torch import torch.nn as nn import torch.nn.functional as F class TransformerModel(nn.Module): def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5): super(TransformerModel, self).__init__() from torch.nn import TransformerEncoder, TransformerEncoderLayer self.model_type = 'Transformer' self.pos_encoder = PositionalEncoding(ninp, dropout) encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) self.encoder = nn.Embedding(ntoken, ninp) self.ninp = ninp self.decoder = nn.Linear(ninp, ntoken) self.init_weights() def generate_square_subsequent_mask(self, sz): mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1) mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) return mask def init_weights(self): initrange = 0.1 self.encoder.weight.data.uniform_(-initrange, initrange) self.decoder.bias.data.zero_() self.decoder.weight.data.uniform_(-initrange, initrange) def forward(self, src, src_mask): src = self.encoder(src) * math.sqrt(self.ninp) src = self.pos_encoder(src) output = self.transformer_encoder(src, src_mask) output = self.decoder(output) return output PositionalEncoding模块注入一些有关标记在序列中的相对或绝对位置的信息。 位置编码的尺寸与嵌入的尺寸相同，因此可以将两者相加。 在这里，我们使用不同频率的sine和cosine函数。 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0).transpose(0, 1) self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:x.size(0), :] return self.dropout(x) 加载和批量数据 本教程使用torchtext生成 Wikitext-2 数据集。 vocab对象是基于训练数据集构建的，用于将标记数字化为张量。 从序列数据开始，batchify()函数将数据集排列为列，以修剪掉数据分成大小为batch_size的批量后剩余的所有标记。 例如，以字母为序列（总长度为 26）并且批大小为 4，我们将字母分为 4 个长度为 6 的序列： 这些列被模型视为独立的，这意味着无法了解G和F的依赖性，但可以进行更有效的批量。 import io import torch from torchtext.utils import download_from_url, extract_archive from torchtext.data.utils import get_tokenizer from torchtext.vocab import build_vocab_from_iterator url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip' test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url)) tokenizer = get_tokenizer('basic_english') vocab = build_vocab_from_iterator(map(tokenizer, iter(io.open(train_filepath, encoding=\"utf8\")))) def data_process(raw_text_iter): data = [torch.tensor([vocab[token] for token in tokenizer(item)], dtype=torch.long) for item in raw_text_iter] return torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\"))) val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\"))) test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\"))) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") def batchify(data, bsz): # Divide the dataset into bsz parts. nbatch = data.size(0) // bsz # Trim off any extra elements that wouldn't cleanly fit (remainders). data = data.narrow(0, 0, nbatch * bsz) # Evenly divide the data across the bsz batches. data = data.view(bsz, -1).t().contiguous() return data.to(device) batch_size = 20 eval_batch_size = 10 train_data = batchify(train_data, batch_size) val_data = batchify(val_data, eval_batch_size) test_data = batchify(test_data, eval_batch_size) 生成输入序列和目标序列的函数 get_batch()函数为转换器模型生成输入和目标序列。 它将源数据细分为长度为bptt的块。 对于语言建模任务，模型需要以下单词作为Target。 例如，如果bptt值为 2，则i = 0时，我们将获得以下两个变量： 应该注意的是，这些块沿着维度 0，与Transformer模型中的S维度一致。 批量尺寸N沿尺寸 1。 bptt = 35 def get_batch(source, i): seq_len = min(bptt, len(source) - 1 - i) data = source[i:i+seq_len] target = source[i+1:i+1+seq_len].reshape(-1) return data, target 启动实例 使用下面的超参数建立模型。 vocab的大小等于vocab对象的长度。 ntokens = len(vocab.stoi) # the size of vocabulary emsize = 200 # embedding dimension nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder nhead = 2 # the number of heads in the multiheadattention models dropout = 0.2 # the dropout value model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device) 运行模型 CrossEntropyLoss用于跟踪损失，SGD实现随机梯度下降方法作为优化器。 初始学习率设置为 5.0。 StepLR用于通过历时调整学习率。 在训练期间，我们使用nn.utils.clip_grad_norm_函数将所有梯度缩放在一起，以防止爆炸。 criterion = nn.CrossEntropyLoss() lr = 5.0 # learning rate optimizer = torch.optim.SGD(model.parameters(), lr=lr) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) import time def train(): model.train() # Turn on the train mode total_loss = 0. start_time = time.time() src_mask = model.generate_square_subsequent_mask(bptt).to(device) for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)): data, targets = get_batch(train_data, i) optimizer.zero_grad() if data.size(0) != bptt: src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device) output = model(data, src_mask) loss = criterion(output.view(-1, ntokens), targets) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) optimizer.step() total_loss += loss.item() log_interval = 200 if batch % log_interval == 0 and batch > 0: cur_loss = total_loss / log_interval elapsed = time.time() - start_time print('| epoch {:3d} | {:5d}/{:5d} batches | ' 'lr {:02.2f} | ms/batch {:5.2f} | ' 'loss {:5.2f} | ppl {:8.2f}'.format( epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0], elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss))) total_loss = 0 start_time = time.time() def evaluate(eval_model, data_source): eval_model.eval() # Turn on the evaluation mode total_loss = 0. src_mask = model.generate_square_subsequent_mask(bptt).to(device) with torch.no_grad(): for i in range(0, data_source.size(0) - 1, bptt): data, targets = get_batch(data_source, i) if data.size(0) != bptt: src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device) output = eval_model(data, src_mask) output_flat = output.view(-1, ntokens) total_loss += len(data) * criterion(output_flat, targets).item() return total_loss / (len(data_source) - 1) 循环遍历。 如果验证损失是迄今为止迄今为止最好的，请保存模型。 在每个周期之后调整学习率。 best_val_loss = float(\"inf\") epochs = 3 # The number of epochs best_model = None for epoch in range(1, epochs + 1): epoch_start_time = time.time() train() val_loss = evaluate(model, val_data) print('-' * 89) print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | ' 'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss))) print('-' * 89) if val_loss 出： | epoch 1 | 200/ 2928 batches | lr 5.00 | ms/batch 30.78 | loss 8.03 | ppl 3085.47 | epoch 1 | 400/ 2928 batches | lr 5.00 | ms/batch 29.85 | loss 6.83 | ppl 929.53 | epoch 1 | 600/ 2928 batches | lr 5.00 | ms/batch 29.92 | loss 6.41 | ppl 610.71 | epoch 1 | 800/ 2928 batches | lr 5.00 | ms/batch 29.88 | loss 6.29 | ppl 539.54 | epoch 1 | 1000/ 2928 batches | lr 5.00 | ms/batch 29.95 | loss 6.17 | ppl 479.92 | epoch 1 | 1200/ 2928 batches | lr 5.00 | ms/batch 29.95 | loss 6.15 | ppl 468.35 | epoch 1 | 1400/ 2928 batches | lr 5.00 | ms/batch 29.95 | loss 6.11 | ppl 450.25 | epoch 1 | 1600/ 2928 batches | lr 5.00 | ms/batch 29.95 | loss 6.10 | ppl 445.77 | epoch 1 | 1800/ 2928 batches | lr 5.00 | ms/batch 29.97 | loss 6.02 | ppl 409.90 | epoch 1 | 2000/ 2928 batches | lr 5.00 | ms/batch 29.92 | loss 6.01 | ppl 408.66 | epoch 1 | 2200/ 2928 batches | lr 5.00 | ms/batch 29.94 | loss 5.90 | ppl 363.89 | epoch 1 | 2400/ 2928 batches | lr 5.00 | ms/batch 29.94 | loss 5.96 | ppl 388.68 | epoch 1 | 2600/ 2928 batches | lr 5.00 | ms/batch 29.94 | loss 5.95 | ppl 382.60 | epoch 1 | 2800/ 2928 batches | lr 5.00 | ms/batch 29.95 | loss 5.88 | ppl 358.87 ----------------------------------------------------------------------------------------- | end of epoch 1 | time: 91.45s | valid loss 5.85 | valid ppl 348.17 ----------------------------------------------------------------------------------------- | epoch 2 | 200/ 2928 batches | lr 4.51 | ms/batch 30.09 | loss 5.86 | ppl 351.70 | epoch 2 | 400/ 2928 batches | lr 4.51 | ms/batch 29.97 | loss 5.85 | ppl 347.85 | epoch 2 | 600/ 2928 batches | lr 4.51 | ms/batch 29.98 | loss 5.67 | ppl 288.80 | epoch 2 | 800/ 2928 batches | lr 4.51 | ms/batch 29.92 | loss 5.70 | ppl 299.81 | epoch 2 | 1000/ 2928 batches | lr 4.51 | ms/batch 29.95 | loss 5.65 | ppl 285.57 | epoch 2 | 1200/ 2928 batches | lr 4.51 | ms/batch 29.99 | loss 5.68 | ppl 293.48 | epoch 2 | 1400/ 2928 batches | lr 4.51 | ms/batch 29.96 | loss 5.69 | ppl 296.90 | epoch 2 | 1600/ 2928 batches | lr 4.51 | ms/batch 29.96 | loss 5.72 | ppl 303.83 | epoch 2 | 1800/ 2928 batches | lr 4.51 | ms/batch 29.93 | loss 5.66 | ppl 285.90 | epoch 2 | 2000/ 2928 batches | lr 4.51 | ms/batch 29.93 | loss 5.67 | ppl 289.58 | epoch 2 | 2200/ 2928 batches | lr 4.51 | ms/batch 29.97 | loss 5.55 | ppl 257.20 | epoch 2 | 2400/ 2928 batches | lr 4.51 | ms/batch 29.96 | loss 5.65 | ppl 283.92 | epoch 2 | 2600/ 2928 batches | lr 4.51 | ms/batch 29.95 | loss 5.65 | ppl 283.76 | epoch 2 | 2800/ 2928 batches | lr 4.51 | ms/batch 29.95 | loss 5.60 | ppl 269.90 ----------------------------------------------------------------------------------------- | end of epoch 2 | time: 91.37s | valid loss 5.60 | valid ppl 270.66 ----------------------------------------------------------------------------------------- | epoch 3 | 200/ 2928 batches | lr 4.29 | ms/batch 30.12 | loss 5.60 | ppl 269.95 | epoch 3 | 400/ 2928 batches | lr 4.29 | ms/batch 29.92 | loss 5.62 | ppl 274.84 | epoch 3 | 600/ 2928 batches | lr 4.29 | ms/batch 29.96 | loss 5.41 | ppl 222.98 | epoch 3 | 800/ 2928 batches | lr 4.29 | ms/batch 29.93 | loss 5.48 | ppl 240.15 | epoch 3 | 1000/ 2928 batches | lr 4.29 | ms/batch 29.94 | loss 5.43 | ppl 229.16 | epoch 3 | 1200/ 2928 batches | lr 4.29 | ms/batch 29.94 | loss 5.48 | ppl 239.42 | epoch 3 | 1400/ 2928 batches | lr 4.29 | ms/batch 29.95 | loss 5.49 | ppl 242.87 | epoch 3 | 1600/ 2928 batches | lr 4.29 | ms/batch 29.93 | loss 5.52 | ppl 250.16 | epoch 3 | 1800/ 2928 batches | lr 4.29 | ms/batch 29.93 | loss 5.47 | ppl 237.70 | epoch 3 | 2000/ 2928 batches | lr 4.29 | ms/batch 29.94 | loss 5.49 | ppl 241.36 | epoch 3 | 2200/ 2928 batches | lr 4.29 | ms/batch 29.92 | loss 5.36 | ppl 211.91 | epoch 3 | 2400/ 2928 batches | lr 4.29 | ms/batch 29.95 | loss 5.47 | ppl 237.16 | epoch 3 | 2600/ 2928 batches | lr 4.29 | ms/batch 29.94 | loss 5.47 | ppl 236.47 | epoch 3 | 2800/ 2928 batches | lr 4.29 | ms/batch 29.92 | loss 5.41 | ppl 223.08 ----------------------------------------------------------------------------------------- | end of epoch 3 | time: 91.32s | valid loss 5.61 | valid ppl 272.10 ----------------------------------------------------------------------------------------- 使用测试数据集评估模型 应用最佳模型以检查测试数据集的结果。 test_loss = evaluate(best_model, test_data) print('=' * 89) print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format( test_loss, math.exp(test_loss))) print('=' * 89) 出： ========================================================================================= | End of training | test loss 5.52 | test ppl 249.05 ========================================================================================= 脚本的总运行时间：（4 分钟 50.218 秒） 下载 Python 源码：transformer_tutorial.py 下载 Jupyter 笔记本：transformer_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"28.html":{"url":"28.html","title":"从零开始的 NLP：使用字符级 RNN 分类名称","keywords":"","body":"从零开始的 NLP：使用字符级 RNN 分类名称 原文：https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html 作者： Sean Robertson 我们将建立和训练基本的字符级 RNN 对单词进行分类。 本教程与以下两个教程一起，展示了如何“从头开始”进行 NLP 建模的预处理数据，特别是不使用torchtext的许多便利函数，因此您可以了解 NLP 建模的预处理如何在低水平上工作。 字符级 RNN 将单词作为一系列字符读取-在每个步骤输出预测和“隐藏状态”，将其先前的隐藏状态输入到每个下一步。 我们将最终的预测作为输出，即单词属于哪个类别。 具体来说，我们将训练来自 18 种起源语言的数千种姓氏，并根据拼写方式预测名称的来源： $ python predict.py Hinton (-0.47) Scottish (-1.52) English (-3.57) Irish $ python predict.py Schmidhuber (-0.19) German (-2.48) Czech (-2.68) Dutch 推荐读物： 我假设您至少已经安装了 PyTorch，Python 和 Tensors： 安装说明 使用 PyTorch 进行深度学习：60 分钟的突击通常开始使用 PyTorch 使用示例学习 PyTorch PyTorch（面向以前的 Torch 用户）（如果您以前是 Lua Torch 用户） 了解 RNN 及其工作方式也将很有用： 《循环神经网络的不合理有效性》显示了许多现实生活中的例子 《了解 LSTM 网络》特别是关于 LSTM 的，但一般来说也有关 RNN 的 准备数据 注意 从的下载数据，并将其提取到当前目录。 data/names目录中包含 18 个文本文件，名称为[Language].txt。 每个文件包含一堆名称，每行一个名称，大多数是罗马化的（但我们仍然需要从 Unicode 转换为 ASCII）。 我们将得到一个字典，其中列出了每种语言的名称列表{language: [names ...]}。 通用变量“类别”和“行”（在本例中为语言和名称）用于以后的扩展。 from __future__ import unicode_literals, print_function, division from io import open import glob import os def findFiles(path): return glob.glob(path) print(findFiles('data/names/*.txt')) import unicodedata import string all_letters = string.ascii_letters + \" .,;'\" n_letters = len(all_letters) # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) print(unicodeToAscii('Ślusàrski')) # Build the category_lines dictionary, a list of names per language category_lines = {} all_categories = [] # Read a file and split into lines def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] for filename in findFiles('data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) 出： ['data/names/French.txt', 'data/names/Czech.txt', 'data/names/Dutch.txt', 'data/names/Polish.txt', 'data/names/Scottish.txt', 'data/names/Chinese.txt', 'data/names/English.txt', 'data/names/Italian.txt', 'data/names/Portuguese.txt', 'data/names/Japanese.txt', 'data/names/German.txt', 'data/names/Russian.txt', 'data/names/Korean.txt', 'data/names/Arabic.txt', 'data/names/Greek.txt', 'data/names/Vietnamese.txt', 'data/names/Spanish.txt', 'data/names/Irish.txt'] Slusarski 现在我们有了category_lines，这是一个字典，将每个类别（语言）映射到行（名称）列表。 我们还跟踪了all_categories（只是语言列表）和n_categories，以供以后参考。 print(category_lines['Italian'][:5]) 出： ['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni'] 将名称转换为张量 现在我们已经组织了所有名称，我们需要将它们转换为张量以使用它们。 为了表示单个字母，我们使用大小为的单热向量。 单热向量用 0 填充，但当前字母的索引处的数字为 1，例如 \"b\" = 。 为了制造一个单词，我们将其中的一些连接成 2D 矩阵。 额外的 1 维是因为 PyTorch 假定所有内容都是成批的-在这里我们仅使用 1 的批量大小。 import torch # Find letter index from all_letters, e.g. \"a\" = 0 def letterToIndex(letter): return all_letters.find(letter) # Just for demonstration, turn a letter into a Tensor def letterToTensor(letter): tensor = torch.zeros(1, n_letters) tensor[0][letterToIndex(letter)] = 1 return tensor # Turn a line into a , # or an array of one-hot letter vectors def lineToTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li, letter in enumerate(line): tensor[li][0][letterToIndex(letter)] = 1 return tensor print(letterToTensor('J')) print(lineToTensor('Jones').size()) 出： tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) torch.Size([5, 1, 57]) 创建网络 在进行自动微分之前，在 Torch 中创建一个循环神经网络涉及在多个时间步长上克隆层的参数。 层保留了隐藏状态和梯度，这些层现在完全由图本身处理。 这意味着您可以非常“纯”的方式将 RNN 用作常规前馈层。 该 RNN 模块（主要从面向 Torch 用户的 PyTorch 教程复制）只有两个线性层，它们在输入和隐藏状态下运行，在输出之后是LogSoftmax层。 import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): combined = torch.cat((input, hidden), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) n_hidden = 128 rnn = RNN(n_letters, n_hidden, n_categories) 要运行此网络的步骤，我们需要传递输入（在本例中为当前字母的张量）和先前的隐藏状态（首先将其初始化为零）。 我们将返回输出（每种语言的概率）和下一个隐藏状态（我们将其保留用于下一步）。 input = letterToTensor('A') hidden =torch.zeros(1, n_hidden) output, next_hidden = rnn(input, hidden) 为了提高效率，我们不想为每个步骤创建一个新的张量，因此我们将使用lineToTensor而不是letterToTensor并使用切片。 这可以通过预先计算一批张量来进一步优化。 input = lineToTensor('Albert') hidden = torch.zeros(1, n_hidden) output, next_hidden = rnn(input[0], hidden) print(output) 出： tensor([[-2.8934, -2.7991, -2.8549, -2.8915, -2.9122, -2.9010, -2.8979, -2.8875, -2.8256, -2.8792, -2.8712, -2.8465, -2.9582, -3.0171, -2.8308, -2.9629, -2.9233, -2.8979]], grad_fn=) 如您所见，输出为张量，其中每个项目都是该类别的可能性（可能性更大）。 训练 准备训练 在接受训练之前，我们应该做一些辅助函数。 首先是解释网络的输出，我们知道这是每个类别的可能性。 我们可以使用Tensor.topk获得最大值的索引： def categoryFromOutput(output): top_n, top_i = output.topk(1) category_i = top_i[0].item() return all_categories[category_i], category_i print(categoryFromOutput(output)) 出： ('Czech', 1) 我们还将希望有一种快速的方法来获取训练示例（名称及其语言）： import random def randomChoice(l): return l[random.randint(0, len(l) - 1)] def randomTrainingExample(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long) line_tensor = lineToTensor(line) return category, line, category_tensor, line_tensor for i in range(10): category, line, category_tensor, line_tensor = randomTrainingExample() print('category =', category, '/ line =', line) 出： category = Chinese / line = Jia category = Korean / line = Son category = Czech / line = Matocha category = Dutch / line = Nifterik category = German / line = Dreschner category = Irish / line = Names category = French / line = Charpentier category = Italian / line = Carboni category = Irish / line = Shannon category = German / line = Adam 训练网络 现在，训练该网络所需要做的就是向它展示大量示例，进行猜测，并告诉它是否错误。 对于损失函数，nn.NLLLoss是适当的，因为 RNN 的最后一层是nn.LogSoftmax。 criterion = nn.NLLLoss() 每个训练循环将： 创建输入和目标张量 创建归零的初始隐藏状态 阅读每个字母 保存下一个字母的隐藏状态 比较最终输出与目标 反向传播 返回输出和损失 learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn def train(category_tensor, line_tensor): hidden = rnn.initHidden() rnn.zero_grad() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) loss = criterion(output, category_tensor) loss.backward() # Add parameters' gradients to their values, multiplied by learning rate for p in rnn.parameters(): p.data.add_(p.grad.data, alpha=-learning_rate) return output, loss.item() 现在，我们只需要运行大量示例。 由于train函数返回输出和损失，因此我们可以打印其猜测并跟踪作图的损失。 因为有 1000 个示例，所以我们仅打印每个print_every示例，并对损失进行平均。 import time import math n_iters = 100000 print_every = 5000 plot_every = 1000 # Keep track of losses for plotting current_loss = 0 all_losses = [] def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) start = time.time() for iter in range(1, n_iters + 1): category, line, category_tensor, line_tensor = randomTrainingExample() output, loss = train(category_tensor, line_tensor) current_loss += loss # Print iter number, loss, name and guess if iter % print_every == 0: guess, guess_i = categoryFromOutput(output) correct = '✓' if guess == category else '✗ (%s)' % category print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct)) # Add current loss avg to list of losses if iter % plot_every == 0: all_losses.append(current_loss / plot_every) current_loss = 0 出： 5000 5% (0m 15s) 2.5667 Ly / Chinese ✗ (Vietnamese) 10000 10% (0m 26s) 2.3171 Rocha / Japanese ✗ (Portuguese) 15000 15% (0m 37s) 2.2941 Gouveia / Spanish ✗ (Portuguese) 20000 20% (0m 49s) 1.3015 Lippi / Italian ✓ 25000 25% (1m 1s) 0.7693 Thuy / Vietnamese ✓ 30000 30% (1m 13s) 1.9341 Murray / Arabic ✗ (Scottish) 35000 35% (1m 25s) 2.3633 Busto / Scottish ✗ (Italian) 40000 40% (1m 38s) 1.0401 Chung / Chinese ✗ (Korean) 45000 45% (1m 50s) 0.0499 Filipowski / Polish ✓ 50000 50% (2m 2s) 0.2598 Mccallum / Scottish ✓ 55000 55% (2m 14s) 4.5375 Mozdzierz / German ✗ (Polish) 60000 60% (2m 26s) 1.7194 Talalihin / Irish ✗ (Russian) 65000 65% (2m 38s) 0.1150 Ziemniak / Polish ✓ 70000 70% (2m 51s) 1.8548 Pharlain / Scottish ✗ (Irish) 75000 75% (3m 3s) 2.1362 Prehatney / Russian ✗ (Czech) 80000 80% (3m 15s) 0.4166 Leclerc / French ✓ 85000 85% (3m 27s) 1.4189 Elford / English ✓ 90000 90% (3m 39s) 2.1959 Gagnon / Scottish ✗ (French) 95000 95% (3m 51s) 0.1622 Bukoski / Polish ✓ 100000 100% (4m 3s) 1.3180 Faucheux / French ✓ 绘制结果 从all_losses绘制历史损失可显示网络学习情况： import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) 评估结果 为了查看网络在不同类别上的表现如何，我们将创建一个混淆矩阵，为每种实际语言（行）指示网络猜测（列）哪种语言。 为了计算混淆矩阵，使用evaluate()通过网络运行一堆样本，该样本等于train()减去反向传播器。 # Keep track of correct guesses in a confusion matrix confusion = torch.zeros(n_categories, n_categories) n_confusion = 10000 # Just return an output given a line def evaluate(line_tensor): hidden = rnn.initHidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) return output # Go through a bunch of examples and record which are correctly guessed for i in range(n_confusion): category, line, category_tensor, line_tensor = randomTrainingExample() output = evaluate(line_tensor) guess, guess_i = categoryFromOutput(output) category_i = all_categories.index(category) confusion[category_i][guess_i] += 1 # Normalize by dividing every row by its sum for i in range(n_categories): confusion[i] = confusion[i] / confusion[i].sum() # Set up plot fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(confusion.numpy()) fig.colorbar(cax) # Set up axes ax.set_xticklabels([''] + all_categories, rotation=90) ax.set_yticklabels([''] + all_categories) # Force label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) # sphinx_gallery_thumbnail_number = 2 plt.show() 您可以从主轴上挑出一些亮点，以显示它猜错了哪些语言，例如中文（朝鲜语）和西班牙语（意大利语）。 它似乎与希腊语搭配得很好，而与英语搭配得很差（可能是因为与其他语言重叠）。 在用户输入上运行 def predict(input_line, n_predictions=3): print('\\n> %s' % input_line) with torch.no_grad(): output = evaluate(lineToTensor(input_line)) # Get top N categories topv, topi = output.topk(n_predictions, 1, True) predictions = [] for i in range(n_predictions): value = topv[0][i].item() category_index = topi[0][i].item() print('(%.2f) %s' % (value, all_categories[category_index])) predictions.append([value, all_categories[category_index]]) predict('Dovesky') predict('Jackson') predict('Satoshi') 出： > Dovesky (-0.82) Russian (-1.06) Czech (-2.22) Polish > Jackson (-0.63) English (-1.75) Scottish (-1.75) Russian > Satoshi (-0.97) Japanese (-1.50) Polish (-2.13) Italian 实际 PyTorch 存储库中的脚本的最终版本将上述代码分成几个文件： data.py（加载文件） model.py（定义 RNN） train.py（进行训练） predict.py（使用命令行参数运行predict()） server.py（通过bottle.py将预测用作 JSON API） 运行train.py训练并保存网络。 使用名称运行predict.py以查看预测： $ python predict.py Hazaki (-0.42) Japanese (-1.39) Polish (-3.51) Czech 运行server.py并访问http://localhost:5533/Yourname以获取预测的 JSON 输出。 练习 尝试使用行 -> 类别的其他数据集，例如： 任何单词 -> 语言 名称 -> 性别 角色名称 -> 作家 页面标题 -> 博客或 subreddit 通过更大和/或形状更好的网络获得更好的结果 添加更多线性层 尝试nn.LSTM和nn.GRU层 将多个这些 RNN 合并为更高级别的网络 脚本的总运行时间：（4 分钟 15.239 秒） 下载 Python 源码：char_rnn_classification_tutorial.py 下载 Jupyter 笔记本：char_rnn_classification_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"29.html":{"url":"29.html","title":"从零开始的 NLP：使用字符级 RNN 生成名称","keywords":"","body":"从零开始的 NLP：使用字符级 RNN 生成名称 原文：https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html 作者： Sean Robertson 这是我们关于“从零开始的 NLP”的三个教程中的第二个。 在第一个教程/intermediate/char_rnn_classification_tutorial中，我们使用了 RNN 将名称分类为源语言。 这次，我们将转过来并使用语言生成名称。 > python sample.py Russian RUS Rovakov Uantov Shavakov > python sample.py German GER Gerren Ereng Rosher > python sample.py Spanish SPA Salla Parer Allan > python sample.py Chinese CHI Chan Hang Iun 我们仍在手工制作带有一些线性层的小型 RNN。 最大的区别在于，我们无需输入名称中的所有字母即可预测类别，而是输入类别并一次输出一个字母。 反复预测字符以形成语言（这也可以用单词或其他高阶结构来完成）通常称为“语言模型”。 推荐读物： 我假设您至少已经安装了 PyTorch，Python 和张量： 安装说明 使用 PyTorch 进行深度学习：60 分钟的突击通常开始使用 PyTorch 使用示例学习 PyTorch PyTorch（面向以前的 Torch 用户）（如果您以前是 Lua Torch 用户） 了解 RNN 及其工作方式也将很有用： 《循环神经网络的不合理有效性》显示了许多现实生活中的例子 《了解 LSTM 网络》特别是关于 LSTM 的，但一般来说也有关 RNN 的 我还建议上一教程《从零开始的 NLP：使用字符级 RNN 对名称进行分类》 准备数据 注意 从的下载数据，并将其提取到当前目录。 有关此过程的更多详细信息，请参见上一教程。 简而言之，有一堆纯文本文件data/names/[Language].txt，每行都有一个名称。 我们将行拆分成一个数组，将 Unicode 转换为 ASCII，最后得到一个字典{language: [names ...]}。 from __future__ import unicode_literals, print_function, division from io import open import glob import os import unicodedata import string all_letters = string.ascii_letters + \" .,;'-\" n_letters = len(all_letters) + 1 # Plus EOS marker def findFiles(path): return glob.glob(path) # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) # Read a file and split into lines def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] # Build the category_lines dictionary, a list of lines per category category_lines = {} all_categories = [] for filename in findFiles('data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) if n_categories == 0: raise RuntimeError('Data not found. Make sure that you downloaded data ' 'from https://download.pytorch.org/tutorial/data.zip and extract it to ' 'the current directory.') print('# categories:', n_categories, all_categories) print(unicodeToAscii(\"O'Néàl\")) 出： # categories: 18 ['French', 'Czech', 'Dutch', 'Polish', 'Scottish', 'Chinese', 'English', 'Italian', 'Portuguese', 'Japanese', 'German', 'Russian', 'Korean', 'Arabic', 'Greek', 'Vietnamese', 'Spanish', 'Irish'] O'Neal 创建网络 该网络扩展最后一个教程的 RNN，并为类别张量附加了一个参数，该参数与其他张量连接在一起。 类别张量就像字母输入一样是一个单向向量。 我们将输出解释为下一个字母的概率。 采样时，最可能的输出字母用作下一个输入字母。 我添加了第二个线性层o2o（在合并了隐藏和输出之后），以使其有更多的肌肉可以使用。 还有一个丢弃层，以给定的概率（此处为 0.1）将输入的部分随机归零，通常用于模糊输入以防止过拟合。 在这里，我们在网络的末端使用它来故意添加一些混乱并增加采样种类。 import torch import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size) self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size) self.o2o = nn.Linear(hidden_size + output_size, output_size) self.dropout = nn.Dropout(0.1) self.softmax = nn.LogSoftmax(dim=1) def forward(self, category, input, hidden): input_combined = torch.cat((category, input, hidden), 1) hidden = self.i2h(input_combined) output = self.i2o(input_combined) output_combined = torch.cat((hidden, output), 1) output = self.o2o(output_combined) output = self.dropout(output) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) 训练 准备训练 首先，辅助函数获取随机对（类别，行）： import random # Random item from a list def randomChoice(l): return l[random.randint(0, len(l) - 1)] # Get a random category and random line from that category def randomTrainingPair(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) return category, line 对于每个时间步（即，对于训练词中的每个字母），网络的输入将为(category, current letter, hidden state)，而输出将为(next letter, next hidden state)。 因此，对于每个训练集，我们都需要类别，一组输入字母和一组输出/目标字母。 由于我们正在预测每个时间步中当前字母的下一个字母，因此字母对是该行中连续字母的组-例如对于\"ABCD\"，我们将创建('A', 'B'), ('B', 'C'), ('C', 'D'), ('D', 'EOS')。 类别张量是大小为的单热张量。 训练时，我们会随时随地将其馈送到网络中-这是一种设计选择，它可能已作为初始隐藏状态或某些其他策略的一部分包含在内。 # One-hot vector for category def categoryTensor(category): li = all_categories.index(category) tensor = torch.zeros(1, n_categories) tensor[0][li] = 1 return tensor # One-hot matrix of first to last letters (not including EOS) for input def inputTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li in range(len(line)): letter = line[li] tensor[li][0][all_letters.find(letter)] = 1 return tensor # LongTensor of second letter to end (EOS) for target def targetTensor(line): letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))] letter_indexes.append(n_letters - 1) # EOS return torch.LongTensor(letter_indexes) 为了方便训练，我们将使用randomTrainingExample函数来获取随机（类别，行）对，并将其转换为所需的（类别，输入，目标）张量。 # Make category, input, and target tensors from a random category, line pair def randomTrainingExample(): category, line = randomTrainingPair() category_tensor = categoryTensor(category) input_line_tensor = inputTensor(line) target_line_tensor = targetTensor(line) return category_tensor, input_line_tensor, target_line_tensor 训练网络 与仅使用最后一个输出的分类相反，我们在每个步骤进行预测，因此在每个步骤都计算损失。 Autograd 的神奇之处在于，您可以简单地在每个步骤中对这些损失进行求和，然后在末尾调用。 criterion = nn.NLLLoss() learning_rate = 0.0005 def train(category_tensor, input_line_tensor, target_line_tensor): target_line_tensor.unsqueeze_(-1) hidden = rnn.initHidden() rnn.zero_grad() loss = 0 for i in range(input_line_tensor.size(0)): output, hidden = rnn(category_tensor, input_line_tensor[i], hidden) l = criterion(output, target_line_tensor[i]) loss += l loss.backward() for p in rnn.parameters(): p.data.add_(p.grad.data, alpha=-learning_rate) return output, loss.item() / input_line_tensor.size(0) 为了跟踪训练需要多长时间，我添加了一个timeSince(timestamp)函数，该函数返回人类可读的字符串： import time import math def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) 训练照常进行-召集训练多次并等待几分钟，每print_every个示例打印当前时间和损失，并在all_losses中保存每个plot_every实例的平均损失以供以后绘制。 rnn = RNN(n_letters, 128, n_letters) n_iters = 100000 print_every = 5000 plot_every = 500 all_losses = [] total_loss = 0 # Reset every plot_every iters start = time.time() for iter in range(1, n_iters + 1): output, loss = train(*randomTrainingExample()) total_loss += loss if iter % print_every == 0: print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss)) if iter % plot_every == 0: all_losses.append(total_loss / plot_every) total_loss = 0 出： 0m 26s (5000 5%) 3.2265 0m 51s (10000 10%) 3.0171 1m 16s (15000 15%) 2.1535 1m 41s (20000 20%) 2.0806 2m 7s (25000 25%) 2.3842 2m 32s (30000 30%) 2.5014 2m 57s (35000 35%) 2.2441 3m 22s (40000 40%) 2.2113 3m 47s (45000 45%) 2.1184 4m 13s (50000 50%) 1.3983 4m 38s (55000 55%) 2.5881 5m 3s (60000 60%) 1.8033 5m 29s (65000 65%) 2.4285 5m 54s (70000 70%) 2.4198 6m 20s (75000 75%) 2.9660 6m 45s (80000 80%) 1.9752 7m 11s (85000 85%) 3.7507 7m 36s (90000 90%) 2.2044 8m 2s (95000 95%) 2.8938 8m 27s (100000 100%) 2.2471 绘制损失图 绘制all_loss的历史损失可显示网络学习情况： import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) 网络采样 为了示例，我们给网络一个字母，询问下一个字母是什么，将其作为下一个字母输入，并重复直到 EOS 标记。 为输入类别，起始字母和空隐藏状态创建张量 用起始字母创建一个字符串output_name 直到最大输出长度， 将当前字母输入网络 从最高输出中获取下一个字母，以及下一个隐藏状态 如果字母是EOS，请在此处停止 如果是普通字母，请添加到output_name并继续 返回姓氏 注意 不必给它起一个开始字母，另一种策略是在训练中包括一个“字符串开始”标记，并让网络选择自己的开始字母。 max_length = 20 # Sample from a category and starting letter def sample(category, start_letter='A'): with torch.no_grad(): # no need to track history in sampling category_tensor = categoryTensor(category) input = inputTensor(start_letter) hidden = rnn.initHidden() output_name = start_letter for i in range(max_length): output, hidden = rnn(category_tensor, input[0], hidden) topv, topi = output.topk(1) topi = topi[0][0] if topi == n_letters - 1: break else: letter = all_letters[topi] output_name += letter input = inputTensor(letter) return output_name # Get multiple samples from one category and multiple starting letters def samples(category, start_letters='ABC'): for start_letter in start_letters: print(sample(category, start_letter)) samples('Russian', 'RUS') samples('German', 'GER') samples('Spanish', 'SPA') samples('Chinese', 'CHI') 出： Rovanov Uarinov Santovov Gangerten Erer Roure Salla Parera Allan Chin Han Iun 练习 尝试使用类别 -> 行的其他数据集，例如： 虚构序列 -> 角色名称 词性 -> 词 国家 -> 城市 使用“句子开头”标记，以便无需选择开始字母即可进行采样 通过更大和/或形状更好的网络获得更好的结果 尝试nn.LSTM和nn.GRU层 将多个这些 RNN 合并为更高级别的网络 脚本的总运行时间：（8 分钟 27.431 秒） 下载 Python 源码：char_rnn_generation_tutorial.py 下载 Jupyter 笔记本：char_rnn_generation_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"30.html":{"url":"30.html","title":"从零开始的 NLP：使用序列到序列网络和注意力的翻译","keywords":"","body":"从零开始的 NLP：使用序列到序列网络和注意力的翻译 原文：https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html 作者： Sean Robertson 这是关于“从头开始进行 NLP”的第三篇也是最后一篇教程，我们在其中编写自己的类和函数来预处理数据以完成 NLP 建模任务。 我们希望在完成本教程后，您将继续学习紧接着本教程的三本教程，torchtext如何为您处理许多此类预处理。 在这个项目中，我们将教授将法语翻译成英语的神经网络。 [KEY: > input, = target, il est en train de peindre un tableau . = he is painting a picture . pourquoi ne pas essayer ce vin delicieux ? = why not try that delicious wine ? elle n est pas poete mais romanciere . = she is not a poet but a novelist . vous etes trop maigre . = you re too skinny . ……取得不同程度的成功。 通过序列到序列网络的简单但强大的构想，使这成为可能，其中两个循环神经网络协同工作，将一个序列转换为另一个序列。 编码器网络将输入序列压缩为一个向量，而解码器网络将该向量展开为一个新序列。 为了改进此模型，我们将使用注意力机制，该机制可使解码器学会专注于输入序列的特定范围。 推荐读物： 我假设您至少已经安装了 PyTorch，Python 和张量： 安装说明 使用 PyTorch 进行深度学习：60 分钟的突击通常开始使用 PyTorch [使用示例]学习 PyTorch(../beginner/pytorch_with_examples.html) PyTorch（面向以前的 Torch 用户）（如果您以前是 Lua Torch 用户） 了解序列到序列网络及其工作方式也将很有用： 《使用 RNN 编解码器学习短语表示法进行统计机器翻译》 《序列到神经网络的序列学习》 《通过共同学习对齐和翻译的神经机器翻译》 《神经对话模型》 您还将找到有关《从零开始的 NLP：使用字符级 RNN 分类名称》和《从零开始的 NLP：使用字符级 RNN 生成名称》的先前教程。 分别与编码器和解码器模型非常相似。 有关更多信息，请阅读介绍以下主题的论文： 《使用 RNN 编解码器学习短语表示法进行统计机器翻译》 《序列到序列神经网络的学习》 《通过共同学习对齐和翻译的神经机器翻译》 《神经对话模型》 要求 from __future__ import unicode_literals, print_function, division from io import open import unicodedata import string import re import random import torch import torch.nn as nn from torch import optim import torch.nn.functional as F device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 加载数据文件 该项目的数据是成千上万的英语到法语翻译对的集合。 开放数据栈交换上的这个问题使我指向开放翻译站点 ，该站点可从这里下载。更好的是，有人在这里做了一些额外的工作，将语言对拆分为单独的文本文件。 英文对法文对太大，无法包含在仓库中，因此请先下载到data/eng-fra.txt，然后再继续。 该文件是制表符分隔的翻译对列表： I am cold. J'ai froid. 注意 从的下载数据，并将其提取到当前目录。 与字符级 RNN 教程中使用的字符编码类似，我们将一种语言中的每个单词表示为一个单向向量，或零个大向量（除单个单向索引外）（在单词的索引处）。 与某种语言中可能存在的数十个字符相比，单词更多很多，因此编码向量要大得多。 但是，我们将作弊并整理数据以使每种语言仅使用几千个单词。 我们将需要每个单词一个唯一的索引，以便以后用作网络的输入和目标。 为了跟踪所有这些，我们将使用一个名为Lang的帮助程序类，该类具有单词→索引（word2index）和索引→单词（index2word）字典，以及每个要使用的单词word2count的计数，以便以后替换稀有词。 SOS_token = 0 EOS_token = 1 class Lang: def __init__(self, name): self.name = name self.word2index = {} self.word2count = {} self.index2word = {0: \"SOS\", 1: \"EOS\"} self.n_words = 2 # Count SOS and EOS def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.n_words self.word2count[word] = 1 self.index2word[self.n_words] = word self.n_words += 1 else: self.word2count[word] += 1 文件全部为 Unicode，为简化起见，我们将 Unicode 字符转换为 ASCII，将所有内容都转换为小写，并修剪大多数标点符号。 # Turn a Unicode string to plain ASCII, thanks to # https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) # Lowercase, trim, and remove non-letter characters def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s 要读取数据文件，我们将文件拆分为几行，然后将几行拆分为两对。 这些文件都是英语→其他语言的，因此，如果我们要从其他语言→英语进行翻译，我添加了reverse标志来反转对。 def readLangs(lang1, lang2, reverse=False): print(\"Reading lines...\") # Read the file and split into lines lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\ read().strip().split('\\n') # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] # Reverse pairs, make Lang instances if reverse: pairs = [list(reversed(p)) for p in pairs] input_lang = Lang(lang2) output_lang = Lang(lang1) else: input_lang = Lang(lang1) output_lang = Lang(lang2) return input_lang, output_lang, pairs 由于示例句子有很多，并且我们想快速训练一些东西，因此我们将数据集修剪为仅相对简短的句子。 在这里，最大长度为 10 个字（包括结尾的标点符号），我们正在过滤翻译成“我是”或“他是”等形式的句子（考虑到前面已替换掉撇号的情况）。 MAX_LENGTH = 10 eng_prefixes = ( \"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s \", \"you are\", \"you re \", \"we are\", \"we re \", \"they are\", \"they re \" ) def filterPair(p): return len(p[0].split(' ')) 准备数据的完整过程是： 读取文本文件并拆分为行，将行拆分为偶对 规范文本，按长度和内容过滤 成对建立句子中的单词列表 def prepareData(lang1, lang2, reverse=False): input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse) print(\"Read %s sentence pairs\" % len(pairs)) pairs = filterPairs(pairs) print(\"Trimmed to %s sentence pairs\" % len(pairs)) print(\"Counting words...\") for pair in pairs: input_lang.addSentence(pair[0]) output_lang.addSentence(pair[1]) print(\"Counted words:\") print(input_lang.name, input_lang.n_words) print(output_lang.name, output_lang.n_words) return input_lang, output_lang, pairs input_lang, output_lang, pairs = prepareData('eng', 'fra', True) print(random.choice(pairs)) 出： Reading lines... Read 135842 sentence pairs Trimmed to 10599 sentence pairs Counting words... Counted words: fra 4345 eng 2803 ['il a l habitude des ordinateurs .', 'he is familiar with computers .'] Seq2Seq 模型 循环神经网络（RNN）是在序列上运行并将其自身的输出用作后续步骤的输入的网络。 序列到序列网络或 seq2seq 网络或编码器解码器网络是由两个称为编码器和解码器的 RNN 组成的模型。 编码器读取输入序列并输出单个向量，而解码器读取该向量以产生输出序列。 与使用单个 RNN 进行序列预测（每个输入对应一个输出）不同，seq2seq 模型使我们摆脱了序列长度和顺序的限制，这使其非常适合两种语言之间的翻译。 考虑一下句子Je ne suis pas le chat noir -> I am not the black cat。 输入句子中的大多数单词在输出句子中具有直接翻译，但是顺序略有不同，例如chat noir和black cat。 由于采用ne/pas结构，因此在输入句子中还有一个单词。 直接从输入单词的序列中产生正确的翻译将是困难的。 使用 seq2seq 模型，编码器创建单个向量，在理想情况下，该向量将输入序列的“含义”编码为单个向量—在句子的 N 维空间中的单个点。 编码器 seq2seq 网络的编码器是 RNN，它为输入句子中的每个单词输出一些值。 对于每个输入字，编码器输出一个向量和一个隐藏状态，并将隐藏状态用于下一个输入字。 class EncoderRNN(nn.Module): def __init__(self, input_size, hidden_size): super(EncoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(input_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) def forward(self, input, hidden): embedded = self.embedding(input).view(1, 1, -1) output = embedded output, hidden = self.gru(output, hidden) return output, hidden def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 解码器 解码器是另一个 RNN，它采用编码器输出向量并输出单词序列来创建翻译。 简单解码器 在最简单的 seq2seq 解码器中，我们仅使用编码器的最后一个输出。 该最后的输出有时称为上下文向量，因为它从整个序列中编码上下文。 该上下文向量用作解码器的初始隐藏状态。 在解码的每个步骤中，为解码器提供输入标记和隐藏状态。 初始输入标记是字符串开始标记，第一个隐藏状态是上下文向量（编码器的最后一个隐藏状态）。 class DecoderRNN(nn.Module): def __init__(self, hidden_size, output_size): super(DecoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(output_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): output = self.embedding(input).view(1, 1, -1) output = F.relu(output) output, hidden = self.gru(output, hidden) output = self.softmax(self.out(output[0])) return output, hidden def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 我鼓励您训练并观察该模型的结果，但是为了节省空间，我们将直接努力并引入注意力机制。 注意力解码器 如果仅上下文向量在编码器和解码器之间传递，则该单个向量承担对整个句子进行编码的负担。 注意使解码器网络可以针对解码器自身输出的每一步，“专注”于编码器输出的不同部分。 首先，我们计算一组注意力权重。 将这些与编码器输出向量相乘以创建加权组合。 结果（在代码中称为attn_applied）应包含有关输入序列特定部分的信息，从而帮助解码器选择正确的输出字。 另一个前馈层attn使用解码器的输入和隐藏状态作为输入来计算注意力权重。 由于训练数据中包含各种大小的句子，因此要实际创建和训练该层，我们必须选择可以应用的最大句子长度（输入长度​​，用于编码器输出）。 最大长度的句子将使用所有注意权重，而较短的句子将仅使用前几个。 class AttnDecoderRNN(nn.Module): def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH): super(AttnDecoderRNN, self).__init__() self.hidden_size = hidden_size self.output_size = output_size self.dropout_p = dropout_p self.max_length = max_length self.embedding = nn.Embedding(self.output_size, self.hidden_size) self.attn = nn.Linear(self.hidden_size * 2, self.max_length) self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) self.dropout = nn.Dropout(self.dropout_p) self.gru = nn.GRU(self.hidden_size, self.hidden_size) self.out = nn.Linear(self.hidden_size, self.output_size) def forward(self, input, hidden, encoder_outputs): embedded = self.embedding(input).view(1, 1, -1) embedded = self.dropout(embedded) attn_weights = F.softmax( self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) output = torch.cat((embedded[0], attn_applied[0]), 1) output = self.attn_combine(output).unsqueeze(0) output = F.relu(output) output, hidden = self.gru(output, hidden) output = F.log_softmax(self.out(output[0]), dim=1) return output, hidden, attn_weights def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 注意 还有其他形式的注意，可以通过使用相对位置方法来解决长度限制问题。 在《基于注意力的神经机器翻译的有效方法》中阅读“本地注意力”。 训练 准备训练数据 为了训练，对于每一对，我们将需要一个输入张量（输入句子中单词的索引）和目标张量（目标句子中单词的索引）。 创建这些向量时，我们会将EOS标记附加到两个序列上。 def indexesFromSentence(lang, sentence): return [lang.word2index[word] for word in sentence.split(' ')] def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1) def tensorsFromPair(pair): input_tensor = tensorFromSentence(input_lang, pair[0]) target_tensor = tensorFromSentence(output_lang, pair[1]) return (input_tensor, target_tensor) 训练模型 为了训练，我们通过编码器运行输入语句，并跟踪每个输出和最新的隐藏状态。 然后，为解码器提供标记作为其第一个输入，为编码器提供最后的隐藏状态作为其第一个隐藏状态。 “教师强制”的概念是使用实际目标输出作为每个下一个输入，而不是使用解码器的猜测作为下一个输入。 使用教师强制会导致其收敛更快，但是当使用受过训练的网络时，可能会显示不稳定。 您可以观察到以教师为主导的网络的输出，这些输出阅读的是连贯的语法，但是却偏离了正确的翻译-直观地，它已经学会了代表输出语法，并且一旦老师说了最初的几个单词就可以“理解”含义，但是首先，它还没有正确地学习如何从翻译中创建句子。 由于 PyTorch 的 Autograd 具有给我们的自由，我们可以通过简单的if语句随意选择是否使用教师强迫。 调高teacher_forcing_ratio以使用更多。 teacher_forcing_ratio = 0.5 def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH): encoder_hidden = encoder.initHidden() encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() input_length = input_tensor.size(0) target_length = target_tensor.size(0) encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) loss = 0 for ei in range(input_length): encoder_output, encoder_hidden = encoder( input_tensor[ei], encoder_hidden) encoder_outputs[ei] = encoder_output[0, 0] decoder_input = torch.tensor([[SOS_token]], device=device) decoder_hidden = encoder_hidden use_teacher_forcing = True if random.random() 这是一个帮助函数，用于在给定当前时间和进度% 的情况下打印经过的时间和估计的剩余时间。 import time import math def asMinutes(s): m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) def timeSince(since, percent): now = time.time() s = now - since es = s / (percent) rs = es - s return '%s (- %s)' % (asMinutes(s), asMinutes(rs)) 整个训练过程如下所示： 启动计时器 初始化优化器和标准 创建一组训练对 启动空损失数组进行绘图 然后，我们多次调用train，并偶尔打印进度（示例的百分比，到目前为止的时间，估计的时间）和平均损失。 def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01): start = time.time() plot_losses = [] print_loss_total = 0 # Reset every print_every plot_loss_total = 0 # Reset every plot_every encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)] criterion = nn.NLLLoss() for iter in range(1, n_iters + 1): training_pair = training_pairs[iter - 1] input_tensor = training_pair[0] target_tensor = training_pair[1] loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) print_loss_total += loss plot_loss_total += loss if iter % print_every == 0: print_loss_avg = print_loss_total / print_every print_loss_total = 0 print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg)) if iter % plot_every == 0: plot_loss_avg = plot_loss_total / plot_every plot_losses.append(plot_loss_avg) plot_loss_total = 0 showPlot(plot_losses) 绘制结果 使用训练时保存的损失值数组plot_losses，使用 matplotlib 进行绘制。 import matplotlib.pyplot as plt plt.switch_backend('agg') import matplotlib.ticker as ticker import numpy as np def showPlot(points): plt.figure() fig, ax = plt.subplots() # this locator puts ticks at regular intervals loc = ticker.MultipleLocator(base=0.2) ax.yaxis.set_major_locator(loc) plt.plot(points) 评估 评估与训练基本相同，但是没有目标，因此我们只需将解码器的预测反馈给每一步。 每当它预测一个单词时，我们都会将其添加到输出字符串中，如果它预测到EOS标记，我们将在此处停止。 我们还将存储解码器的注意输出，以供以后显示。 def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH): with torch.no_grad(): input_tensor = tensorFromSentence(input_lang, sentence) input_length = input_tensor.size()[0] encoder_hidden = encoder.initHidden() encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) for ei in range(input_length): encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) encoder_outputs[ei] += encoder_output[0, 0] decoder_input = torch.tensor([[SOS_token]], device=device) # SOS decoder_hidden = encoder_hidden decoded_words = [] decoder_attentions = torch.zeros(max_length, max_length) for di in range(max_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) decoder_attentions[di] = decoder_attention.data topv, topi = decoder_output.data.topk(1) if topi.item() == EOS_token: decoded_words.append('') break else: decoded_words.append(output_lang.index2word[topi.item()]) decoder_input = topi.squeeze().detach() return decoded_words, decoder_attentions[:di + 1] 我们可以从训练集中评估随机句子，并打印出输入，目标和输出以做出一些主观的质量判断： def evaluateRandomly(encoder, decoder, n=10): for i in range(n): pair = random.choice(pairs) print('>', pair[0]) print('=', pair[1]) output_words, attentions = evaluate(encoder, decoder, pair[0]) output_sentence = ' '.join(output_words) print(' 训练和评估 有了所有这些辅助函数（看起来像是额外的工作，但它使运行多个实验更加容易），我们实际上可以初始化网络并开始训练。 请记住，输入语句已被大量过滤。 对于这个小的数据集，我们可以使用具有 256 个隐藏节点和单个 GRU 层的相对较小的网络。 在 MacBook CPU 上运行约 40 分钟后，我们会得到一些合理的结果。 注意 如果运行此笔记本，则可以进行训练，中断内核，评估并在以后继续训练。 注释掉编码器和解码器已初始化的行，然后再次运行trainIters。 hidden_size = 256 encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device) attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device) trainIters(encoder1, attn_decoder1, 75000, print_every=5000) 出： 2m 6s (- 29m 28s) (5000 6%) 2.8538 4m 7s (- 26m 49s) (10000 13%) 2.3035 6m 10s (- 24m 40s) (15000 20%) 1.9812 8m 13s (- 22m 37s) (20000 26%) 1.7083 10m 15s (- 20m 31s) (25000 33%) 1.5199 12m 17s (- 18m 26s) (30000 40%) 1.3580 14m 18s (- 16m 20s) (35000 46%) 1.2002 16m 18s (- 14m 16s) (40000 53%) 1.0832 18m 21s (- 12m 14s) (45000 60%) 0.9719 20m 22s (- 10m 11s) (50000 66%) 0.8879 22m 23s (- 8m 8s) (55000 73%) 0.8130 24m 25s (- 6m 6s) (60000 80%) 0.7509 26m 27s (- 4m 4s) (65000 86%) 0.6524 28m 27s (- 2m 1s) (70000 93%) 0.6007 30m 30s (- 0m 0s) (75000 100%) 0.5699 evaluateRandomly(encoder1, attn_decoder1) 出： > nous sommes desolees . = we re sorry . > tu plaisantes bien sur . = you re joking of course . > vous etes trop stupide pour vivre . = you re too stupid to live . > c est un scientifique de niveau international . = he s a world class scientist . > j agis pour mon pere . = i am acting for my father . > ils courent maintenant . = they are running now . > je suis tres heureux d etre ici . = i m very happy to be here . > vous etes bonne . = you re good . > il a peur de la mort . = he is afraid of death . > je suis determine a devenir un scientifique . = i am determined to be a scientist . 可视化注意力 注意力机制的一个有用特性是其高度可解释的输出。 因为它用于加权输入序列的特定编码器输出，所以我们可以想象一下在每个时间步长上网络最关注的位置。 您可以简单地运行plt.matshow(attentions)以将注意力输出显示为矩阵，其中列为输入步骤，行为输出步骤： output_words, attentions = evaluate( encoder1, attn_decoder1, \"je suis trop froid .\") plt.matshow(attentions.numpy()) 为了获得更好的观看体验，我们将做一些额外的工作来添加轴和标签： def showAttention(input_sentence, output_words, attentions): # Set up figure with colorbar fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(attentions.numpy(), cmap='bone') fig.colorbar(cax) # Set up axes ax.set_xticklabels([''] + input_sentence.split(' ') + [''], rotation=90) ax.set_yticklabels([''] + output_words) # Show label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) plt.show() def evaluateAndShowAttention(input_sentence): output_words, attentions = evaluate( encoder1, attn_decoder1, input_sentence) print('input =', input_sentence) print('output =', ' '.join(output_words)) showAttention(input_sentence, output_words, attentions) evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\") evaluateAndShowAttention(\"elle est trop petit .\") evaluateAndShowAttention(\"je ne crains pas de mourir .\") evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\") 出： input = elle a cinq ans de moins que moi . output = she s five years younger than i am . input = elle est trop petit . output = she s too loud . input = je ne crains pas de mourir . output = i m not scared to die . input = c est un jeune directeur plein de talent . output = he s a talented young writer . 练习 尝试使用其他数据集 另一对语言 人机 → 机器（例如 IOT 命令） 聊天 → 回复 问题 → 答案 用预训练的单词嵌入（例如 word2vec 或 GloVe）替换嵌入 尝试使用更多的层，更多的隐藏单元和更多的句子。 比较训练时间和结果。 如果您使用翻译对，其中成对具有两个相同的词组（I am test \\t I am test），则可以将其用作自编码器。 尝试这个： 训练为自编码器 仅保存编码器网络 从那里训练新的解码器进行翻译 脚本的总运行时间：（30 分钟 37.929 秒） 下载 Python 源码：seq2seq_translation_tutorial.py 下载 Jupyter 笔记本：seq2seq_translation_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"31.html":{"url":"31.html","title":"使用torchtext的文本分类","keywords":"","body":"使用torchtext的文本分类 原文：https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html 本教程说明如何使用torchtext中的文本分类数据集，包括 - AG_NEWS, - SogouNews, - DBpedia, - YelpReviewPolarity, - YelpReviewFull, - YahooAnswers, - AmazonReviewPolarity, - AmazonReviewFull 此示例显示了如何使用这些TextClassification数据集之一训练用于分类的监督学习算法。 使用 N 元组加载数据 一袋 N 元组特征用于捕获有关本地单词顺序的一些部分信息。 在实践中，应用二元语法或三元语法作为单词组比仅一个单词提供更多的好处。 一个例子： \"load data with ngrams\" Bi-grams results: \"load data\", \"data with\", \"with ngrams\" Tri-grams results: \"load data with\", \"data with ngrams\" TextClassification数据集支持ngrams方法。 通过将ngrams设置为 2，数据集中的示例文本将是一个单字加二元组字符串的列表。 import torch import torchtext from torchtext.datasets import text_classification NGRAMS = 2 import os if not os.path.isdir('./.data'): os.mkdir('./.data') train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS']( root='./.data', ngrams=NGRAMS, vocab=None) BATCH_SIZE = 16 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 定义模型 该模型由EmbeddingBag层和线性层组成（请参见下图）。 nn.EmbeddingBag计算嵌入“袋”的平均值。 此处的文本条目具有不同的长度。 nn.EmbeddingBag此处不需要填充，因为文本长度以偏移量保存。 另外，由于nn.EmbeddingBag会动态累积嵌入中的平均值，因此nn.EmbeddingBag可以提高性能和存储效率，以处理张量序列。 import torch.nn as nn import torch.nn.functional as F class TextSentiment(nn.Module): def __init__(self, vocab_size, embed_dim, num_class): super().__init__() self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) self.fc = nn.Linear(embed_dim, num_class) self.init_weights() def init_weights(self): initrange = 0.5 self.embedding.weight.data.uniform_(-initrange, initrange) self.fc.weight.data.uniform_(-initrange, initrange) self.fc.bias.data.zero_() def forward(self, text, offsets): embedded = self.embedding(text, offsets) return self.fc(embedded) 启动实例 AG_NEWS数据集具有四个标签，因此类别数是四个。 1 : World 2 : Sports 3 : Business 4 : Sci/Tec 词汇的大小等于词汇的长度（包括单个单词和 N 元组）。 类的数量等于标签的数量，在AG_NEWS情况下为 4。 VOCAB_SIZE = len(train_dataset.get_vocab()) EMBED_DIM = 32 NUN_CLASS = len(train_dataset.get_labels()) model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device) 用于生成批量的函数 由于文本条目的长度不同，因此使用自定义函数generate_batch()生成数据批和偏移量。 该函数被传递到torch.utils.data.DataLoader中的collate_fn。 collate_fn的输入是张量列表，其大小为batch_size，collate_fn函数将它们打包成一个小批量。 请注意此处，并确保将collate_fn声明为顶级def。 这样可以确保该函数在每个工作程序中均可用。 原始数据批量输入中的文本条目打包到一个列表中，并作为单个张量级联，作为nn.EmbeddingBag的输入。 偏移量是定界符的张量，表示文本张量中各个序列的起始索引。 Label是一个张量，用于保存单个文本条目的标签。 def generate_batch(batch): label = torch.tensor([entry[0] for entry in batch]) text = [entry[1] for entry in batch] offsets = [0] + [len(entry) for entry in text] # torch.Tensor.cumsum returns the cumulative sum # of elements in the dimension dim. # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0) offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) text = torch.cat(text) return text, offsets, label 定义函数来训练模型并评估结果 建议 PyTorch 用户使用torch.utils.data.DataLoader，它可以轻松地并行加载数据（教程在这里）。 我们在此处使用DataLoader加载AG_NEWS数据集，并将其发送到模型以进行训练/验证。 from torch.utils.data import DataLoader def train_func(sub_train_): # Train the model train_loss = 0 train_acc = 0 data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch) for i, (text, offsets, cls) in enumerate(data): optimizer.zero_grad() text, offsets, cls = text.to(device), offsets.to(device), cls.to(device) output = model(text, offsets) loss = criterion(output, cls) train_loss += loss.item() loss.backward() optimizer.step() train_acc += (output.argmax(1) == cls).sum().item() # Adjust the learning rate scheduler.step() return train_loss / len(sub_train_), train_acc / len(sub_train_) def test(data_): loss = 0 acc = 0 data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch) for text, offsets, cls in data: text, offsets, cls = text.to(device), offsets.to(device), cls.to(device) with torch.no_grad(): output = model(text, offsets) loss = criterion(output, cls) loss += loss.item() acc += (output.argmax(1) == cls).sum().item() return loss / len(data_), acc / len(data_) 分割数据集并运行模型 由于原始的AG_NEWS没有有效的数据集，因此我们将训练数据集分为训练/有效集，其分割比率为 0.95（训练）和 0.05（有效）。 在这里，我们在 PyTorch 核心库中使用torch.utils.data.dataset.random_split函数。 CrossEntropyLoss标准将nn.LogSoftmax()和nn.NLLLoss()合并到一个类中。 在训练带有C类的分类问题时很有用。 SGD实现了随机梯度下降方法作为优化程序。 初始学习率设置为 4.0。 StepLR在此处用于通过历时调整学习率。 import time from torch.utils.data.dataset import random_split N_EPOCHS = 5 min_valid_loss = float('inf') criterion = torch.nn.CrossEntropyLoss().to(device) optimizer = torch.optim.SGD(model.parameters(), lr=4.0) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9) train_len = int(len(train_dataset) * 0.95) sub_train_, sub_valid_ = \\ random_split(train_dataset, [train_len, len(train_dataset) - train_len]) for epoch in range(N_EPOCHS): start_time = time.time() train_loss, train_acc = train_func(sub_train_) valid_loss, valid_acc = test(sub_valid_) secs = int(time.time() - start_time) mins = secs / 60 secs = secs % 60 print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs)) print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)') print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)') 出： Epoch: 1 | time in 0 minutes, 11 seconds Loss: 0.0262(train) | Acc: 84.7%(train) Loss: 0.0002(valid) | Acc: 89.3%(valid) Epoch: 2 | time in 0 minutes, 11 seconds Loss: 0.0119(train) | Acc: 93.6%(train) Loss: 0.0002(valid) | Acc: 89.6%(valid) Epoch: 3 | time in 0 minutes, 11 seconds Loss: 0.0069(train) | Acc: 96.3%(train) Loss: 0.0000(valid) | Acc: 91.8%(valid) Epoch: 4 | time in 0 minutes, 11 seconds Loss: 0.0038(train) | Acc: 98.1%(train) Loss: 0.0000(valid) | Acc: 91.5%(valid) Epoch: 5 | time in 0 minutes, 11 seconds Loss: 0.0022(train) | Acc: 99.0%(train) Loss: 0.0000(valid) | Acc: 91.4%(valid) 使用以下信息在 GPU 上运行模型： 周期：1 | 时间在 0 分 11 秒内 Loss: 0.0263(train) | Acc: 84.5%(train) Loss: 0.0001(valid) | Acc: 89.0%(valid) 周期：2 | 时间在 0 分钟 10 秒内 Loss: 0.0119(train) | Acc: 93.6%(train) Loss: 0.0000(valid) | Acc: 89.6%(valid) 周期：3 | 时间在 0 分钟 9 秒内 Loss: 0.0069(train) | Acc: 96.4%(train) Loss: 0.0000(valid) | Acc: 90.5%(valid) 周期：4 | 时间在 0 分 11 秒内 Loss: 0.0038(train) | Acc: 98.2%(train) Loss: 0.0000(valid) | Acc: 90.4%(valid) 周期：5 | 时间在 0 分 11 秒内 Loss: 0.0022(train) | Acc: 99.0%(train) Loss: 0.0000(valid) | Acc: 91.0%(valid) 使用测试数据集评估模型 print('Checking the results of test dataset...') test_loss, test_acc = test(test_dataset) print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)') 出： Checking the results of test dataset... Loss: 0.0002(test) | Acc: 90.9%(test) 正在检查测试数据集的结果… Loss: 0.0237(test) | Acc: 90.5%(test) 测试随机新闻 使用到目前为止最好的模型并测试高尔夫新闻。 标签信息在这里。 import re from torchtext.data.utils import ngrams_iterator from torchtext.data.utils import get_tokenizer ag_news_label = {1 : \"World\", 2 : \"Sports\", 3 : \"Business\", 4 : \"Sci/Tec\"} def predict(text, model, vocab, ngrams): tokenizer = get_tokenizer(\"basic_english\") with torch.no_grad(): text = torch.tensor([vocab[token] for token in ngrams_iterator(tokenizer(text), ngrams)]) output = model(text, torch.tensor([0])) return output.argmax(1).item() + 1 ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\ enduring the season's worst weather conditions on Sunday at The \\ Open on his way to a closing 75 at Royal Portrush, which \\ considering the wind and the rain was a respectable showing. \\ Thursday's first round at the WGC-FedEx St. Jude Invitational \\ was another story. With temperatures in the mid-80s and hardly any \\ wind, the Spaniard was 13 strokes better in a flawless round. \\ Thanks to his best putting performance on the PGA Tour, Rahm \\ finished with an 8-under 62 for a three-stroke lead, which \\ was even more impressive considering he'd never played the \\ front nine at TPC Southwind.\" vocab = train_dataset.get_vocab() model = model.to(\"cpu\") print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)]) 出： This is a Sports news 这是体育新闻 您可以在此处找到本说明中显示的代码示例。 脚本的总运行时间：（1 分 38.483 秒） 下载 Python 源码：text_sentiment_ngrams_tutorial.py 下载 Jupyter 笔记本：text_sentiment_ngrams_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"32.html":{"url":"32.html","title":"torchtext语言翻译","keywords":"","body":"torchtext语言翻译 原文：https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html 本教程介绍了如何使用torchtext预处理包含英语和德语句子的著名数据集的数据，并使用它来训练序列到序列模型，并能将德语句子翻译成英语。 它基于 PyTorch 社区成员 Ben Trevett 的本教程，并获得 Ben 的许可。 我们通过删除一些旧代码来更新教程。 在本教程结束时，您将可以将句子预处理为张量以用于 NLP 建模，并可以使用torch.utils.data.DataLoader来训练和验证模型。 数据处理 torchtext具有工具，可用于创建可以轻松迭代的数据集，以创建语言翻译模型。 在此示例中，我们展示了如何对原始文本句子进行标记，构建词汇表以及将标记数字化为张量。 注意：本教程中的分词需要 Spacy 我们使用 Spacy 是因为它为英语以外的其他语言的分词提供了强大的支持。 torchtext提供了basic_english标记器，并支持其他英语标记器（例如 Moses），但对于语言翻译（需要多种语言），Spacy 是您的最佳选择。 要运行本教程，请先使用pip或conda安装spacy。 接下来，下载英语和德语 Spacy 分词器的原始数据： python -m spacy download en python -m spacy download de import torchtext import torch from torchtext.data.utils import get_tokenizer from collections import Counter from torchtext.vocab import Vocab from torchtext.utils import download_from_url, extract_archive import io url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/' train_urls = ('train.de.gz', 'train.en.gz') val_urls = ('val.de.gz', 'val.en.gz') test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz') train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls] val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls] test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls] de_tokenizer = get_tokenizer('spacy', language='de') en_tokenizer = get_tokenizer('spacy', language='en') def build_vocab(filepath, tokenizer): counter = Counter() with io.open(filepath, encoding=\"utf8\") as f: for string_ in f: counter.update(tokenizer(string_)) return Vocab(counter, specials=['', '', '', '']) de_vocab = build_vocab(train_filepaths[0], de_tokenizer) en_vocab = build_vocab(train_filepaths[1], en_tokenizer) def data_process(filepaths): raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\")) raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\")) data = [] for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter): de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long) en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long) data.append((de_tensor_, en_tensor_)) return data train_data = data_process(train_filepaths) val_data = data_process(val_filepaths) test_data = data_process(test_filepaths) DataLoader 我们将使用的最后torch个特定函数是DataLoader，它易于使用，因为它将数据作为第一个参数。 具体来说，正如文档所说：DataLoader结合了一个数据集和一个采样器，并在给定的数据集上提供了可迭代的。 DataLoader支持映射样式和可迭代样式的数据集，具有单进程或多进程加载，自定义加载顺序以及可选的自动批量（归类）和内存固定。 请注意collate_fn（可选），它将合并样本列表以形成张量的小批量。 在从映射样式数据集中使用批量加载时使用。 import torch device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') BATCH_SIZE = 128 PAD_IDX = de_vocab[''] BOS_IDX = de_vocab[''] EOS_IDX = de_vocab[''] from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader def generate_batch(data_batch): de_batch, en_batch = [], [] for (de_item, en_item) in data_batch: de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0)) en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0)) de_batch = pad_sequence(de_batch, padding_value=PAD_IDX) en_batch = pad_sequence(en_batch, padding_value=PAD_IDX) return de_batch, en_batch train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch) valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch) test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch) 定义我们的nn.Module和Optimizer 这大部分是从torchtext角度出发的：构建了数据集并定义了迭代器，本教程的其余部分仅将模型定义为nn.Module以及Optimizer，然后对其进行训练。 具体来说，我们的模型遵循此处描述的架构（您可以在这里找到注释更多的版本。 注意：此模型只是可用于语言翻译的示例模型； 我们选择它是因为它是任务的标准模型，而不是因为它是用于翻译的推荐模型。 如您所知，目前最先进的模型基于“转换器”； 您可以看到 PyTorch 的实现Transformer层的功能； 特别是，以下模型中使用的“注意”与转换器模型中存在的多头自我注意不同。 import random from typing import Tuple import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch import Tensor class Encoder(nn.Module): def __init__(self, input_dim: int, emb_dim: int, enc_hid_dim: int, dec_hid_dim: int, dropout: float): super().__init__() self.input_dim = input_dim self.emb_dim = emb_dim self.enc_hid_dim = enc_hid_dim self.dec_hid_dim = dec_hid_dim self.dropout = dropout self.embedding = nn.Embedding(input_dim, emb_dim) self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True) self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim) self.dropout = nn.Dropout(dropout) def forward(self, src: Tensor) -> Tuple[Tensor]: embedded = self.dropout(self.embedding(src)) outputs, hidden = self.rnn(embedded) hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))) return outputs, hidden class Attention(nn.Module): def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int): super().__init__() self.enc_hid_dim = enc_hid_dim self.dec_hid_dim = dec_hid_dim self.attn_in = (enc_hid_dim * 2) + dec_hid_dim self.attn = nn.Linear(self.attn_in, attn_dim) def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor: src_len = encoder_outputs.shape[0] repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1) encoder_outputs = encoder_outputs.permute(1, 0, 2) energy = torch.tanh(self.attn(torch.cat(( repeated_decoder_hidden, encoder_outputs), dim = 2))) attention = torch.sum(energy, dim=2) return F.softmax(attention, dim=1) class Decoder(nn.Module): def __init__(self, output_dim: int, emb_dim: int, enc_hid_dim: int, dec_hid_dim: int, dropout: int, attention: nn.Module): super().__init__() self.emb_dim = emb_dim self.enc_hid_dim = enc_hid_dim self.dec_hid_dim = dec_hid_dim self.output_dim = output_dim self.dropout = dropout self.attention = attention self.embedding = nn.Embedding(output_dim, emb_dim) self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim) self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim) self.dropout = nn.Dropout(dropout) def _weighted_encoder_rep(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor: a = self.attention(decoder_hidden, encoder_outputs) a = a.unsqueeze(1) encoder_outputs = encoder_outputs.permute(1, 0, 2) weighted_encoder_rep = torch.bmm(a, encoder_outputs) weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2) return weighted_encoder_rep def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]: input = input.unsqueeze(0) embedded = self.dropout(self.embedding(input)) weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs) rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2) output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0)) embedded = embedded.squeeze(0) output = output.squeeze(0) weighted_encoder_rep = weighted_encoder_rep.squeeze(0) output = self.out(torch.cat((output, weighted_encoder_rep, embedded), dim = 1)) return output, decoder_hidden.squeeze(0) class Seq2Seq(nn.Module): def __init__(self, encoder: nn.Module, decoder: nn.Module, device: torch.device): super().__init__() self.encoder = encoder self.decoder = decoder self.device = device def forward(self, src: Tensor, trg: Tensor, teacher_forcing_ratio: float = 0.5) -> Tensor: batch_size = src.shape[1] max_len = trg.shape[0] trg_vocab_size = self.decoder.output_dim outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device) encoder_outputs, hidden = self.encoder(src) # first input to the decoder is the token output = trg[0,:] for t in range(1, max_len): output, hidden = self.decoder(output, hidden, encoder_outputs) outputs[t] = output teacher_force = random.random() 出： The model has 3,491,552 trainable parameters 注意：特别是对语言翻译模型的表现进行评分时，我们必须告诉nn.CrossEntropyLoss函数忽略仅填充目标的索引。 PAD_IDX = en_vocab.stoi[''] criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) 最后，我们可以训练和评估该模型： import math import time def train(model: nn.Module, iterator: torch.utils.data.DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, clip: float): model.train() epoch_loss = 0 for _, (src, trg) in enumerate(iterator): src, trg = src.to(device), trg.to(device) optimizer.zero_grad() output = model(src, trg) output = output[1:].view(-1, output.shape[-1]) trg = trg[1:].view(-1) loss = criterion(output, trg) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), clip) optimizer.step() epoch_loss += loss.item() return epoch_loss / len(iterator) def evaluate(model: nn.Module, iterator: torch.utils.data.DataLoader, criterion: nn.Module): model.eval() epoch_loss = 0 with torch.no_grad(): for _, (src, trg) in enumerate(iterator): src, trg = src.to(device), trg.to(device) output = model(src, trg, 0) #turn off teacher forcing output = output[1:].view(-1, output.shape[-1]) trg = trg[1:].view(-1) loss = criterion(output, trg) epoch_loss += loss.item() return epoch_loss / len(iterator) def epoch_time(start_time: int, end_time: int): elapsed_time = end_time - start_time elapsed_mins = int(elapsed_time / 60) elapsed_secs = int(elapsed_time - (elapsed_mins * 60)) return elapsed_mins, elapsed_secs N_EPOCHS = 10 CLIP = 1 best_valid_loss = float('inf') for epoch in range(N_EPOCHS): start_time = time.time() train_loss = train(model, train_iter, optimizer, criterion, CLIP) valid_loss = evaluate(model, valid_iter, criterion) end_time = time.time() epoch_mins, epoch_secs = epoch_time(start_time, end_time) print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s') print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}') print(f'\\t Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}') test_loss = evaluate(model, test_iter, criterion) print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |') 出： Epoch: 01 | Time: 0m 59s Train Loss: 5.790 | Train PPL: 327.039 Val. Loss: 5.250 | Val. PPL: 190.532 Epoch: 02 | Time: 0m 59s Train Loss: 4.762 | Train PPL: 116.990 Val. Loss: 5.037 | Val. PPL: 153.939 Epoch: 03 | Time: 0m 59s Train Loss: 4.527 | Train PPL: 92.475 Val. Loss: 4.924 | Val. PPL: 137.525 Epoch: 04 | Time: 0m 59s Train Loss: 4.344 | Train PPL: 76.977 Val. Loss: 4.801 | Val. PPL: 121.673 Epoch: 05 | Time: 0m 59s Train Loss: 4.210 | Train PPL: 67.356 Val. Loss: 4.758 | Val. PPL: 116.536 Epoch: 06 | Time: 0m 59s Train Loss: 4.125 | Train PPL: 61.875 Val. Loss: 4.691 | Val. PPL: 109.004 Epoch: 07 | Time: 0m 59s Train Loss: 4.043 | Train PPL: 56.979 Val. Loss: 4.639 | Val. PPL: 103.446 Epoch: 08 | Time: 0m 59s Train Loss: 3.947 | Train PPL: 51.771 Val. Loss: 4.589 | Val. PPL: 98.396 Epoch: 09 | Time: 0m 59s Train Loss: 3.874 | Train PPL: 48.135 Val. Loss: 4.514 | Val. PPL: 91.324 Epoch: 10 | Time: 0m 59s Train Loss: 3.785 | Train PPL: 44.021 Val. Loss: 4.467 | Val. PPL: 87.126 | Test Loss: 4.433 | Test PPL: 84.168 | 后续步骤 查看其余的 Ben Trevett 的torchtext使用教程。 敬请关注使用其他torchtext函数以及nn.Transformer通过下一个单词预测进行语言建模的教程！ 脚本的总运行时间：（10 分钟 13.398 秒） 下载 Python 源码：torchtext_translation_tutorial.py 下载 Jupyter 笔记本：torchtext_translation_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"33.html":{"url":"33.html","title":"强化学习","keywords":"","body":"强化学习 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"34.html":{"url":"34.html","title":"强化学习（DQN）教程","keywords":"","body":"强化学习（DQN）教程 原文：https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html 作者： Adam Paszke 本教程说明如何使用 PyTorch 在 OpenAI Gym 上的 CartPole-v0 任务上训练深度 Q 学习（DQN）智能体。 任务 智能体必须在两个动作之间做出决定-向左或向右移动推车-以便使与之相连的杆子保持直立。 您可以在 Gym 网站上找到具有各种算法和可视化效果的官方排行榜。 卡特波尔 当智能体观察环境的当前状态并选择一个动作时，环境会转换为到新状态，并且还会返回表示该动作后果的奖励。 在此任务中，每增加一个时间步长，奖励为 +1，并且如果杆子掉落得太远或手推车离中心的距离超过 2.4 个单位，则环境终止。 这意味着表现更好的方案将持续更长的时间，从而积累更大的回报。 对 CartPole 任务进行了设计，以使对智能体的输入是代表环境状态（位置，速度等）的 4 个实际值。 但是，神经网络可以完全通过查看场景来解决任务，因此我们将以推车为中心的一部分屏幕作为输入。 因此，我们的结果无法直接与官方排行榜上的结果进行比较-我们的任务更加艰巨。 不幸的是，这确实减慢了训练速度，因为我们必须渲染所有帧。 严格来说，我们将状态显示为当前屏幕补丁与前一个屏幕补丁之间的差异。 这将允许智能体从一张图像中考虑极点的速度。 包 首先，让我们导入所需的包。 首先，我们需要针对环境的 Gym（使用pip install Gym进行安装）。 我们还将使用 PyTorch 中的以下内容： 神经网络（torch.nn） 优化（torch.optim） 自动微分（torch.autograd） 视觉任务的工具（torchvision-单独的包）。 import gym import math import random import numpy as np import matplotlib import matplotlib.pyplot as plt from collections import namedtuple from itertools import count from PIL import Image import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision.transforms as T env = gym.make('CartPole-v0').unwrapped # set up matplotlib is_ipython = 'inline' in matplotlib.get_backend() if is_ipython: from IPython import display plt.ion() # if gpu is to be used device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 回放记忆 我们将使用经验回放记忆来训练我们的 DQN。 它存储智能体观察到的转换，使我们以后可以重用此数据。 通过从中随机采样，可以构建批量的转换相关。 已经表明，这极大地稳定和改善了 DQN 训练程序。 为此，我们将需要两个类： Transition-表示我们环境中单个过渡的命名元组。 它本质上将（状态，动作）对映射到其（下一个状态，奖励）结果，该状态是屏幕差异图像，如下所述。 ReplayMemory-有界大小的循环缓冲区，用于保存最近观察到的转换。 它还实现了.sample()方法，用于选择随机的过渡批量进行训练。 Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) class ReplayMemory(object): def __init__(self, capacity): self.capacity = capacity self.memory = [] self.position = 0 def push(self, *args): \"\"\"Saves a transition.\"\"\" if len(self.memory) 现在，让我们定义我们的模型。 但是首先，让我们快速回顾一下 DQN 是什么。 DQN 算法 我们的环境是确定性的，因此为简单起见，此处介绍的所有方程式也都确定性地制定。 在强化学习文献中，它们还将包含对环境中随机转变的期望。 我们的目标是制定一种策略，尝试最大化折扣的累积奖励R[t[0]] = Σ γ^(t - t[0]) r[t], t = t[0] -> ∞，其中R[t[0]]也称为回报。 折扣γ应该是0和1之间的常数，以确保总和收敛。 这使得来自不确定的遥远未来的回报对我们的智能体而言不如可以对其充满信心的近期回报重要。 Q 学习的主要思想是，如果我们有一个函数Q*：State x Action => R，这可以告诉我们，如果我们在给定状态下采取行动，那么我们就可以轻松地制定出使我们的回报最大化的策略： 但是，我们对世界一无所知，因此无法访问Q*。 但是，由于神经网络是通用函数逼近器，因此我们可以简单地创建一个并将其训练为类似于Q*的函数。 对于我们的训练更新规则，我们将使用一个事实，即某些策略的每个Q函数都遵循贝尔曼方程： 等式两侧之间的差异称为时间差异误差delta： 为了最小化此误差，我们将使用 Huber 损失。 当误差较小时，Huber 损失的作用类似于均方误差，而当误差较大时，则表现为平均绝对误差-当Q的估计值非常嘈杂时，这使它对异常值的鲁棒性更高。 我们通过从重播内存中采样的一批过渡B来计算： Q 网络 我们的模型将是一个卷积神经网络，该卷积神经网络将吸收当前屏幕补丁和先前屏幕补丁之间的差异。 它有两个输出，分别代表Q(s, left)和Q(s, right)（其中s是网络的输入）。 实际上，网络正在尝试预测在给定当前输入的情况下执行每个操作的预期收益。 class DQN(nn.Module): def __init__(self, h, w, outputs): super(DQN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2) self.bn2 = nn.BatchNorm2d(32) self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2) self.bn3 = nn.BatchNorm2d(32) # Number of Linear input connections depends on output of conv2d layers # and therefore the input image size, so compute it. def conv2d_size_out(size, kernel_size = 5, stride = 2): return (size - (kernel_size - 1) - 1) // stride + 1 convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w))) convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h))) linear_input_size = convw * convh * 32 self.head = nn.Linear(linear_input_size, outputs) # Called with either one element to determine next action, or a batch # during optimization. Returns tensor([[left0exp,right0exp]...]). def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) return self.head(x.view(x.size(0), -1)) 输入提取 以下代码是用于从环境中提取和处理渲染图像的工具。 它使用torchvision包，可轻松组成图像变换。 一旦运行单元，它将显示它提取的示例补丁。 resize = T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()]) def get_cart_location(screen_width): world_width = env.x_threshold * 2 scale = screen_width / world_width return int(env.state[0] * scale + screen_width / 2.0) # MIDDLE OF CART def get_screen(): # Returned screen requested by gym is 400x600x3, but is sometimes larger # such as 800x1200x3\\. Transpose it into torch order (CHW). screen = env.render(mode='rgb_array').transpose((2, 0, 1)) # Cart is in the lower half, so strip off the top and bottom of the screen _, screen_height, screen_width = screen.shape screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)] view_width = int(screen_width * 0.6) cart_location = get_cart_location(screen_width) if cart_location (screen_width - view_width // 2): slice_range = slice(-view_width, None) else: slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2) # Strip off the edges, so that we have a square image centered on a cart screen = screen[:, :, slice_range] # Convert to float, rescale, convert to torch tensor # (this doesn't require a copy) screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 screen = torch.from_numpy(screen) # Resize, and add a batch dimension (BCHW) return resize(screen).unsqueeze(0).to(device) env.reset() plt.figure() plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none') plt.title('Example extracted screen') plt.show() 训练 超参数和工具 该单元实例化我们的模型及其优化器，并定义一些工具： select_action-将根据 ε 贪婪策略选择一个动作。 简而言之，有时我们会使用模型来选择操作，有时我们会统一采样。 选择随机动作的可能性将从EPS_START开始，并朝EPS_END呈指数衰减。 EPS_DECAY控制衰减率。 plot_durations-绘制剧集持续时间以及最近 100 个剧集的平均值（官方评估中使用的度量）的助手。 该图将在包含主要训练循环的单元下面，并且将在每个剧集之后更新。 BATCH_SIZE = 128 GAMMA = 0.999 EPS_START = 0.9 EPS_END = 0.05 EPS_DECAY = 200 TARGET_UPDATE = 10 # Get screen size so that we can initialize layers correctly based on shape # returned from AI gym. Typical dimensions at this point are close to 3x40x90 # which is the result of a clamped and down-scaled render buffer in get_screen() init_screen = get_screen() _, _, screen_height, screen_width = init_screen.shape # Get number of actions from gym action space n_actions = env.action_space.n policy_net = DQN(screen_height, screen_width, n_actions).to(device) target_net = DQN(screen_height, screen_width, n_actions).to(device) target_net.load_state_dict(policy_net.state_dict()) target_net.eval() optimizer = optim.RMSprop(policy_net.parameters()) memory = ReplayMemory(10000) steps_done = 0 def select_action(state): global steps_done sample = random.random() eps_threshold = EPS_END + (EPS_START - EPS_END) * \\ math.exp(-1\\. * steps_done / EPS_DECAY) steps_done += 1 if sample > eps_threshold: with torch.no_grad(): # t.max(1) will return largest column value of each row. # second column on max result is index of where max element was # found, so we pick action with the larger expected reward. return policy_net(state).max(1)[1].view(1, 1) else: return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long) episode_durations = [] def plot_durations(): plt.figure(2) plt.clf() durations_t = torch.tensor(episode_durations, dtype=torch.float) plt.title('Training...') plt.xlabel('Episode') plt.ylabel('Duration') plt.plot(durations_t.numpy()) # Take 100 episode averages and plot them too if len(durations_t) >= 100: means = durations_t.unfold(0, 100, 1).mean(1).view(-1) means = torch.cat((torch.zeros(99), means)) plt.plot(means.numpy()) plt.pause(0.001) # pause a bit so that plots are updated if is_ipython: display.clear_output(wait=True) display.display(plt.gcf()) 训练循环 最后，是训练模型的代码。 在这里，您可以找到执行优化步骤的optimize_model函数。 它首先对一批进行采样，将所有张量连接为一个张量，计算Q(s[t], a[t])和V(s[t+1])= max[a] Q(s[t+1], a)，并将其合并为我们的损失。 根据定义，如果s为终端状态，则设置V(s) = 0。 我们还使用目标网络来计算V(s[t+1])，以提高稳定性。 目标网络的权重大部分时间保持冻结状态，但经常更新以策略网络的权重。 通常这是一组固定的步骤，但是为了简单起见，我们将使用剧集。 def optimize_model(): if len(memory) 在下面，您可以找到主要的训练循环。 首先，我们重置环境并初始化state张量。 然后，我们采样一个动作，执行它，观察下一个屏幕和奖励（总是 1），并一次优化我们的模型。 当剧集结束（我们的模型失败）时，我们重新开始循环。 下面，将num_episodes设置得较小。 您应该下载笔记本并运行更多的片段，例如 300 多个片段，才能显着改善持续时间。 num_episodes = 50 for i_episode in range(num_episodes): # Initialize the environment and state env.reset() last_screen = get_screen() current_screen = get_screen() state = current_screen - last_screen for t in count(): # Select and perform an action action = select_action(state) _, reward, done, _ = env.step(action.item()) reward = torch.tensor([reward], device=device) # Observe new state last_screen = current_screen current_screen = get_screen() if not done: next_state = current_screen - last_screen else: next_state = None # Store the transition in memory memory.push(state, action, next_state, reward) # Move to the next state state = next_state # Perform one step of the optimization (on the target network) optimize_model() if done: episode_durations.append(t + 1) plot_durations() break # Update the target network, copying all weights and biases in DQN if i_episode % TARGET_UPDATE == 0: target_net.load_state_dict(policy_net.state_dict()) print('Complete') env.render() env.close() plt.ioff() plt.show() 这是说明总体结果数据流的图。 可以随机选择或根据策略选择动作，从健身环境中获取下一步样本。 我们将结果记录在重播内存中，并在每次迭代时运行优化步骤。 优化会从重播内存中随机抽取一批来进行新策略的训练。 “较旧”的target_net也用于优化计算期望的 Q 值； 有时会对其进行更新以使其保持最新状态。 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：reinforcement_q_learning.py 下载 Jupyter 笔记本：reinforcement_q_learning.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"35.html":{"url":"35.html","title":"训练玩马里奥的 RL 智能体","keywords":"","body":"训练玩马里奥的 RL 智能体 原文：https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html Authors: Yuansong Feng , Suraj Subramanian , Howard Wang , Steven Guo . 本教程将向您介绍深度强化学习的基础知识。 最后，您将实现一个 AI 驱动的马里奥（使用双重深度 Q 网络），它可以自己玩游戏。 尽管本教程不需要任何有关 RL 的先验知识，但是您可以熟悉这些 RL 概念，并将此方便的备忘单作为您的伴侣。完整代码可在此处获得。 # !pip install gym-super-mario-bros==7.3.0 import torch from torch import nn from torchvision import transforms as T from PIL import Image import numpy as np from pathlib import Path from collections import deque import random, datetime, os, copy # Gym is an OpenAI toolkit for RL import gym from gym.spaces import Box from gym.wrappers import FrameStack # NES Emulator for OpenAI Gym from nes_py.wrappers import JoypadSpace # Super Mario environment for OpenAI Gym import gym_super_mario_bros RL 定义 环境：智能体与之交互并学习的世界。 操作a：智能体如何响应环境。 所有可能动作的集合称为动作空间。 状态s：环境的当前特征。 环境可以处于的所有可能状态的集合称为状态空间。 奖励r：奖励是从环境到智能体的关键反馈。 这是驱动智能体学习并改变其未来行动的动力。 多个时间步长上的奖励汇总称为回报。 最佳操作的值函数Q*(s, a)：如果您以状态s开始，执行任意操作a并给出期望的回报， 然后针对每个未来时间步长采取使收益最大化的行动。 可以说Q代表状态中动作的“质量”。 我们尝试近似该函数。 环境 初始化环境 在马里奥，环境由试管，蘑菇和其他成分组成。 当马里奥采取行动时，环境会以已更改的（下一个）状态，奖励和其他信息作为响应。 # Initialize Super Mario environment env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\") # Limit the action-space to # 0\\. walk right # 1\\. jump right env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]]) env.reset() next_state, reward, done, info = env.step(action=0) print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\") 出： (240, 256, 3), 0, False, {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79} 预处理环境 环境数据在next_state中返回给智能体。 正如您在上面看到的，每个状态都由[3, 240, 256]大小数组表示。 通常，这比我们的智能体需要的信息更多； 例如，马里奥的举动并不取决于管道或天空的颜色！ 我们使用包装程序在将环境数据发送到智能体之前对其进行预处理。 GrayScaleObservation是将 RGB 图像转换为灰度的通用包装器； 这样做可以减少状态表示的大小，而不会丢失有用的信息。 现在每个状态的大小：[1, 240, 256] ResizeObservation将每个观察值下采样为正方形图像。 新尺寸：[1, 84, 84] SkipFrame是一个自定义包装器，它继承自gym.Wrapper并实现了step()函数。 由于连续的帧变化不大，因此我们可以跳过 n 个中间帧而不会丢失太多信息。 第 n 帧聚集在每个跳过的帧上累积的奖励。 FrameStack是一个包装器，它使我们可以将环境的连续帧压缩到单个观察点中，以提供给我们的学习模型。 这样，我们可以根据前几个帧中马里奥的运动方向来确定马里奥是在降落还是跳跃。 class SkipFrame(gym.Wrapper): def __init__(self, env, skip): \"\"\"Return only every `skip`-th frame\"\"\" super().__init__(env) self._skip = skip def step(self, action): \"\"\"Repeat action, and sum reward\"\"\" total_reward = 0.0 done = False for i in range(self._skip): # Accumulate reward and repeat the same action obs, reward, done, info = self.env.step(action) total_reward += reward if done: break return obs, total_reward, done, info class GrayScaleObservation(gym.ObservationWrapper): def __init__(self, env): super().__init__(env) obs_shape = self.observation_space.shape[:2] self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8) def permute_orientation(self, observation): # permute [H, W, C] array to [C, H, W] tensor observation = np.transpose(observation, (2, 0, 1)) observation = torch.tensor(observation.copy(), dtype=torch.float) return observation def observation(self, observation): observation = self.permute_orientation(observation) transform = T.Grayscale() observation = transform(observation) return observation class ResizeObservation(gym.ObservationWrapper): def __init__(self, env, shape): super().__init__(env) if isinstance(shape, int): self.shape = (shape, shape) else: self.shape = tuple(shape) obs_shape = self.shape + self.observation_space.shape[2:] self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8) def observation(self, observation): transforms = T.Compose( [T.Resize(self.shape), T.Normalize(0, 255)] ) observation = transforms(observation).squeeze(0) return observation # Apply Wrappers to environment env = SkipFrame(env, skip=4) env = GrayScaleObservation(env) env = ResizeObservation(env, shape=84) env = FrameStack(env, num_stack=4) 将上述包装纸应用于环境后，最终的包装状态由 4 个灰度连续的帧堆叠在一起组成，如左图所示。 每次马里奥采取行动时，环境都会以这种结构的状态做出响应。 该结构由大小为[4, 84, 84]的 3D 数组表示。 智能体 我们创建一个类Mario来表示我们的智能体在游戏中。 马里奥应该能够： 根据（环境的）当前状态，执行最佳操作策略。 记住经验。 经验为（当前状态，当前动作，奖励，下一个状态）。 马里奥缓存并且后来回忆起他的经验来更新其行动策略。 逐步了解更好的操作策略 class Mario: def __init__(): pass def act(self, state): \"\"\"Given a state, choose an epsilon-greedy action\"\"\" pass def cache(self, experience): \"\"\"Add the experience to memory\"\"\" pass def recall(self): \"\"\"Sample experiences from memory\"\"\" pass def learn(self): \"\"\"Update online action value (Q) function with a batch of experiences\"\"\" pass 在以下各节中，我们将填充马里奥的参数并定义其函数。 行动 对于任何给定状态，智能体都可以选择执行最佳操作（利用）或执行随机操作（探索）。 马里奥随机发掘并发self.exploration_rate 当他选择利用时，他依靠MarioNet（在Learn部分中实现）提供最佳操作。 class Mario: def __init__(self, state_dim, action_dim, save_dir): self.state_dim = state_dim self.action_dim = action_dim self.save_dir = save_dir self.use_cuda = torch.cuda.is_available() # Mario's DNN to predict the most optimal action - we implement this in the Learn section self.net = MarioNet(self.state_dim, self.action_dim).float() if self.use_cuda: self.net = self.net.to(device=\"cuda\") self.exploration_rate = 1 self.exploration_rate_decay = 0.99999975 self.exploration_rate_min = 0.1 self.curr_step = 0 self.save_every = 5e5 # no. of experiences between saving Mario Net def act(self, state): \"\"\" Given a state, choose an epsilon-greedy action and update value of step. Inputs: state(LazyFrame): A single observation of the current state, dimension is (state_dim) Outputs: action_idx (int): An integer representing which action Mario will perform \"\"\" # EXPLORE if np.random.rand() 缓存和回忆 这两个函数是马里奥的“记忆”过程。 cache()：每次马里奥执行操作时，都会将experience存储到他的内存中。 他的经验包括当前状态，动作，从动作中获得的奖励，下一个状态以及游戏是否为完成。 recall()：马里奥从他的记忆中随机抽取一批经验，并以此来学习游戏。 class Mario(Mario): # subclassing for continuity def __init__(self, state_dim, action_dim, save_dir): super().__init__(state_dim, action_dim, save_dir) self.memory = deque(maxlen=100000) self.batch_size = 32 def cache(self, state, next_state, action, reward, done): \"\"\" Store the experience to self.memory (replay buffer) Inputs: state (LazyFrame), next_state (LazyFrame), action (int), reward (float), done(bool)) \"\"\" state = state.__array__() next_state = next_state.__array__() if self.use_cuda: state = torch.tensor(state).cuda() next_state = torch.tensor(next_state).cuda() action = torch.tensor([action]).cuda() reward = torch.tensor([reward]).cuda() done = torch.tensor([done]).cuda() else: state = torch.tensor(state) next_state = torch.tensor(next_state) action = torch.tensor([action]) reward = torch.tensor([reward]) done = torch.tensor([done]) self.memory.append((state, next_state, action, reward, done,)) def recall(self): \"\"\" Retrieve a batch of experiences from memory \"\"\" batch = random.sample(self.memory, self.batch_size) state, next_state, action, reward, done = map(torch.stack, zip(*batch)) return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze() 学习 马里奥在后台使用 DDQN 算法。 DDQN 使用两个 ConvNet-Q_online和Q_target-独立地逼近最佳作用值函数。 在我们的实现中，我们在Q_online和Q_target之间共享特征生成器features，但是为每个特征维护单独的 FC 分类器。 θ_target（Q_target的参数）被冻结，以防止反向传播进行更新。 而是定期与θ_online同步（稍后会对此进行详细介绍）。 神经网络 class MarioNet(nn.Module): \"\"\"mini cnn structure input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output \"\"\" def __init__(self, input_dim, output_dim): super().__init__() c, h, w = input_dim if h != 84: raise ValueError(f\"Expecting input height: 84, got: {h}\") if w != 84: raise ValueError(f\"Expecting input width: 84, got: {w}\") self.online = nn.Sequential( nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4), nn.ReLU(), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(), nn.Flatten(), nn.Linear(3136, 512), nn.ReLU(), nn.Linear(512, output_dim), ) self.target = copy.deepcopy(self.online) # Q_target parameters are frozen. for p in self.target.parameters(): p.requires_grad = False def forward(self, input, model): if model == \"online\": return self.online(input) elif model == \"target\": return self.target(input) TD 估计和 TD 目标 学习涉及两个值： TD 估计-给定状态s的预测最佳Q* TD 目标-当前奖励和下一状态s'中的估计Q*的汇总 由于我们不知道下一个动作a'是什么，因此我们在下一个状态s'中使用动作a'最大化Q_online。 请注意，我们在td_target()上使用了@torch.no_grad()装饰器来禁用梯度计算（因为我们无需在θ_target上进行反向传播。） class Mario(Mario): def __init__(self, state_dim, action_dim, save_dir): super().__init__(state_dim, action_dim, save_dir) self.gamma = 0.9 def td_estimate(self, state, action): current_Q = self.net(state, model=\"online\")[ np.arange(0, self.batch_size), action ] # Q_online(s,a) return current_Q @torch.no_grad() def td_target(self, reward, next_state, done): next_state_Q = self.net(next_state, model=\"online\") best_action = torch.argmax(next_state_Q, axis=1) next_Q = self.net(next_state, model=\"target\")[ np.arange(0, self.batch_size), best_action ] return (reward + (1 - done.float()) * self.gamma * next_Q).float() 更新模型 当马里奥从其重播缓冲区中采样输入时，我们计算TD_t和TD_e并反向传播该损失Q_online以更新其参数θ_online（\\ （\\ alpha \\）是传递给optimizer的学习率lr） θ_target不会通过反向传播进行更新。 相反，我们会定期将θ_online复制到θ_target class Mario(Mario): def __init__(self, state_dim, action_dim, save_dir): super().__init__(state_dim, action_dim, save_dir) self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025) self.loss_fn = torch.nn.SmoothL1Loss() def update_Q_online(self, td_estimate, td_target): loss = self.loss_fn(td_estimate, td_target) self.optimizer.zero_grad() loss.backward() self.optimizer.step() return loss.item() def sync_Q_target(self): self.net.target.load_state_dict(self.net.online.state_dict()) 保存检查点 class Mario(Mario): def save(self): save_path = ( self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\" ) torch.save( dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), save_path, ) print(f\"MarioNet saved to {save_path} at step {self.curr_step}\") 全部放在一起 class Mario(Mario): def __init__(self, state_dim, action_dim, save_dir): super().__init__(state_dim, action_dim, save_dir) self.burnin = 1e4 # min. experiences before training self.learn_every = 3 # no. of experiences between updates to Q_online self.sync_every = 1e4 # no. of experiences between Q_target & Q_online sync def learn(self): if self.curr_step % self.sync_every == 0: self.sync_Q_target() if self.curr_step % self.save_every == 0: self.save() if self.curr_step 日志记录 import numpy as np import time, datetime import matplotlib.pyplot as plt class MetricLogger: def __init__(self, save_dir): self.save_log = save_dir / \"log\" with open(self.save_log, \"w\") as f: f.write( f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\" f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\" f\"{'TimeDelta':>15}{'Time':>20}\\n\" ) self.ep_rewards_plot = save_dir / \"reward_plot.jpg\" self.ep_lengths_plot = save_dir / \"length_plot.jpg\" self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\" self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\" # History metrics self.ep_rewards = [] self.ep_lengths = [] self.ep_avg_losses = [] self.ep_avg_qs = [] # Moving averages, added for every call to record() self.moving_avg_ep_rewards = [] self.moving_avg_ep_lengths = [] self.moving_avg_ep_avg_losses = [] self.moving_avg_ep_avg_qs = [] # Current episode metric self.init_episode() # Timing self.record_time = time.time() def log_step(self, reward, loss, q): self.curr_ep_reward += reward self.curr_ep_length += 1 if loss: self.curr_ep_loss += loss self.curr_ep_q += q self.curr_ep_loss_length += 1 def log_episode(self): \"Mark end of episode\" self.ep_rewards.append(self.curr_ep_reward) self.ep_lengths.append(self.curr_ep_length) if self.curr_ep_loss_length == 0: ep_avg_loss = 0 ep_avg_q = 0 else: ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5) ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5) self.ep_avg_losses.append(ep_avg_loss) self.ep_avg_qs.append(ep_avg_q) self.init_episode() def init_episode(self): self.curr_ep_reward = 0.0 self.curr_ep_length = 0 self.curr_ep_loss = 0.0 self.curr_ep_q = 0.0 self.curr_ep_loss_length = 0 def record(self, episode, epsilon, step): mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3) mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3) mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3) mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3) self.moving_avg_ep_rewards.append(mean_ep_reward) self.moving_avg_ep_lengths.append(mean_ep_length) self.moving_avg_ep_avg_losses.append(mean_ep_loss) self.moving_avg_ep_avg_qs.append(mean_ep_q) last_record_time = self.record_time self.record_time = time.time() time_since_last_record = np.round(self.record_time - last_record_time, 3) print( f\"Episode {episode} - \" f\"Step {step} - \" f\"Epsilon {epsilon} - \" f\"Mean Reward {mean_ep_reward} - \" f\"Mean Length {mean_ep_length} - \" f\"Mean Loss {mean_ep_loss} - \" f\"Mean Q Value {mean_ep_q} - \" f\"Time Delta {time_since_last_record} - \" f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\" ) with open(self.save_log, \"a\") as f: f.write( f\"{episode:8d}{step:8d}{epsilon:10.3f}\" f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\" f\"{time_since_last_record:15.3f}\" f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\" ) for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]: plt.plot(getattr(self, f\"moving_avg_{metric}\")) plt.savefig(getattr(self, f\"{metric}_plot\")) plt.clf() 开始吧！ 在此示例中，我们运行了 10 个剧集的训练循环，但是对于马里奥要真正了解他的世界的方式，我们建议运行至少 40,000 个剧集的循环！ use_cuda = torch.cuda.is_available() print(f\"Using CUDA: {use_cuda}\") print() save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") save_dir.mkdir(parents=True) mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir) logger = MetricLogger(save_dir) episodes = 10 for e in range(episodes): state = env.reset() # Play the game! while True: # Run agent on the state action = mario.act(state) # Agent performs action next_state, reward, done, info = env.step(action) # Remember mario.cache(state, next_state, action, reward, done) # Learn q, loss = mario.learn() # Logging logger.log_step(reward, loss, q) # Update state state = next_state # Check if end of game if done or info[\"flag_get\"]: break logger.log_episode() if e % 20 == 0: logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step) 出： Using CUDA: True Episode 0 - Step 40 - Epsilon 0.9999900000487484 - Mean Reward 231.0 - Mean Length 40.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.444 - Time 2021-01-05T20:23:08 总结 在本教程中，我们看到了如何使用 PyTorch 来训练玩游戏的 AI。 您可以使用相同的方法训练 AI 在 OpenAI Gym上玩任何游戏。 希望您喜欢本教程，请随时通过我们的 Github 与我们联系！ 脚本的总运行时间：（0 分钟 21.485 秒） 下载 Python 源码：mario_rl_tutorial.py 下载 Jupyter 笔记本：mario_rl_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"36.html":{"url":"36.html","title":"在生产中部署 PyTorch 模型","keywords":"","body":"在生产中部署 PyTorch 模型 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"37.html":{"url":"37.html","title":"通过使用 Flask 的 REST API 在 Python 中部署 PyTorch","keywords":"","body":"通过使用 Flask 的 REST API 在 Python 中部署 PyTorch 原文：https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html 作者： Avinash Sajjanshetty 在本教程中，我们将使用 Flask 部署 PyTorch 模型，并公开用于模型推理的 REST API。 特别是，我们将部署预训练的 DenseNet 121 模型来检测图像。 小费 此处使用的所有代码均以 MIT 许可发布，可在 Github 上找到。 这是在生产中部署 PyTorch 模型的系列教程中的第一篇。 到目前为止，以这种方式使用 Flask 是开始为 PyTorch 模型提供服务的最简单方法，但不适用于具有高性能要求的用例。 为了那个原因： 如果您已经熟悉 TorchScript，则可以直接进入我们的通过 C++ 加载 TorchScript 模型的教程。 如果您首先需要在 TorchScript 上进行复习，请查看我们的 TorchScript 入门教程。 API 定义 我们将首先定义 API 端点，请求和响应类型。 我们的 API 端点将位于/predict，它通过包含图片的file参数接受 HTTP POST 请求。 响应将是包含预测的 JSON 响应： {\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"} 依赖项 通过运行以下命令来安装所需的依赖项： $ pip install Flask==1.0.3 torchvision-0.3.0 简单的 Web 服务器 以下是一个简单的网络服务器，摘自 Flask 的文档 from flask import Flask app = Flask(__name__) @app.route('/') def hello(): return 'Hello World!' 将以上代码段保存在名为app.py的文件中，您现在可以通过输入以下内容来运行 Flask 开发服务器： $ FLASK_ENV=development FLASK_APP=app.py flask run 当您在网络浏览器中访问http://localhost:5000/时，您会看到Hello World!文字 我们将对上面的代码片段进行一些更改，以使其适合我们的 API 定义。 首先，我们将方法重命名为predict。 我们将端点路径更新为/predict。 由于图像文件将通过 HTTP POST 请求发送，因此我们将对其进行更新，使其也仅接受 POST 请求： @app.route('/predict', methods=['POST']) def predict(): return 'Hello World!' 我们还将更改响应类型，以使其返回包含 ImageNet 类 ID 和名称的 JSON 响应。 更新后的app.py文件现在为： from flask import Flask, jsonify app = Flask(__name__) @app.route('/predict', methods=['POST']) def predict(): return jsonify({'class_id': 'IMAGE_NET_XXX', 'class_name': 'Cat'}) 推断 在下一部分中，我们将重点介绍编写推理代码。 这将涉及两部分，第一部分是准备图像，以便可以将其馈送到 DenseNet；第二部分，我们将编写代码以从模型中获取实际的预测。 准备图像 DenseNet 模型要求图像为尺寸为224 x 224的 3 通道 RGB 图像。我们还将使用所需的均值和标准差值对图像张量进行归一化。 您可以在上阅读有关它的更多信息。 我们将使用torchvision库中的transforms并建立一个转换管道，该转换管道可根据需要转换图像。 您可以这里阅读有关转换的更多信息。 import io import torchvision.transforms as transforms from PIL import Image def transform_image(image_bytes): my_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) image = Image.open(io.BytesIO(image_bytes)) return my_transforms(image).unsqueeze(0) 上面的方法以字节为单位获取图像数据，应用一系列变换并返回张量。 要测试上述方法，请以字节模式读取图像文件（首先将../_static/img/sample_file.jpeg替换为计算机上文件的实际路径），然后查看是否取回张量： with open(\"../_static/img/sample_file.jpeg\", 'rb') as f: image_bytes = f.read() tensor = transform_image(image_bytes=image_bytes) print(tensor) 出： tensor([[[[ 0.4508, 0.4166, 0.3994, ..., -1.3473, -1.3302, -1.3473], [ 0.5364, 0.4851, 0.4508, ..., -1.2959, -1.3130, -1.3302], [ 0.7077, 0.6392, 0.6049, ..., -1.2959, -1.3302, -1.3644], ..., [ 1.3755, 1.3927, 1.4098, ..., 1.1700, 1.3584, 1.6667], [ 1.8893, 1.7694, 1.4440, ..., 1.2899, 1.4783, 1.5468], [ 1.6324, 1.8379, 1.8379, ..., 1.4783, 1.7352, 1.4612]], [[ 0.5728, 0.5378, 0.5203, ..., -1.3704, -1.3529, -1.3529], [ 0.6604, 0.6078, 0.5728, ..., -1.3004, -1.3179, -1.3354], [ 0.8529, 0.7654, 0.7304, ..., -1.3004, -1.3354, -1.3704], ..., [ 1.4657, 1.4657, 1.4832, ..., 1.3256, 1.5357, 1.8508], [ 2.0084, 1.8683, 1.5182, ..., 1.4657, 1.6583, 1.7283], [ 1.7458, 1.9384, 1.9209, ..., 1.6583, 1.9209, 1.6408]], [[ 0.7228, 0.6879, 0.6531, ..., -1.6476, -1.6302, -1.6476], [ 0.8099, 0.7576, 0.7228, ..., -1.6476, -1.6476, -1.6650], [ 1.0017, 0.9145, 0.8797, ..., -1.6476, -1.6650, -1.6999], ..., [ 1.6291, 1.6291, 1.6465, ..., 1.6291, 1.8208, 2.1346], [ 2.1868, 2.0300, 1.6814, ..., 1.7685, 1.9428, 2.0125], [ 1.9254, 2.0997, 2.0823, ..., 1.9428, 2.2043, 1.9080]]]]) 预测 现在将使用预训练的 DenseNet 121 模型来预测图像类别。 我们将使用torchvision库中的一个，加载模型并进行推断。 在此示例中，我们将使用预训练模型，但您可以对自己的模型使用相同的方法。 在此教程中查看有关加载模型的更多信息。 from torchvision import models # Make sure to pass `pretrained` as `True` to use the pretrained weights: model = models.densenet121(pretrained=True) # Since we are using our model only for inference, switch to `eval` mode: model.eval() def get_prediction(image_bytes): tensor = transform_image(image_bytes=image_bytes) outputs = model.forward(tensor) _, y_hat = outputs.max(1) return y_hat 张量y_hat将包含预测的类 ID 的索引。 但是，我们需要一个人类可读的类名。 为此，我们需要一个类 ID 来进行名称映射。 将这个文件下载为imagenet_class_index.json，并记住它的保存位置（或者，如果您按照本教程中的确切步骤操作，请将其保存在tutorials/_static中）。 此文件包含 ImageNet 类 ID 到 ImageNet 类名称的映射。 我们将加载此 JSON 文件并获取预测索引的类名称。 import json imagenet_class_index = json.load(open('../_static/imagenet_class_index.json')) def get_prediction(image_bytes): tensor = transform_image(image_bytes=image_bytes) outputs = model.forward(tensor) _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) return imagenet_class_index[predicted_idx] 在使用imagenet_class_index字典之前，首先我们将张量值转换为字符串值，因为imagenet_class_index字典中的键是字符串。 我们将测试上述方法： with open(\"../_static/img/sample_file.jpeg\", 'rb') as f: image_bytes = f.read() print(get_prediction(image_bytes=image_bytes)) 出： ['n02124075', 'Egyptian_cat'] 您应该得到如下响应： ['n02124075', 'Egyptian_cat'] 数组中的第一项是 ImageNet 类 ID，第二项是人类可读的名称。 注意 您是否注意到model变量不属于get_prediction方法？ 还是为什么模型是全局变量？ 就内存和计算而言，加载模型可能是一项昂贵的操作。 如果我们以get_prediction方法加载模型，则每次调用该方法时都会不必要地加载该模型。 由于我们正在构建一个 Web 服务器，因此每秒可能有成千上万的请求，因此我们不应该浪费时间为每个推断重复加载模型。 因此，我们仅将模型加载到内存中一次。 在生产系统中，必须高效使用计算以能够大规模处理请求，因此通常应在处理请求之前加载模型。 将模型集成到我们的 API 服务器中 在最后一部分中，我们将模型添加到 Flask API 服务器中。 由于我们的 API 服务器应该获取图像文件，因此我们将更新predict方法以从请求中读取文件： from flask import request @app.route('/predict', methods=['POST']) def predict(): if request.method == 'POST': # we will get the file from the request file = request.files['file'] # convert that to bytes img_bytes = file.read() class_id, class_name = get_prediction(image_bytes=img_bytes) return jsonify({'class_id': class_id, 'class_name': class_name}) app.py文件现在完成。 以下是完整版本； 将路径替换为保存文件的路径，它应运行： import io import json from torchvision import models import torchvision.transforms as transforms from PIL import Image from flask import Flask, jsonify, request app = Flask(__name__) imagenet_class_index = json.load(open('/imagenet_class_index.json')) model = models.densenet121(pretrained=True) model.eval() def transform_image(image_bytes): my_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) image = Image.open(io.BytesIO(image_bytes)) return my_transforms(image).unsqueeze(0) def get_prediction(image_bytes): tensor = transform_image(image_bytes=image_bytes) outputs = model.forward(tensor) _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) return imagenet_class_index[predicted_idx] @app.route('/predict', methods=['POST']) def predict(): if request.method == 'POST': file = request.files['file'] img_bytes = file.read() class_id, class_name = get_prediction(image_bytes=img_bytes) return jsonify({'class_id': class_id, 'class_name': class_name}) if __name__ == '__main__': app.run() 让我们测试一下我们的网络服务器！ 跑： $ FLASK_ENV=development FLASK_APP=app.py flask run 我们可以使用requests库向我们的应用发送 POST 请求： import requests resp = requests.post(\"http://localhost:5000/predict\", files={\"file\": open('/cat.jpg','rb')}) 现在打印resp.json()将显示以下内容： {\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"} 后续步骤 我们编写的服务器非常琐碎，可能无法完成生产应用所需的一切。 因此，您可以采取一些措施来改善它： 端点/predict假定请求中始终会有一个图像文件。 这可能并不适用于所有请求。 我们的用户可能发送带有其他参数的图像，或者根本不发送任何图像。 用户也可以发送非图像类型的文件。 由于我们没有处理错误，因此这将破坏我们的服务器。 添加显式的错误处理路径将引发异常，这将使我们能够更好地处理错误的输入 即使模型可以识别大量类别的图像，也可能无法识别所有图像。 增强实现以处理模型无法识别图像中的任何情况的情况。 我们在开发模式下运行 Flask 服务器，该服务器不适合在生产中进行部署。 您可以查看本教程，以便在生产环境中部署 Flask 服务器。 您还可以通过创建一个带有表单的页面来添加 UI，该表单可以拍摄图像并显示预测。 查看类似项目的演示及其源代码。 在本教程中，我们仅展示了如何构建可以一次返回单个图像预测的服务。 我们可以修改服务以能够一次返回多个图像的预测。 此外，service-streamer 库自动将对服务的请求排队，并将请求采样到微型批量中，这些微型批量可输入模型中。 您可以查看本教程。 最后，我们鼓励您在页面顶部查看链接到的其他 PyTorch 模型部署教程。 脚本的总运行时间：（0 分钟 1.232 秒） 下载 Python 源码：flask_rest_api_tutorial.py 下载 Jupyter 笔记本：flask_rest_api_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"38.html":{"url":"38.html","title":"TorchScript 简介","keywords":"","body":"TorchScript 简介 原文：https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html James Reed (jamesreed@fb.com)，Michael Suo (suo@fb.com)，修订 2 本教程是 TorchScript 的简介，TorchScript 是 PyTorch 模型（nn.Module的子类）的中间表示，可以在高性能环境（例如 C++）中运行。 在本教程中，我们将介绍： PyTorch 中模型创作的基础，包括： 模组 定义forward函数 将模块组成模块的层次结构 将 PyTorch 模块转换为 TorchScript（我们的高性能部署运行时）的特定方法 跟踪现有模块 使用脚本直接编译模块 如何组合两种方法 保存和加载 TorchScript 模块 我们希望在完成本教程之后，您将继续学习后续教程，该教程将引导您完成一个从 C++ 实际调用 TorchScript 模型的示例。 import torch # This is all you need to use both PyTorch and TorchScript! print(torch.__version__) 出： 1.7.1 PyTorch 模型创建基础 首先定义一个简单的Module。 Module是 PyTorch 中组成的基本单位。 它包含： 为调用准备模块的构造器 一组Parameters和子Modules。 这些由构造器初始化，并且可以在调用期间由模块使用。 forward函数。 这是调用模块时运行的代码。 我们来看一个小例子： class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() def forward(self, x, h): new_h = torch.tanh(x + h) return new_h, new_h my_cell = MyCell() x = torch.rand(3, 4) h = torch.rand(3, 4) print(my_cell(x, h)) 出： (tensor([[0.8837, 0.5372, 0.4951, 0.9124], [0.6124, 0.7072, 0.6395, 0.9585], [0.6178, 0.8701, 0.8071, 0.2415]]), tensor([[0.8837, 0.5372, 0.4951, 0.9124], [0.6124, 0.7072, 0.6395, 0.9585], [0.6178, 0.8701, 0.8071, 0.2415]])) 因此，我们已经： 创建了一个子类torch.nn.Module的类。 定义一个构造器。 构造器没有做很多事情，只是调用super的构造器。 定义了forward函数，该函数具有两个输入并返回两个输出。 forward函数的实际内容并不是很重要，但它是一种伪造的 RNN 单元，即，该函数应用于循环。 我们实例化了该模块，并制作了x和y，它们只是3x4随机值矩阵。 然后，我们使用my_cell(x, h)调用该单元格。 这依次调用我们的forward函数。 让我们做一些更有趣的事情： class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.linear(x) + h) return new_h, new_h my_cell = MyCell() print(my_cell) print(my_cell(x, h)) 出： MyCell( (linear): Linear(in_features=4, out_features=4, bias=True) ) (tensor([[ 0.5042, 0.8137, -0.1593, 0.4167], [ 0.1716, 0.8078, -0.2267, 0.7011], [ 0.5616, 0.8753, 0.1597, -0.3899]], grad_fn=), tensor([[ 0.5042, 0.8137, -0.1593, 0.4167], [ 0.1716, 0.8078, -0.2267, 0.7011], [ 0.5616, 0.8753, 0.1597, -0.3899]], grad_fn=)) 我们已经重新定义了模块MyCell，但是这次我们添加了self.linear属性，并且在forward函数中调用了self.linear。 这里到底发生了什么？ torch.nn.Linear是 PyTorch 标准库中的Module。 就像MyCell一样，可以使用调用语法来调用它。 我们正在建立Module的层次结构。 Module上的print将直观地表示Module的子类层次结构。 在我们的示例中，我们可以看到Linear子类及其参数。 通过以这种方式组成Module，我们可以简洁易读地编写具有可重用组件的模型。 您可能已经在输出上注意到grad_fn。 这是 PyTorch 自动微分方法的详细信息，称为 autograd 。 简而言之，该系统允许我们通过潜在的复杂程序来计算导数。 该设计为模型创作提供了极大的灵活性。 现在，让我们检查一下灵活性： class MyDecisionGate(torch.nn.Module): def forward(self, x): if x.sum() > 0: return x else: return -x class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() self.dg = MyDecisionGate() self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.dg(self.linear(x)) + h) return new_h, new_h my_cell = MyCell() print(my_cell) print(my_cell(x, h)) 出： MyCell( (dg): MyDecisionGate() (linear): Linear(in_features=4, out_features=4, bias=True) ) (tensor([[0.8636, 0.5572, 0.6262, 0.8546], [0.7766, 0.5056, 0.5357, 0.8360], [0.7293, 0.7581, 0.7117, 0.2432]], grad_fn=), tensor([[0.8636, 0.5572, 0.6262, 0.8546], [0.7766, 0.5056, 0.5357, 0.8360], [0.7293, 0.7581, 0.7117, 0.2432]], grad_fn=)) 我们再次重新定义了MyCell类，但是在这里我们定义了MyDecisionGate。 该模块利用控制流。 控制流包括循环和if语句之类的内容。 给定完整的程序表示形式，许多框架都采用计算符号派生的方法。 但是，在 PyTorch 中，我们使用梯度色带。 我们记录发生的操作，并在计算派生时向后回放。 这样，框架不必为语言中的所有构造显式定义派生类。 Autograd 的工作原理 TorchScript 的基础 现在，让我们以正在运行的示例为例，看看如何应用 TorchScript。 简而言之，即使 PyTorch 具有灵活和动态的特性，TorchScript 也提供了捕获模型定义的工具。 让我们开始研究所谓的跟踪。 跟踪Modules class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.linear(x) + h) return new_h, new_h my_cell = MyCell() x, h = torch.rand(3, 4), torch.rand(3, 4) traced_cell = torch.jit.trace(my_cell, (x, h)) print(traced_cell) traced_cell(x, h) 出： MyCell( original_name=MyCell (linear): Linear(original_name=Linear) ) 我们倒退了一点，并学习了MyCell类的第二版。 和以前一样，我们实例化了它，但是这一次，我们调用了torch.jit.trace，将其传递给Module，并传递给了示例输入，网络可能会看到。 这到底是做什么的？ 它调用了Module，记录了运行Module时发生的操作，并创建了torch.jit.ScriptModule的实例（其中TracedModule是实例） TorchScript 将其定义记录在中间表示（或 IR）中，在深度学习中通常称为图。 我们可以检查带有.graph属性的图： print(traced_cell.graph) 出： graph(%self.1 : __torch__.MyCell, %input : Float(3:4, 4:1, requires_grad=0, device=cpu), %h : Float(3:4, 4:1, requires_grad=0, device=cpu)): %19 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1) %21 : Tensor = prim::CallMethod[name=\"forward\"](%19, %input) %12 : int = prim::Constant[value=1]() # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0 %13 : Float(3:4, 4:1, requires_grad=1, device=cpu) = aten::add(%21, %h, %12) # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0 %14 : Float(3:4, 4:1, requires_grad=1, device=cpu) = aten::tanh(%13) # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0 %15 : (Float(3:4, 4:1, requires_grad=1, device=cpu), Float(3:4, 4:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%14, %14) return (%15) 但是，这是一个非常低级的表示形式，图中包含的大多数信息对最终用户没有用。 相反，我们可以使用.code属性来给出代码的 Python 语法解释： print(traced_cell.code) 出： def forward(self, input: Tensor, h: Tensor) -> Tuple[Tensor, Tensor]: _0 = torch.add((self.linear).forward(input, ), h, alpha=1) _1 = torch.tanh(_0) return (_1, _1) 那么为什么我们要进行所有这些操作？ 有以下几个原因： TorchScript 代码可以在其自己的解释器中调用，该解释器基本上是受限制的 Python 解释器。 该解释器不获取全局解释器锁定，因此可以在同一实例上同时处理许多请求。 这种格式允许我们将整个模型保存到磁盘上，然后将其加载到另一个环境中，例如在以 Python 以外的语言编写的服务器中 TorchScript 为我们提供了一种表示形式，其中我们可以对代码进行编译器优化以提供更有效的执行 TorchScript 允许我们与许多后端/设备运行时进行交互，与单个运算符相比，它们要求更广泛的程序视图。 我们可以看到，调用traced_cell会产生与 Python 模块相同的结果： print(my_cell(x, h)) print(traced_cell(x, h)) 出： (tensor([[-0.3869, 0.0678, 0.5692, 0.6332], [ 0.1230, 0.4653, 0.8051, 0.3346], [-0.5288, 0.2767, 0.9063, 0.4727]], grad_fn=), tensor([[-0.3869, 0.0678, 0.5692, 0.6332], [ 0.1230, 0.4653, 0.8051, 0.3346], [-0.5288, 0.2767, 0.9063, 0.4727]], grad_fn=)) (tensor([[-0.3869, 0.0678, 0.5692, 0.6332], [ 0.1230, 0.4653, 0.8051, 0.3346], [-0.5288, 0.2767, 0.9063, 0.4727]], grad_fn=), tensor([[-0.3869, 0.0678, 0.5692, 0.6332], [ 0.1230, 0.4653, 0.8051, 0.3346], [-0.5288, 0.2767, 0.9063, 0.4727]], grad_fn=)) 使用脚本转换模块 有一个原因是我们使用了模块的第二版，而不是使用带有大量控制流的子模块。 现在让我们检查一下： class MyDecisionGate(torch.nn.Module): def forward(self, x): if x.sum() > 0: return x else: return -x class MyCell(torch.nn.Module): def __init__(self, dg): super(MyCell, self).__init__() self.dg = dg self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.dg(self.linear(x)) + h) return new_h, new_h my_cell = MyCell(MyDecisionGate()) traced_cell = torch.jit.trace(my_cell, (x, h)) print(traced_cell.code) 出： def forward(self, input: Tensor, h: Tensor) -> Tuple[Tensor, Tensor]: _0 = self.dg _1 = (self.linear).forward(input, ) _2 = (_0).forward(_1, ) _3 = torch.tanh(torch.add(_1, h, alpha=1)) return (_3, _3) 查看.code输出，可以发现找不到if-else分支！ 为什么？ 跟踪完全按照我们所说的去做：运行代码，记录发生的操作，并构造一个执行此操作的ScriptModule。 不幸的是，诸如控制流之类的东西被擦除了。 我们如何在 TorchScript 中忠实地表示此模块？ 我们提供了脚本编译器，它可以直接分析您的 Python 源代码以将其转换为 TorchScript。 让我们使用脚本编译器转换MyDecisionGate： scripted_gate = torch.jit.script(MyDecisionGate()) my_cell = MyCell(scripted_gate) traced_cell = torch.jit.script(my_cell) print(traced_cell.code) 出： def forward(self, x: Tensor, h: Tensor) -> Tuple[Tensor, Tensor]: _0 = (self.dg).forward((self.linear).forward(x, ), ) new_h = torch.tanh(torch.add(_0, h, alpha=1)) return (new_h, new_h) 万岁！ 现在，我们已经忠实地捕获了我们在 TorchScript 中程序的行为。 现在，让我们尝试运行该程序： # New inputs x, h = torch.rand(3, 4), torch.rand(3, 4) traced_cell(x, h) 混合脚本和跟踪 在某些情况下，需要使用跟踪而不是脚本（例如，一个模块具有许多基于不变的 Python 值做出的架构决策，而我们不希望它们出现在 TorchScript 中）。 在这种情况下，可以通过跟踪来编写脚本：torch.jit.script将内联被跟踪模块的代码，而跟踪将内联脚本模块的代码。 第一种情况的示例： class MyRNNLoop(torch.nn.Module): def __init__(self): super(MyRNNLoop, self).__init__() self.cell = torch.jit.trace(MyCell(scripted_gate), (x, h)) def forward(self, xs): h, y = torch.zeros(3, 4), torch.zeros(3, 4) for i in range(xs.size(0)): y, h = self.cell(xs[i], h) return y, h rnn_loop = torch.jit.script(MyRNNLoop()) print(rnn_loop.code) 出： def forward(self, xs: Tensor) -> Tuple[Tensor, Tensor]: h = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None) y = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None) y0 = y h0 = h for i in range(torch.size(xs, 0)): _0 = (self.cell).forward(torch.select(xs, 0, i), h0, ) y1, h1, = _0 y0, h0 = y1, h1 return (y0, h0) 还有第二种情况的示例： class WrapRNN(torch.nn.Module): def __init__(self): super(WrapRNN, self).__init__() self.loop = torch.jit.script(MyRNNLoop()) def forward(self, xs): y, h = self.loop(xs) return torch.relu(y) traced = torch.jit.trace(WrapRNN(), (torch.rand(10, 3, 4))) print(traced.code) 出： def forward(self, argument_1: Tensor) -> Tensor: _0, h, = (self.loop).forward(argument_1, ) return torch.relu(h) 这样，当情况需要它们时，可以使用脚本和跟踪并将它们一起使用。 保存和加载模型 我们提供 API，以存档格式将 TorchScript 模块保存到磁盘或从磁盘加载 TorchScript 模块。 这种格式包括代码，参数，属性和调试信息，这意味着归档文件是模型的独立表示形式，可以在完全独立的过程中加载。 让我们保存并加载包装好的 RNN 模块： traced.save('wrapped_rnn.zip') loaded = torch.jit.load('wrapped_rnn.zip') print(loaded) print(loaded.code) 出： RecursiveScriptModule( original_name=WrapRNN (loop): RecursiveScriptModule( original_name=MyRNNLoop (cell): RecursiveScriptModule( original_name=MyCell (dg): RecursiveScriptModule(original_name=MyDecisionGate) (linear): RecursiveScriptModule(original_name=Linear) ) ) ) def forward(self, argument_1: Tensor) -> Tensor: _0, h, = (self.loop).forward(argument_1, ) return torch.relu(h) 如您所见，序列化保留了模块层次结构和我们一直在研究的代码。 也可以将模型加载到 C++ 中，以实现不依赖 Python 的执行。 进一步阅读 我们已经完成了教程！ 有关更多涉及的演示，请查看 NeurIPS 演示来使用 TorchScript 转换机器翻译模型。 脚本的总运行时间：（0 分钟 0.269 秒） 下载 Python 源码：Intro_to_TorchScript_tutorial.py 下载 Jupyter 笔记本：Intro_to_TorchScript_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"39.html":{"url":"39.html","title":"在 C++ 中加载 TorchScript 模型","keywords":"","body":"在 C++ 中加载 TorchScript 模型 原文：https://pytorch.org/tutorials/advanced/cpp_export.html 顾名思义，PyTorch 的主要接口是 Python 编程语言。 尽管 Python 是许多需要动态性和易于迭代的场景的合适且首选的语言，但是在同样许多情况下，Python 的这些属性恰恰是不利的。 后者经常应用的一种环境是生产 –低延迟和严格部署要求的土地。 对于生产场景，即使仅将 C++ 绑定到 Java，Rust 或 Go 之类的另一种语言中，它也是经常选择的语言。 以下各段将概述 PyTorch 提供的从现有 Python 模型到序列化表示形式的路径，该序列化表示形式可以完全由 C++ 加载和执行，不依赖于 Python。 第 1 步：将 PyTorch 模型转换为 Torch 脚本 PyTorch 模型从 Python 到 C++ 的旅程由 Torch 脚本启用，它是 PyTorch 模型的一种表示形式，可以由 Torch 脚本编译器理解，编译和序列化。 如果您从使用原始“渴望” API 编写的现有 PyTorch 模型开始，则必须首先将模型转换为 Torch 脚本。 在最常见的情况下（如下所述），只需很少的努力。 如果您已经有了 Torch 脚本模块，则可以跳到本教程的下一部分。 有两种将 PyTorch 模型转换为 Torch 脚本的方法。 第一种称为跟踪，该机制通过使用示例输入对模型的结构进行一次评估，并记录这些输入在模型中的流量来捕获模型的结构。 这适用于有限使用控制流的模型。 第二种方法是在模型中添加显式注解，以告知 TorchScript 编译器可以根据 Torch Script 语言施加的约束直接解析和编译模型代码。 小费 您可以在官方 Torch 脚本参考中找到这两种方法的完整文档以及使用方法的进一步指导。 通过跟踪转换为 Torch 脚本 要将 PyTorch 模型通过跟踪转换为 Torch 脚本，必须将模型的实例以及示例输入传递给torch.jit.trace函数。 这将产生一个torch.jit.ScriptModule对象，并将模型评估的轨迹嵌入到模块的forward方法中： import torch import torchvision # An instance of your model. model = torchvision.models.resnet18() # An example input you would normally provide to your model's forward() method. example = torch.rand(1, 3, 224, 224) # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing. traced_script_module = torch.jit.trace(model, example) 现在可以对跟踪的ScriptModule进行评估，使其与常规 PyTorch 模块相同： In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224)) In[2]: output[0, :5] Out[2]: tensor([-0.2698, -0.0381, 0.4023, -0.3010, -0.0448], grad_fn=) 通过注解转换为 Torch 脚本 在某些情况下，例如，如果模型采用特定形式的控制流，则可能需要直接在 Torch 脚本中编写模型并相应地注解模型。 例如，假设您具有以下原始 Pytorch 模型： import torch class MyModule(torch.nn.Module): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) def forward(self, input): if input.sum() > 0: output = self.weight.mv(input) else: output = self.weight + input return output 因为此模块的forward方法使用取决于输入的控制流，所以它不适合跟踪。 相反，我们可以将其转换为ScriptModule。 为了将模块转换为ScriptModule，需要使用torch.jit.script编译模块，如下所示： class MyModule(torch.nn.Module): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) def forward(self, input): if input.sum() > 0: output = self.weight.mv(input) else: output = self.weight + input return output my_module = MyModule(10,20) sm = torch.jit.script(my_module) 如果您需要在nn.Module中排除某些方法，因为它们使用的是 TorchScript 不支持的 Python 函数，则可以使用@torch.jit.ignore来注解这些方法 my_module是已准备好进行序列化的ScriptModule的实例。 第 2 步：将脚本模块序列化为文件 跟踪或注解 PyTorch 模型后，一旦有了ScriptModule，就可以将其序列化为文件了。 稍后，您将能够使用 C++ 从此文件加载模块并执行它，而无需依赖 Python。 假设我们要序列化先前在跟踪示例中显示的ResNet18模型。 要执行此序列化，只需在模块上调用save并为其传递文件名： traced_script_module.save(\"traced_resnet_model.pt\") 这将在您的工作目录中生成一个traced_resnet_model.pt文件。 如果您还想序列化my_module，请致电my_module.save(\"my_module_model.pt\")。我们现在已经正式离开 Python 领域，并准备跨入 C++ 领域。 第 3 步：在 C++ 中加载脚本模块 要在 C++ 中加载序列化的 PyTorch 模型，您的应用必须依赖于 PyTorch C++ API –也称为 LibTorch 。 LibTorch 发行版包含共享库，头文件和 CMake 构建配置文件的集合。 虽然 CMake 不是依赖 LibTorch 的要求，但它是推荐的方法，将来会得到很好的支持。 对于本教程，我们将使用 CMake 和 LibTorch 构建一个最小的 C++ 应用，该应用简单地加载并执行序列化的 PyTorch 模型。 最小的 C++ 应用 让我们从讨论加载模块的代码开始。 以下将已经做： #include // One-stop header. #include #include int main(int argc, const char* argv[]) { if (argc != 2) { std::cerr \\n\"; return -1; } torch::jit::script::Module module; try { // Deserialize the ScriptModule from a file using torch::jit::load(). module = torch::jit::load(argv[1]); } catch (const c10::Error& e) { std::cerr 标头包含了运行示例所需的 LibTorch 库中的所有相关包含。 我们的应用接受序列化的 PyTorch ScriptModule的文件路径作为其唯一的命令行参数，然后继续使用torch::jit::load()函数对该模块进行反序列化，该函数将该文件路径作为输入。 作为回报，我们收到一个torch::jit::script::Module对象。 我们将稍后讨论如何执行它。 依赖 LibTorch 并构建应用 假设我们将以上代码存储到名为example-app.cpp的文件中。 最小的CMakeLists.txt构建起来看起来很简单： cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(custom_ops) find_package(Torch REQUIRED) add_executable(example-app example-app.cpp) target_link_libraries(example-app \"${TORCH_LIBRARIES}\") set_property(TARGET example-app PROPERTY CXX_STANDARD 14) 建立示例应用的最后一件事是 LibTorch 发行版。 您可以随时从 PyTorch 网站上的下载页面获取最新的稳定版本。 如果下载并解压缩最新的归档文件，则应该收到具有以下目录结构的文件夹： libtorch/ bin/ include/ lib/ share/ lib/文件夹包含您必须链接的共享库， include/文件夹包含程序需要包含的头文件， share/文件夹包含必要的 CMake 配置，以启用上面的简单find_package(Torch)命令。 小费 在 Windows 上，调试和发行版本不兼容 ABI。 如果计划以调试模式构建项目，请尝试使用 LibTorch 的调试版本。 另外，请确保在下面的cmake --build .行中指定正确的配置。 最后一步是构建应用。 为此，假定示例目录的布局如下： example-app/ CMakeLists.txt example-app.cpp 现在，我们可以运行以下命令从example-app/文件夹中构建应用： mkdir build cd build cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. cmake --build . --config Release 其中/path/to/libtorch应该是解压缩的 LibTorch 发行版的完整路径。 如果一切顺利，它将看起来像这样： root@4b5a67132e81:/example-app# mkdir build root@4b5a67132e81:/example-app# cd build root@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Configuring done -- Generating done -- Build files have been written to: /example-app/build root@4b5a67132e81:/example-app/build# make Scanning dependencies of target example-app [ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o [100%] Linking CXX executable example-app [100%] Built target example-app 如果我们提供到先前创建的跟踪ResNet18模型traced_resnet_model.pt到生成的example-app二进制文件的路径，则应该以友好的“确定”来回报。 请注意，如果尝试使用my_module_model.pt运行此示例，则会收到一条错误消息，提示您输入的形状不兼容。 my_module_model.pt期望使用 1D 而不是 4D。 root@4b5a67132e81:/example-app/build# ./example-app /traced_resnet_model.pt ok 步骤 4：在 C++ 中执行脚本模块 在用 C++ 成功加载序列化的ResNet18之后，我们现在离执行它仅几行代码了！ 让我们将这些行添加到 C++ 应用的main()函数中： // Create a vector of inputs. std::vector inputs; inputs.push_back(torch::ones({1, 3, 224, 224})); // Execute the model and turn its output into a tensor. at::Tensor output = module.forward(inputs).toTensor(); std::cout 前两行设置了模型的输入。 我们创建一个torch::jit::IValue的向量（类型擦除的值类型script::Module方法接受并返回），并添加单个输入。 要创建输入张量，我们使用torch::ones()，等效于 C++ API 中的torch.ones。 然后，我们运行script::Module的forward方法，并将其传递给我们创建的输入向量。 作为回报，我们得到了一个新的IValue，我们可以通过调用toTensor()将其转换为张量。 小费 要总体上了解有关torch::ones和 PyTorch C++ API 之类的功能的更多信息，请参阅这个页面上的文档。 PyTorch C++ API 提供了与 Python API 几乎相同的功能，使您可以像在 Python 中一样进一步操纵和处理张量。 在最后一行，我们打印输出的前五个条目。 由于在本教程前面的部分中，我们为 Python 中的模型提供了相同的输入，因此理想情况下，我们应该看到相同的输出。 让我们通过重新编译我们的应用并以相同的序列化模型运行它来进行尝试： root@4b5a67132e81:/example-app/build# make Scanning dependencies of target example-app [ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o [100%] Linking CXX executable example-app [100%] Built target example-app root@4b5a67132e81:/example-app/build# ./example-app traced_resnet_model.pt -0.2698 -0.0381 0.4023 -0.3010 -0.0448 [ Variable[CPUFloatType]{1,5} ] 作为参考，Python 以前的输出为： tensor([-0.2698, -0.0381, 0.4023, -0.3010, -0.0448], grad_fn=) 看起来很不错！ 小费 要将模型移至 GPU 内存，可以编写model.to(at::kCUDA);。 通过调用tensor.to(at::kCUDA)来确保模型的输入也位于 CUDA 内存中，这将在 CUDA 内存中返回新的张量。 第 5 步：获得帮助并探索 API 本教程有望使您对 PyTorch 模型从 Python 到 C++ 的路径有一个大致的了解。 利用本教程中介绍的概念，您应该能够从原始的“急切的” PyTorch 模型，到 Python 中的已编译ScriptModule，再到磁盘上的序列化文件，以及–结束循环–到可执行文件script::Module在 C++ 中。 当然，有许多我们没有介绍的概念。 例如，您可能会发现自己想要扩展使用 C++ 或 CUDA 实现的自定义运算符来扩展ScriptModule，并希望在纯 C++ 生产环境中加载的ScriptModule内执行该自定义运算符。 好消息是：这是可能的，并且得到了很好的支持！ 现在，您可以浏览这个文件夹作为示例，我们将很快提供一个教程。 目前，以下链接通常可能会有所帮助： Torch 脚本参考 PyTorch C++ API 文档 PyTorch Python API 文档 与往常一样，如果您遇到任何问题或疑问，可以使用我们的论坛或 GitHub ISSUE 进行联系。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"40.html":{"url":"40.html","title":"将模型从 PyTorch 导出到 ONNX 并使用 ONNX 运行时运行它（可选）","keywords":"","body":"将模型从 PyTorch 导出到 ONNX 并使用 ONNX 运行时运行它（可选） 原文：https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html 在本教程中，我们描述了如何将 PyTorch 中定义的模型转换为 ONNX 格式，然后在 ONNX 运行时中运行它。 ONNX 运行时是针对 ONNX 模型的以性能为中心的引擎，可在多个平台和硬件（Windows，Linux 和 Mac 以及 CPU 和 GPU 上）高效地进行推理。 事实证明，如此处所述，ONNX 运行时大大提高了多个模型的性能。 对于本教程，您将需要安装 ONNX 和 ONNX 运行时。 您可以使用pip install onnx onnxruntime获得 ONNX 和 ONNX 运行时的二进制版本。 请注意，ONNX 运行时与 Python 3.5 至 3.7 版本兼容。 NOTE：本教程需要 PyTorch master分支，可以按照此处的说明进行安装 # Some standard imports import io import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx 超分辨率是一种提高图像，视频分辨率的方法，广泛用于图像处理或视频编辑中。 在本教程中，我们将使用一个小的超分辨率模型。 首先，让我们在 PyTorch 中创建一个SuperResolution模型。 该模型使用了《使用高效的子像素卷积神经网络的实时单幅图像和视频超分辨率》（Shi 等人）中所述的高效子像素卷积层来提高图像的分辨率受向上缩放因子的影响。 该模型期望图像的 YCbCr 的 Y 分量作为输入，并以超分辨率输出放大的 Y 分量。 模型直接来自 PyTorch 的示例，未经修改： # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv2.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv3.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) 通常，您现在将训练此模型。 但是，在本教程中，我们将下载一些预训练的权重。 请注意，此模型未经过充分训练以提供良好的准确率，此处仅用于演示目的。 在导出模型之前，请先调用torch_model.eval()或torch_model.train(False)，以将模型转换为推理模式，这一点很重要。 这是必需的，因为像dropout或batchnorm这样的运算符在推断和训练模式下的行为会有所不同。 # Load pretrained model weights model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth' batch_size = 1 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() 在 PyTorch 中导出模型是通过跟踪或脚本编写的。 本教程将以通过跟踪导出的模型为例。 要导出模型，我们调用torch.onnx.export()函数。 这将执行模型，并记录使用什么运算符计算输出的轨迹。 因为export运行模型，所以我们需要提供输入张量x。 只要是正确的类型和大小，其中的值就可以是随机的。 请注意，除非指定为动态轴，否则输入尺寸将在导出的 ONNX 图中固定为所有输入尺寸。 在此示例中，我们使用输入batch_size 1导出模型，但随后在torch.onnx.export()的dynamic_axes参数中将第一维指定为动态。 因此，导出的模型将接受大小为[batch_size, 1, 224, 224]的输入，其中batch_size可以是可变的。 要了解有关 PyTorch 导出接口的更多详细信息，请查看torch.onnx文档。 # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = ['input'], # the model's input names output_names = ['output'], # the model's output names dynamic_axes={'input' : {0 : 'batch_size'}, # variable lenght axes 'output' : {0 : 'batch_size'}}) 我们还计算了torch_out（模型之后的输出），我们将用来验证导出的模型在 ONNX 运行时中运行时是否计算出相同的值。 但是，在通过 ONNX 运行时验证模型的输出之前，我们将使用 ONNX 的 API 检查 ONNX 模型。 首先，onnx.load(\"super_resolution.onnx\")将加载保存的模型并输出onnx.ModelProto结构（用于捆绑 ML 模型的顶级文件/容器格式。有关更多信息，请参见onnx.proto文档。 然后，onnx.checker.check_model(onnx_model)将验证模型的结构并确认模型具有有效的架构。 通过检查模型的版本，图的结构以及节点及其输入和输出，可以验证 ONNX 图的有效性。 import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) 现在，我们使用 ONNX 运行时的 Python API 计算输出。 这部分通常可以在单独的过程中或在另一台机器上完成，但是我们将继续同一过程，以便我们可以验证 ONNX 运行时和 PyTorch 正在为网络计算相同的值。 为了使用 ONNX 运行时运行模型，我们需要使用所选的配置参数为模型创建一个推理会话（此处使用默认配置）。 创建会话后，我们将使用run() API 评估模型。 此调用的输出是一个列表，其中包含由 ONNX 运行时计算的模型的输出。 import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\") def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") 我们应该看到 PyTorch 和 ONNX 运行时的输出在数值上与给定的精度匹配（rtol = 1e-03和atol = 1e-05）。 附带说明一下，如果它们不匹配，则说明 ONNX 导出器中存在问题，因此请与我们联系。 使用 ONNX 运行时在图像上运行模型 到目前为止，我们已经从 PyTorch 导出了一个模型，并演示了如何使用虚拟张量作为输入在 ONNX 运行时中加载和运行该模型。 在本教程中，我们将使用广泛使用的著名猫图像，如下图所示 首先，让我们加载图片，然后使用标准的 PIL python 库对其进行预处理。 请注意，此预处理是处理数据以训练/测试神经网络的标准做法。 我们首先调整图像大小以适合模型输入的大小（224x224）。 然后，我们将图像分为 Y，Cb 和 Cr 分量。 这些分量代表灰度图像（Y），以及蓝差（Cb）和红差（Cr）色度分量。 Y 分量对人眼更敏感，我们对将要转换的这个分量很感兴趣。 提取 Y 分量后，我们将其转换为张量，这将是模型的输入。 from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert('YCbCr') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) 现在，作为下一步，让我们使用代表灰度尺寸调整后的猫图像的张量，并按照先前的说明在 ONNX 运行时中运行超分辨率模型。 ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] 此时，模型的输出为张量。 现在，我们将处理模型的输出，以根据输出张量构造最终的输出图像，并保存图像。 采用了来自此处的超分辨率模型的 PyTorch 实现的后处理步骤。 img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") ONNX 运行时是跨平台引擎，您可以在多个平台上以及在 CPU 和 GPU 上运行它。 还可以使用 Azure 机器学习服务将 ONNX 运行时部署到云中以进行模型推断。 更多信息在此处。 在这里了解有关 ONNX 运行时性能的更多信息。 有关 ONNX 运行时的更多信息，请点击这里。 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：super_resolution_with_onnxruntime.py 下载 Jupyter 笔记本：super_resolution_with_onnxruntime.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"41.html":{"url":"41.html","title":"前端 API","keywords":"","body":"前端 API 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"42.html":{"url":"42.html","title":"PyTorch 中的命名张量简介（原型）","keywords":"","body":"PyTorch 中的命名张量简介（原型） 原文：https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html 作者： Richard Zou 命名张量旨在通过允许用户将显式名称与张量维度相关联来使张量更易于使用。 在大多数情况下，采用尺寸参数的操作将接受尺寸名称，而无需按位置跟踪尺寸。 此外，命名张量使用名称来自动检查运行时是否正确使用了 API，从而提供了额外的安全性。 名称也可以用于重新排列尺寸，例如，支持“按名称广播”而不是“按位置广播”。 本教程旨在作为 1.3 启动中将包含的功能的指南。 到最后，您将能够： 创建具有命名尺寸的张量，以及删除或重命名这些尺寸 了解操作如何传播维度名称的基础 了解命名尺寸如何在两个关键区域实现更清晰的代码： 广播操作 重塑和展开尺寸 最后，我们将通过使用命名张量编写一个多头注意力模块来将其付诸实践。 PyTorch 中的命名张量受 Sasha Rush 的启发并与之合作。 Sasha 在他的 2019 年 1 月博客文章中提出了最初的想法和概念证明。 基础知识：命名维度 PyTorch 现在允许张量具有命名维度； 工厂函数采用新的名称参数，该参数将名称与每个维度相关联。 这适用于大多数工厂函数，例如 tensor empty ones zeros randn rand 这里我们用名字构造一个张量： import torch imgs = torch.randn(1, 2, 2, 3, names=('N', 'C', 'H', 'W')) print(imgs.names) 出： ('N', 'C', 'H', 'W') 与命名张量的原始博客文章不同，命名维度是有序的：tensor.names[i]是tensor的第i个维度的名称。 重命名Tensor尺寸的方法有两种： # Method #1: set the .names attribute (this changes name in-place) imgs.names = ['batch', 'channel', 'width', 'height'] print(imgs.names) # Method #2: specify new names (this changes names out-of-place) imgs = imgs.rename(channel='C', width='W', height='H') print(imgs.names) 出： ('batch', 'channel', 'width', 'height') ('batch', 'C', 'W', 'H') 删除名称的首选方法是调用tensor.rename(None)： imgs = imgs.rename(None) print(imgs.names) 出： (None, None, None, None) 未命名的张量（没有命名尺寸的张量）仍然可以正常工作，并且在其repr中没有名称。 unnamed = torch.randn(2, 1, 3) print(unnamed) print(unnamed.names) 出： tensor([[[-0.7420, -0.3646, 0.1424]], [[-0.6065, -1.4888, 0.2935]]]) (None, None, None) 命名张量不需要命名所有尺寸。 imgs = torch.randn(3, 1, 1, 2, names=('N', None, None, None)) print(imgs.names) 出： ('N', None, None, None) 由于命名张量可以与未命名张量共存，因此我们需要一种不错的方式来编写可识别命名张量的代码，该代码可用于命名张量和未命名张量。 使用tensor.refine_names(*names)优化尺寸并将未命名的暗淡提升为已命名的暗淡。 细化维度定义为“重命名”，并具有以下限制： 可以将None暗号细化为任何名称 命名的维度只能精简为具有相同的名称。 imgs = torch.randn(3, 1, 1, 2) named_imgs = imgs.refine_names('N', 'C', 'H', 'W') print(named_imgs.names) # Refine the last two dims to 'H' and 'W'. In Python 2, use the string '...' # instead of ... named_imgs = imgs.refine_names(..., 'H', 'W') print(named_imgs.names) def catch_error(fn): try: fn() assert False except RuntimeError as err: err = str(err) if len(err) > 180: err = err[:180] + \"...\" print(err) named_imgs = imgs.refine_names('N', 'C', 'H', 'W') # Tried to refine an existing name to a different name catch_error(lambda: named_imgs.refine_names('N', 'C', 'H', 'width')) 出： ('N', 'C', 'H', 'W') (None, None, 'H', 'W') refine_names: cannot coerce Tensor['N', 'C', 'H', 'W'] to Tensor['N', 'C', 'H', 'width'] because 'W' is different from 'width' at index 3 大多数简单的操作都会传播名称。 命名张量的最终目标是所有操作以合理，直观的方式传播名称。 在 1.3 版本发布时，已添加了对许多常用操作的支持。 例如，这里是.abs()： print(named_imgs.abs().names) 出： ('N', 'C', 'H', 'W') 访问器和归约 可以使用尺寸名称来引用尺寸而不是位置尺寸。 这些操作还传播名称。 索引（基本索引和高级索引）尚未实现，但仍在规划中。 使用上面的named_imgs张量，我们可以执行以下操作： output = named_imgs.sum('C') # Perform a sum over the channel dimension print(output.names) img0 = named_imgs.select('N', 0) # get one image print(img0.names) 出： ('N', 'H', 'W') ('C', 'H', 'W') 名称推断 名称在称为名称推断的两步过程中在操作上传播： 检查名称：运算符可以在运行时执行自动检查，以检查某些尺寸名称是否匹配。 传播名称：名称推断将输出名称传播到输出张量。 让我们看一个非常小的例子，添加 2 个一维张量，不进行广播。 x = torch.randn(3, names=('X',)) y = torch.randn(3) z = torch.randn(3, names=('Z',)) 检查名称：首先，我们将检查这两个张量的名称是否相匹配。 当且仅当两个名称相等（字符串相等）或至少一个为None（None本质上是一个特殊的通配符名称）时，两个名称才匹配。 因此，这三者中唯一会出错的是x + z： catch_error(lambda: x + z) 出： Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match. 传播名称：通过返回两个名称中最精确的名称来统一这两个名称。 使用x + y时，X比None更精细。 print((x + y).names) 出： ('X',) 大多数名称推断规则都很简单明了，但是其中一些可能具有意想不到的语义。 让我们来看看您可能会遇到的一对：广播和矩阵乘法。 广播 命名张量不会改变广播行为； 他们仍然按位置广播。 但是，在检查两个尺寸是否可以广播时，PyTorch 还会检查这些尺寸的名称是否匹配。 这导致命名张量防止广播操作期间意外对齐。 在下面的示例中，我们将per_batch_scale应用于imgs。 imgs = torch.randn(2, 2, 2, 2, names=('N', 'C', 'H', 'W')) per_batch_scale = torch.rand(2, names=('N',)) catch_error(lambda: imgs * per_batch_scale) 出： Error when attempting to broadcast dims ['N', 'C', 'H', 'W'] and dims ['N']: dim 'W' and dim 'N' are at the same position from the right but do not match. 如果没有names，则per_batch_scale张量与imgs的最后一个尺寸对齐，这不是我们想要的。 我们确实想通过将per_batch_scale与imgs的批量尺寸对齐来执行操作。 有关如何按名称对齐张量的信息，请参见新的“按名称显式广播”功能，如下所述。 矩阵乘法 torch.mm(A, B)在A的第二个暗角和B的第一个暗角之间执行点积，返回具有A的第一个暗角和B的第二个暗角的张量。 （其他matmul函数，例如torch.matmul，torch.mv和torch.dot的行为类似）。 markov_states = torch.randn(128, 5, names=('batch', 'D')) transition_matrix = torch.randn(5, 5, names=('in', 'out')) # Apply one transition new_state = markov_states @ transition_matrix print(new_state.names) 出： ('batch', 'out') 如您所见，矩阵乘法不会检查收缩尺寸是否具有相同的名称。 接下来，我们将介绍命名张量启用的两个新行为：按名称的显式广播以及按名称的展平和展平尺寸 新行为：按名称显式广播 有关使用多个维度的主要抱怨之一是需要unsqueeze“虚拟”维度，以便可以进行操作。 例如，在之前的每批比例示例中，使用未命名的张量，我们将执行以下操作： imgs = torch.randn(2, 2, 2, 2) # N, C, H, W per_batch_scale = torch.rand(2) # N correct_result = imgs * per_batch_scale.view(2, 1, 1, 1) # N, C, H, W incorrect_result = imgs * per_batch_scale.expand_as(imgs) assert not torch.allclose(correct_result, incorrect_result) 通过使用名称，我们可以使这些操作更安全（并且易于与尺寸数量无关）。 我们提供了一个新的tensor.align_as(other)操作，可以对张量的尺寸进行排列以匹配other.names中指定的顺序，并在适当的地方添加一个尺寸的尺寸（tensor.align_to(*names)也可以）： imgs = imgs.refine_names('N', 'C', 'H', 'W') per_batch_scale = per_batch_scale.refine_names('N') named_result = imgs * per_batch_scale.align_as(imgs) # note: named tensors do not yet work with allclose assert torch.allclose(named_result.rename(None), correct_result) 新行为：按名称展平或取消展平维度 一种常见的操作是展平和展平尺寸。 现在，用户可以使用view，reshape或flatten来执行此操作； 用例包括将批量尺寸展平以将张量发送到必须采用一定数量尺寸的输入的运算符（即conv2d采用 4D 输入）。 为了使这些操作比查看或整形更具语义意义，我们引入了一种新的tensor.unflatten(dim, namedshape)方法并更新flatten以使用名称：tensor.flatten(dims, new_dim)。 flatten只能展平相邻的尺寸，但也可以用于不连续的维度。 必须将名称和形状传递到unflatten中，该形状是(dim, size)元组的列表，以指定如何展开维度。 可以在flatten期间保存unflatten的尺寸，但我们尚未这样做。 imgs = imgs.flatten(['C', 'H', 'W'], 'features') print(imgs.names) imgs = imgs.unflatten('features', (('C', 2), ('H', 2), ('W', 2))) print(imgs.names) 出： ('N', 'features') ('N', 'C', 'H', 'W') Autograd 支持 Autograd 当前会忽略所有张量上的名称，只是将它们视为常规张量。 梯度计算是正确的，但是我们失去了名称赋予我们的安全性。 在路线图上引入名称以自动微分的处理。 x = torch.randn(3, names=('D',)) weight = torch.randn(3, names=('D',), requires_grad=True) loss = (x - weight).abs() grad_loss = torch.randn(3) loss.backward(grad_loss) correct_grad = weight.grad.clone() print(correct_grad) # Unnamed for now. Will be named in the future weight.grad.zero_() grad_loss = grad_loss.refine_names('C') loss = (x - weight).abs() # Ideally we'd check that the names of loss and grad_loss match, but we don't # yet loss.backward(grad_loss) print(weight.grad) # still unnamed assert torch.allclose(weight.grad, correct_grad) 出： tensor([0.5398, 0.7907, 0.7784]) tensor([0.5398, 0.7907, 0.7784]) 其他受支持的（和不受支持的）功能 有关 1.3 发行版支持的功能的详细分类，请参见此处。 特别是，我们要指出当前不支持的三个重要函数： 通过torch.save或torch.load保存或加载命名张量 通过torch.multiprocessing进行多重处理 JIT 支持； 例如，以下将错误 imgs_named = torch.randn(1, 2, 2, 3, names=('N', 'C', 'H', 'W')) @torch.jit.script def fn(x): return x catch_error(lambda: fn(imgs_named)) 出： NYI: Named tensors are currently unsupported in TorchScript. As a workaround please drop names via `tensor = tensor.rename(None)`. 解决方法是，在使用尚不支持命名张量的任何东西之前，请通过tensor = tensor.rename(None)删除名称。 更长的例子：多头关注 现在，我们将通过一个完整的示例来实现一个常见的 PyTorch nn.Module：多头注意。 我们假设读者已经熟悉多头注意； 要进行复习，请查看此说明或此说明。 我们采用 ParlAI 来实现多头注意力的实现； 具体来说此处。 阅读该示例中的代码； 然后，与下面的代码进行比较，注意有四个标记为（I），（II），（III）和（IV）的位置，使用命名张量可以使代码更易读； 在代码块之后，我们将深入探讨其中的每一个。 import torch.nn as nn import torch.nn.functional as F import math class MultiHeadAttention(nn.Module): def __init__(self, n_heads, dim, dropout=0): super(MultiHeadAttention, self).__init__() self.n_heads = n_heads self.dim = dim self.attn_dropout = nn.Dropout(p=dropout) self.q_lin = nn.Linear(dim, dim) self.k_lin = nn.Linear(dim, dim) self.v_lin = nn.Linear(dim, dim) nn.init.xavier_normal_(self.q_lin.weight) nn.init.xavier_normal_(self.k_lin.weight) nn.init.xavier_normal_(self.v_lin.weight) self.out_lin = nn.Linear(dim, dim) nn.init.xavier_normal_(self.out_lin.weight) def forward(self, query, key=None, value=None, mask=None): # (I) query = query.refine_names(..., 'T', 'D') self_attn = key is None and value is None if self_attn: mask = mask.refine_names(..., 'T') else: mask = mask.refine_names(..., 'T', 'T_key') # enc attn dim = query.size('D') assert dim == self.dim, \\ f'Dimensions do not match: {dim} query vs {self.dim} configured' assert mask is not None, 'Mask is None, please specify a mask' n_heads = self.n_heads dim_per_head = dim // n_heads scale = math.sqrt(dim_per_head) # (II) def prepare_head(tensor): tensor = tensor.refine_names(..., 'T', 'D') return (tensor.unflatten('D', [('H', n_heads), ('D_head', dim_per_head)]) .align_to(..., 'H', 'T', 'D_head')) assert value is None if self_attn: key = value = query elif value is None: # key and value are the same, but query differs key = key.refine_names(..., 'T', 'D') value = key dim = key.size('D') # Distinguish between query_len (T) and key_len (T_key) dims. k = prepare_head(self.k_lin(key)).rename(T='T_key') v = prepare_head(self.v_lin(value)).rename(T='T_key') q = prepare_head(self.q_lin(query)) dot_prod = q.div_(scale).matmul(k.align_to(..., 'D_head', 'T_key')) dot_prod.refine_names(..., 'H', 'T', 'T_key') # just a check # (III) attn_mask = (mask == 0).align_as(dot_prod) dot_prod.masked_fill_(attn_mask, -float(1e20)) attn_weights = self.attn_dropout(F.softmax(dot_prod / scale, dim='T_key')) # (IV) attentioned = ( attn_weights.matmul(v).refine_names(..., 'H', 'T', 'D_head') .align_to(..., 'T', 'H', 'D_head') .flatten(['H', 'D_head'], 'D') ) return self.out_lin(attentioned).refine_names(..., 'T', 'D') （I）细化输入张量维度 def forward(self, query, key=None, value=None, mask=None): # (I) query = query.refine_names(..., 'T', 'D') query = query.refine_names(..., 'T', 'D')用作可执行的文档，并将输入尺寸提升为名称。 它检查最后两个维度是否可以调整为['T', 'D']，以防止在以后出现潜在的无声或混乱的尺寸不匹配错误。 （II）在prepare_head中操纵尺寸 # (II) def prepare_head(tensor): tensor = tensor.refine_names(..., 'T', 'D') return (tensor.unflatten('D', [('H', n_heads), ('D_head', dim_per_head)]) .align_to(..., 'H', 'T', 'D_head')) 首先要注意的是代码如何清楚地说明输入和输出尺寸：输入张量必须以T和D变暗结束，输出张量应以H，T和D_head维度结束。 要注意的第二件事是代码清楚地描述了正在发生的事情。 prepare_head获取键，查询和值，并将嵌入的维度拆分为多个头部，最后将维度顺序重新排列为[..., 'H', 'T', 'D_head']。 ParlAI 使用view和transpose操作实现以下prepare_head： def prepare_head(tensor): # input is [batch_size, seq_len, n_heads * dim_per_head] # output is [batch_size * n_heads, seq_len, dim_per_head] batch_size, seq_len, _ = tensor.size() tensor = tensor.view(batch_size, tensor.size(1), n_heads, dim_per_head) tensor = ( tensor.transpose(1, 2) .contiguous() .view(batch_size * n_heads, seq_len, dim_per_head) ) return tensor 我们命名的张量变量使用的操作虽然较为冗长，但比view和transpose具有更多的语义含义，并包含以名称形式出现的可执行文档。 （III）按名称显式广播 def ignore(): # (III) attn_mask = (mask == 0).align_as(dot_prod) dot_prod.masked_fill_(attn_mask, -float(1e20)) mask通常具有暗淡[N, T]（在自我关注的情况下）或[N, T, T_key]（对于编码器注意的情况），而dot_prod具有暗淡的[N, H, T, T_key]。 为了使mask与dot_prod正确广播，我们通常会在自注意的情况下将的调暗1和-1压下，在编码器的情况下，我们将unsqueeze调暗unsqueeze 。 使用命名张量，我们只需使用align_as将attn_mask与dot_prod对齐，而不必担心unsqueeze变暗的位置。 （IV）使用align_to和flatten进行更多尺寸操作 def ignore(): # (IV) attentioned = ( attn_weights.matmul(v).refine_names(..., 'H', 'T', 'D_head') .align_to(..., 'T', 'H', 'D_head') .flatten(['H', 'D_head'], 'D') ) 在这里，与（II）一样，align_to和flatten在语义上比view和transpose更有意义（尽管更冗长）。 运行示例 n, t, d, h = 7, 5, 2 * 3, 3 query = torch.randn(n, t, d, names=('N', 'T', 'D')) mask = torch.ones(n, t, names=('N', 'T')) attn = MultiHeadAttention(h, d) output = attn(query, mask=mask) # works as expected! print(output.names) 出： ('N', 'T', 'D') 以上工作正常。 此外，请注意，在代码中我们根本没有提到批量维度的名称。 实际上，我们的MultiHeadAttention模块与批量尺寸的存在无关。 query = torch.randn(t, d, names=('T', 'D')) mask = torch.ones(t, names=('T',)) output = attn(query, mask=mask) print(output.names) 出： ('T', 'D') 总结 感谢您的阅读！ 命名张量仍在发展中。 如果您有反馈和/或改进建议，请通过创建 ISSUE 来通知我们。 脚本的总运行时间：（0 分钟 0.094 秒） 下载 Python 源码：named_tensor_tutorial.py 下载 Jupyter 笔记本：named_tensor_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"43.html":{"url":"43.html","title":"PyTorch 中通道在最后的内存格式（beta）","keywords":"","body":"PyTorch 中通道在最后的内存格式（beta） 原文：https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html 作者： Vitaly Fedyunin 什么是通道在最后 通道在最后的内存格式是在保留内存尺寸的顺序中对 NCHW 张量进行排序的另一种方法。 通道最后一个张量的排序方式使通道成为最密集的维度（又称为每像素存储图像）。 例如，NCHW 张量的经典（连续）存储（在我们的示例中是具有 3 个颜色通道的两个2x2图像）如下所示： 通道最后的存储格式对数据的排序方式不同： Pytorch 通过使用现有的跨步结构支持内存格式（并提供与现有模型（包括 eager，JIT 和 TorchScript）的向后兼容性）。 例如，通道在最后的格式中的10x3x16x16批量的步幅等于(768, 1, 48, 3)。 通道最后一个存储格式仅适用于 4D NCWH 张量。 import torch N, C, H, W = 10, 3, 32, 32 内存格式 API 这是在连续和通道最后存储格式之间转换张量的方法。 经典 PyTorch 连续张量 x = torch.empty(N, C, H, W) print(x.stride()) # Ouputs: (3072, 1024, 32, 1) 出： (3072, 1024, 32, 1) 转换运算符 x = x.contiguous(memory_format=torch.channels_last) print(x.shape) # Outputs: (10, 3, 32, 32) as dimensions order preserved print(x.stride()) # Outputs: (3072, 1, 96, 3) 出： torch.Size([10, 3, 32, 32]) (3072, 1, 96, 3) 返回连续 x = x.contiguous(memory_format=torch.contiguous_format) print(x.stride()) # Outputs: (3072, 1024, 32, 1) 出： (3072, 1024, 32, 1) 替代选择 x = x.to(memory_format=torch.channels_last) print(x.stride()) # Ouputs: (3072, 1, 96, 3) 出： (3072, 1, 96, 3) 格式检查 print(x.is_contiguous(memory_format=torch.channels_last)) # Ouputs: True 出： True 最后创建为渠道 x = torch.empty(N, C, H, W, memory_format=torch.channels_last) print(x.stride()) # Ouputs: (3072, 1, 96, 3) 出： (3072, 1, 96, 3) clone保留内存格式 y = x.clone() print(y.stride()) # Ouputs: (3072, 1, 96, 3) 出： (3072, 1, 96, 3) to，cuda，float…保留内存格式 if torch.cuda.is_available(): y = x.cuda() print(y.stride()) # Ouputs: (3072, 1, 96, 3) 出： (3072, 1, 96, 3) empty_like和*_like运算符保留内存格式 y = torch.empty_like(x) print(y.stride()) # Ouputs: (3072, 1, 96, 3) 出： (3072, 1, 96, 3) 点向运算符保留内存格式 z = x + y print(z.stride()) # Ouputs: (3072, 1, 96, 3) 出： (3072, 1, 96, 3) 转换，Batchnorm模块支持通道在最后（仅适用于CudNN >= 7.6） if torch.backends.cudnn.version() >= 7603: input = torch.randint(1, 10, (2, 8, 4, 4), dtype=torch.float32, device=\"cuda\", requires_grad=True) model = torch.nn.Conv2d(8, 4, 3).cuda().float() input = input.contiguous(memory_format=torch.channels_last) model = model.to(memory_format=torch.channels_last) # Module parameters need to be Channels Last out = model(input) print(out.is_contiguous(memory_format=torch.channels_last)) # Ouputs: True 出： True 性能提升 在具有张量核心支持的 Nvidia 硬件上观察到了最大的性能提升。 在运行 Nvidia 提供的 AMP（自动混合精度）训练脚本时，我们可以将性能提高 22% 以上。 python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 ./data # opt_level = O2 # keep_batchnorm_fp32 = None # loss_scale = None # CUDNN VERSION: 7603 # => creating model 'resnet50' # Selected optimization level O2: FP16 training with FP32 batchnorm and FP32 master weights. # Defaults for this optimization level are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # Processing user overrides (additional kwargs that are not None)... # After processing overrides, optimization options are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000) # Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000) # Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000) # Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000) # Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000) # Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000) # Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000) # Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000) 传递--channels-last true允许以通道在最后的格式运行模型，观察到 22% 的表现增益。 python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 --channels-last true ./data # opt_level = O2 # keep_batchnorm_fp32 = None # loss_scale = None # # CUDNN VERSION: 7603 # # => creating model 'resnet50' # Selected optimization level O2: FP16 training with FP32 batchnorm and FP32 master weights. # # Defaults for this optimization level are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # Processing user overrides (additional kwargs that are not None)... # After processing overrides, optimization options are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # # Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000) # Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000) # Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000) # Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000) # Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000) # Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000) # Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000) # Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000) 以下模型列表完全支持通道在最后，并在 Volta 设备上显示了 8%-35% 的表现增益：alexnet，mnasnet0_5，mnasnet0_75，mnasnet1_0，mnasnet1_3，mobilenet_v2，resnet101，resnet152，resnet18，resnet34，resnet50，resnext50_32x4d，shufflenet_v2_x0_5，shufflenet_v2_x1_0，shufflenet_v2_x1_5，shufflenet_v2_x2_0，squeezenet1_0，squeezenet1_1，vgg11 ，vgg11_bn，vgg13，vgg13_bn，vgg16，vgg16_bn，vgg19，vgg19_bn，wide_resnet101_2，wide_resnet50_2 转换现有模型 通道在最后支持不受现有模型的限制，因为只要输入格式正确，任何模型都可以转换为通道在最后，并通过图传播格式。 # Need to be done once, after model initialization (or load) model = model.to(memory_format=torch.channels_last) # Replace with your model # Need to be done for every input input = input.to(memory_format=torch.channels_last) # Replace with your input output = model(input) 但是，并非所有运算符都完全转换为支持通道在最后（通常返回连续输出）。 这意味着您需要根据支持的运算符列表来验证已使用运算符的列表，或将内存格式检查引入急切的执行模式并运行模型。 运行以下代码后，如果运算符的输出与输入的存储格式不匹配，运算符将引发异常。 def contains_cl(args): for t in args: if isinstance(t, torch.Tensor): if t.is_contiguous(memory_format=torch.channels_last) and not t.is_contiguous(): return True elif isinstance(t, list) or isinstance(t, tuple): if contains_cl(list(t)): return True return False def print_inputs(args, indent=''): for t in args: if isinstance(t, torch.Tensor): print(indent, t.stride(), t.shape, t.device, t.dtype) elif isinstance(t, list) or isinstance(t, tuple): print(indent, type(t)) print_inputs(list(t), indent=indent + ' ') else: print(indent, t) def check_wrapper(fn): name = fn.__name__ def check_cl(*args, **kwargs): was_cl = contains_cl(args) try: result = fn(*args, **kwargs) except Exception as e: print(\"`{}` inputs are:\".format(name)) print_inputs(args) print('-------------------') raise e failed = False if was_cl: if isinstance(result, torch.Tensor): if result.dim() == 4 and not result.is_contiguous(memory_format=torch.channels_last): print(\"`{}` got channels_last input, but output is not channels_last:\".format(name), result.shape, result.stride(), result.device, result.dtype) failed = True if failed and True: print(\"`{}` inputs are:\".format(name)) print_inputs(args) raise Exception( 'Operator `{}` lost channels_last property'.format(name)) return result return check_cl old_attrs = dict() def attribute(m): old_attrs[m] = dict() for i in dir(m): e = getattr(m, i) exclude_functions = ['is_cuda', 'has_names', 'numel', 'stride', 'Tensor', 'is_contiguous', '__class__'] if i not in exclude_functions and not i.startswith('_') and '__call__' in dir(e): try: old_attrs[m][i] = e setattr(m, i, check_wrapper(e)) except Exception as e: print(i) print(e) attribute(torch.Tensor) attribute(torch.nn.functional) attribute(torch) 出： Optional '_Optional' object has no attribute '__name__' 如果您发现不支持通道在最后的张量的运算符并且想要贡献力量，请随时使用以下开发人员指南。 下面的代码是恢复火炬的属性。 for (m, attrs) in old_attrs.items(): for (k,v) in attrs.items(): setattr(m, k, v) 要做的工作 仍有许多事情要做，例如： 解决 N1HW 和 NC11 张量的歧义； 测试分布式训练支持； 提高运算符覆盖率。 如果您有反馈和/或改进建议，请通过创建 ISSUE 来通知我们。 脚本的总运行时间：（0 分钟 2.300 秒） 下载 Python 源码：memory_format_tutorial.py 下载 Jupyter 笔记本：memory_format_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"44.html":{"url":"44.html","title":"使用 PyTorch C++ 前端","keywords":"","body":"使用 PyTorch C++ 前端 原文：https://pytorch.org/tutorials/advanced/cpp_frontend.html PyTorch C++ 前端是 PyTorch 机器学习框架的纯 C++ 接口。 虽然 PyTorch 的主要接口自然是 Python，但此 Python API 位于强大的 C++ 代码库之上，提供基本的数据结构和功能，例如张量和自动微分。 C++ 前端公开了纯 C++ 11 API，该 API 使用机器学习训练和推理所需的工具扩展了此基础 C++ 代码库。 这包括用于神经网络建模的通用组件的内置集合； 使用自定义模块扩展此集合的 API； 一个流行的优化算法库，例如随机梯度下降； 具有 API 的并行数据加载器，用于定义和加载数据集； 序列化例程等。 本教程将引导您完成使用 C++ 前端训练模型的端到端示例。 具体来说，我们将训练 DCGAN （一种生成模型），以生成 MNIST 数字的图像。 虽然从概念上讲是一个简单的示例，但它足以使您对 PyTorch C++ 前端有个大概的了解，并且可以满足您训练更复杂模型的需求。 我们将从一些鼓舞人心的词开始，说明您为什么要使用 C++ 前端，然后直接深入定义和训练我们的模型。 小费 观看来自 CppCon 2018 的简短演讲，获得有关 C++ 前端的快速（幽默）演示。 小费 本笔记概述了 C++ 前端的组件和设计原理。 小费 有关 PyTorch C++ 生态系统的文档，请访问这个页面。 您可以在此处找到高级描述以及 API 级文档。 动机 在我们开始 GAN 和 MNIST 数字的激动人心的旅程之前，让我们退后一步，讨论为什么要使用 C++ 前端而不是 Python。 我们（PyTorch 团队）创建了 C++ 前端，以便能够在无法使用 Python 或根本不适合该工具的环境中进行研究。 此类环境的示例包括： 低延迟系统：您可能希望在具有高每秒帧数和低延迟要求的纯 C++ 游戏引擎中进行强化学习研究。 与 Python 库相比，使用纯 C++ 库更适合这种环境。 由于 Python 解释器的缓慢性，Python 可能根本无法处理。 高度多线程环境：由于全局解释器锁定（GIL），Python 一次不能运行多个系统线程。 多处理是一种替代方法，但可伸缩性却不如它，并且存在很多缺点。 C++ 没有这样的约束，线程易于使用和创建。 需要重型并行化的模型，例如深度神经演化中使用的模型，可以从中受益。 现有 C++ 代码库：您可能是现有 C++ 应用的所有者，该应用从事从后端服务器中的网页服务到照片编辑软件中的 3D 图形渲染等所有工作，并且希望将机器学习方法集成到您的系统中。 C++ 前端使您可以继续使用 C++，并避免在 Python 和 C++ 之间来回绑定的麻烦，同时保留了传统 PyTorch（Python）体验的大部分灵活性和直观性。 C++ 前端无意与 Python 前端竞争。 它是对它的补充。 我们知道研究人员和工程师都喜欢 PyTorch，因为它具有简单，灵活和直观的 API。 我们的目标是确保您可以在所有可能的环境（包括上述环境）中利用这些核心设计原则。 如果这些场景中的一种很好地描述了您的用例，或者您只是感兴趣或好奇，请在以下段落中继续研究 C++ 前端。 小费 C++ 前端试图提供一个与 Python 前端尽可能接近的 API。 如果您对 Python 前端有丰富的经验，并且问过自己“我如何使用 C++ 前端 X？”，请像在 Python 中那样编写代码，而且大多数情况下，相同的函数和方法也可以在 C++ 中使用，就像在 Python 中一样（只记得用双冒号替换点）。 编写基本应用 首先，编写一个最小的 C++ 应用，以验证我们是否在同一页面上了解我们的设置和构建环境。 首先，您需要获取 LibTorch 发行版的副本-我们现成的 zip 归档文件，其中打包了使用 C++ 前端所需的所有相关标头，库和 CMake 构建文件。 LibTorch 发行版可从 PyTorch 网站下载，适用于 Linux，MacOS 和 Windows。 本教程的其余部分将假定基本的 Ubuntu Linux 环境，但是您也可以在 MacOS 或 Windows 上随意进行操作。 小费 有关安装 PyTorch 的 C++ 发行版的说明，更详细地描述了以下步骤。 小费 在 Windows 上，调试和发行版本不兼容 ABI。 如果计划以调试模式构建项目，请尝试使用 LibTorch 的调试版本。 另外，请确保在下面的cmake --build .行中指定正确的配置。 第一步是通过从 PyTorch 网站获取的链接在本地下载 LibTorch 发行版。 对于普通的 Ubuntu Linux 环境，这意味着运行： # If you need e.g. CUDA 9.0 support, please replace \"cpu\" with \"cu90\" in the URL below. wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip unzip libtorch-shared-with-deps-latest.zip 接下来，让我们编写一个名为dcgan.cpp的小型 C++ 文件，其中包含torch/torch.h，现在只需打印出三乘三的标识矩阵即可： #include #include int main() { torch::Tensor tensor = torch::eye(3); std::cout 稍后，为了构建这个小应用以及我们完整的训练脚本，我们将使用以下CMakeLists.txt文件： cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(dcgan) find_package(Torch REQUIRED) add_executable(dcgan dcgan.cpp) target_link_libraries(dcgan \"${TORCH_LIBRARIES}\") set_property(TARGET dcgan PROPERTY CXX_STANDARD 14) 注意 虽然 CMake 是 LibTorch 的推荐构建系统，但这并不是硬性要求。 您还可以使用 Visual Studio 项目文件，QMake，普通 Makefile 或您认为合适的任何其他构建环境。 但是，我们不为此提供现成的支持。 在上面的 CMake 文件中记下第 4 行：find_package(Torch REQUIRED)。 这指示 CMake 查找 LibTorch 库的构建配置。 为了使 CMake 知道在哪里找到这些文件，调用cmake时必须设置CMAKE_PREFIX_PATH。 在执行此操作之前，让我们就dcgan应用的以下目录结构达成一致： dcgan/ CMakeLists.txt dcgan.cpp 此外，我将指向未压缩的 LibTorch 分布的路径称为/path/to/libtorch。 注意，它必须是绝对路径。 特别是，将CMAKE_PREFIX_PATH设置为../../libtorch之类的内容会以意想不到的方式中断。 而是写$PWD/../../libtorch以获取相应的绝对路径。 现在，我们准备构建我们的应用： root@fa350df05ecf:/home# mkdir build root@fa350df05ecf:/home# cd build root@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /path/to/libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /home/build root@fa350df05ecf:/home/build# cmake --build . --config Release Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan 上面，我们首先在dcgan目录内创建一个build文件夹，进入该文件夹，运行cmake命令以生成必要的构建（Make）文件，最后通过运行cmake --build . --config Release成功编译该项目。 现在我们准备执行最小的二进制文件并完成有关基本项目配置的这一部分： root@fa350df05ecf:/home/build# ./dcgan 1 0 0 0 1 0 0 0 1 [ Variable[CPUFloatType]{3,3} ] 在我看来就像一个身份矩阵！ 定义神经网络模型 现在我们已经配置了基本环境，我们可以深入研究本教程中更有趣的部分。 首先，我们将讨论如何在 C++ 前端中定义模块并与之交互。 我们将从基本的小规模示例模块开始，然后使用 C++ 前端提供的广泛的内置模块库来实现全面的 GAN。 模块 API 基础 与 Python 接口一致，基于 C++ 前端的神经网络由称为模块的可重用构建块组成。 有一个基础模块类，所有其他模块都从该基础类派生。 在 Python 中，此类为torch.nn.Module，在 C++ 中为torch::nn::Module。 除了实现模块封装的算法的forward()方法之外，模块通常还包含以下三种子对象中的任何一种：参数，缓冲区和子模块。 参数和缓冲区以张量的形式存储状态。 参数记录梯度，而缓冲区不记录。 参数通常是神经网络的可训练权重。 缓冲区的示例包括批量标准化的均值和方差。 为了重用特定的逻辑和状态块，PyTorch API 允许嵌套模块。 嵌套模块称为子模块。 参数，缓冲区和子模块必须显式注册。 注册后，可以使用parameters()或buffers()之类的方法来检索整个（嵌套）模块层次结构中所有参数的容器。 类似地，使用to(...)之类的方法，例如 to(torch::kCUDA)将所有参数和缓冲区从 CPU 移到 CUDA 内存，在整个模块层次结构上工作。 定义模块和注册参数 为了将这些词写成代码，让我们考虑一下用 Python 接口编写的简单模块： import torch class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() self.W = torch.nn.Parameter(torch.randn(N, M)) self.b = torch.nn.Parameter(torch.randn(M)) def forward(self, input): return torch.addmm(self.b, input, self.W) 在 C++ 中，它看起来像这样： #include struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { W = register_parameter(\"W\", torch::randn({N, M})); b = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return torch::addmm(b, input, W); } torch::Tensor W, b; }; 就像在 Python 中一样，我们定义了一个名为Net的类（为简单起见，这里是struct而不是class），然后从模块基类派生它。 在构造器内部，我们使用torch::randn创建张量，就像在 Python 中使用torch.randn一样。 一个有趣的区别是我们如何注册参数。 在 Python 中，我们用torch.nn.Parameter类包装了张量，而在 C++ 中，我们不得不通过register_parameter方法传递张量。 这样做的原因是 Python API 可以检测到属性为torch.nn.Parameter类型并自动注册此类张量。 在 C++ 中，反射非常受限制，因此提供了一种更传统（且不太神奇）的方法。 注册子模块并遍历模块层次结构 以相同的方式我们可以注册参数，我们也可以注册子模块。 在 Python 中，将子模块分配为模块的属性时，会自动检测并注册这些子模块： class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() # Registered as a submodule behind the scenes self.linear = torch.nn.Linear(N, M) self.another_bias = torch.nn.Parameter(torch.rand(M)) def forward(self, input): return self.linear(input) + self.another_bias 例如，这允许使用parameters()方法来递归访问模块层次结构中的所有参数： >>> net = Net(4, 5) >>> print(list(net.parameters())) [Parameter containing: tensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing: tensor([[-0.3740, -0.0976, -0.4786, -0.4928], [-0.1434, 0.4713, 0.1735, -0.3293], [-0.3467, -0.3858, 0.1980, 0.1986], [-0.1975, 0.4278, -0.1831, -0.2709], [ 0.3730, 0.4307, 0.3236, -0.0629]], requires_grad=True), Parameter containing: tensor([ 0.2038, 0.4638, -0.2023, 0.1230, -0.0516], requires_grad=True)] 要在 C++ 中注册子模块，请使用恰当命名的register_module()方法注册类似torch::nn::Linear的模块： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { another_bias = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return linear(input) + another_bias; } torch::nn::Linear linear; torch::Tensor another_bias; }; 小费 您可以在torch::nn命名空间的文档中找到可用的内置模块的完整列表，例如torch::nn::Linear，torch::nn::Dropout或torch::nn::Conv2d。 关于上述代码的一个微妙之处在于，为什么在构造器的初始值设定项列表中创建子模块，而在构造器的主体内部创建参数。 这是有充分的理由的，我们将在下面有关“C++ 前端所有权模型”的部分中对此进行介绍。 但是，最终结果是，就像 Python 中一样，我们可以递归访问模块树的参数。 调用parameters()返回一个std::vector，我们可以对其进行迭代： int main() { Net net(4, 5); for (const auto& p : net.parameters()) { std::cout 打印： root@fa350df05ecf:/home/build# ./dcgan 0.0345 1.4456 -0.6313 -0.3585 -0.4008 [ Variable[CPUFloatType]{5} ] -0.1647 0.2891 0.0527 -0.0354 0.3084 0.2025 0.0343 0.1824 -0.4630 -0.2862 0.2500 -0.0420 0.3679 -0.1482 -0.0460 0.1967 0.2132 -0.1992 0.4257 0.0739 [ Variable[CPUFloatType]{5,4} ] 0.01 * 3.6861 -10.1166 -45.0333 7.9983 -20.0705 [ Variable[CPUFloatType]{5} ] 具有三个参数，就像在 Python 中一样。 为了也查看这些参数的名称，C++ API 提供了named_parameters()方法，该方法返回OrderedDict就像在 Python 中一样： Net net(4, 5); for (const auto& pair : net.named_parameters()) { std::cout 我们可以再次执行以查看输出： root@fa350df05ecf:/home/build# make && ./dcgan 11:13:48 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan b: -0.1863 -0.8611 -0.1228 1.3269 0.9858 [ Variable[CPUFloatType]{5} ] linear.weight: 0.0339 0.2484 0.2035 -0.2103 -0.0715 -0.2975 -0.4350 -0.1878 -0.3616 0.1050 -0.4982 0.0335 -0.1605 0.4963 0.4099 -0.2883 0.1818 -0.3447 -0.1501 -0.0215 [ Variable[CPUFloatType]{5,4} ] linear.bias: -0.0250 0.0408 0.3756 -0.2149 -0.3636 [ Variable[CPUFloatType]{5} ] 注意 torch::nn::Module的文档包含在模块层次结构上运行的方法的完整列表。 在正向模式下运行网络 要使用 C++ 执行网络，我们只需调用我们自己定义的forward()方法： int main() { Net net(4, 5); std::cout 打印类似： root@fa350df05ecf:/home/build# ./dcgan 0.8559 1.1572 2.1069 -0.1247 0.8060 0.8559 1.1572 2.1069 -0.1247 0.8060 [ Variable[CPUFloatType]{2,5} ] 模块所有权 至此，我们知道了如何使用 C++ 定义模块，注册参数，注册子模块，通过parameters()之类的方法遍历模块层次结构并最终运行模块的forward()方法。 尽管在 C++ API 中还有很多方法，类和主题需要使用，但我将为您提供完整菜单的文档。 我们将在稍后实现 DCGAN 模型和端到端训练管道的过程中，涉及更多概念。 在我们这样做之前，让我简要介绍一下 C++ 前端为torch::nn::Module的子类提供的所有权模型。 在本次讨论中，所有权模型是指模块的存储和传递方式-确定特定模块实例的所有者或所有者。 在 Python 中，对象始终是动态分配的（在堆上），并且具有引用语义。 这是非常容易使用且易于理解的。 实际上，在 Python 中，您可以很大程度上忽略对象的位置以及如何引用它们，而将精力集中在完成事情上。 C++ 是一种较低级的语言，它在此领域提供了更多选择。 这增加了复杂性，并严重影响了 C++ 前端的设计和人体工程学。 特别是，对于 C++ 前端中的模块，我们可以选择使用值语义或引用语义。 第一种情况是最简单的，并且在到目前为止的示例中已进行了展示：模块对象在栈上分配，并在传递给函数时可以被复制，移动（使用std::move）或通过引用或指针获取： struct Net : torch::nn::Module { }; void a(Net net) { } void b(Net& net) { } void c(Net* net) { } int main() { Net net; a(net); a(std::move(net)); b(net); c(&net); } 对于第二种情况-引用语义-我们可以使用std::shared_ptr。 引用语义的优势在于，就像在 Python 中一样，它减少了思考如何将模块传递给函数以及如何声明参数的认知开销（假设您在任何地方都使用shared_ptr）。 struct Net : torch::nn::Module {}; void a(std::shared_ptr net) { } int main() { auto net = std::make_shared(); a(net); } 根据我们的经验，来自动态语言的研究人员非常喜欢引用语义而不是值语义，即使后者比 C++ 更“原生”。 同样重要的是要注意，torch::nn::Module的设计要与 Python API 的人体工程学保持紧密联系，因此要依靠共享所有权。 例如，采用我们较早的（此处为缩短的）Net定义： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; 为了使用linear子模块，我们想将其直接存储在我们的类中。 但是，我们还希望模块基类了解并有权访问此子模块。 为此，它必须存储对此子模块的引用。 至此，我们已经达到了共享所有权的需要。 torch::nn::Module类和具体的Net类都需要引用该子模块。 因此，基类将模块存储为shared_ptr，因此具体类也必须存储。 可是等等！ 在上面的代码中我没有提到shared_ptr！ 这是为什么？ 好吧，因为std::shared_ptr实在令人难受。 为了保持研究人员的生产力，我们提出了一个精心设计的方案，以隐藏shared_ptr的提法-通常保留给值语义的好处-同时保留引用语义。 要了解它是如何工作的，我们可以看一下核心库中torch::nn::Linear模块的简化定义（完整定义在此处）： struct LinearImpl : torch::nn::Module { LinearImpl(int64_t in, int64_t out); Tensor forward(const Tensor& input); Tensor weight, bias; }; TORCH_MODULE(Linear); 简而言之：该模块不是Linear，而是LinearImpl。 然后，宏TORCH_MODULE定义了实际的Linear类。 这个“生成的”类实际上是std::shared_ptr的包装。 它是一个包装器，而不是简单的typedef，因此，除其他事项外，构造器仍可按预期工作，即，您仍然可以编写torch::nn::Linear(3, 4)而不是std::make_shared(3, 4)。 我们将由宏创建的类称为模块所有者。 与（共享）指针一样，您可以使用箭头运算符（例如model->forward(...)）访问基础对象。 最终结果是一个所有权模型，该模型非常类似于 Python API。 引用语义成为默认语义，但是没有额外输入std::shared_ptr或std::make_shared。 对于我们的Net，使用模块持有人 API 如下所示： struct NetImpl : torch::nn::Module {}; TORCH_MODULE(Net); void a(Net net) { } int main() { Net net; a(net); } 这里有一个微妙的问题值得一提。 默认构造的std::shared_ptr为“空”，即包含空指针。 什么是默认构造的Linear或Net？ 好吧，这是一个棘手的选择。 我们可以说它应该是一个空（null）std::shared_ptr。 但是，请记住Linear(3, 4)与std::make_shared(3, 4)相同。 这意味着如果我们已确定Linear linear;应该为空指针，则将无法构造不采用任何构造器参数或都不使用所有缺省构造器的模块。 因此，在当前的 API 中，默认构造的模块持有人（如Linear()）将调用基础模块的默认构造器（LinearImpl()）。 如果基础模块没有默认构造器，则会出现编译器错误。 要构造空持有人，可以将nullptr传递给持有人的构造器。 实际上，这意味着您可以使用如先前所示的子模块，在初始化器列表中注册并构造该模块： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; 或者，您可以先使用空指针构造持有人，然后在构造器中为其分配值（Python 爱好者更熟悉）： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { linear = register_module(\"linear\", torch::nn::Linear(N, M)); } torch::nn::Linear linear{nullptr}; // construct an empty holder }; 结论：您应该使用哪种所有权模型–哪种语义？ C++ 前端的 API 最能支持模块所有者提供的所有权模型。 这种机制的唯一缺点是在模块声明下方多了一行样板。 也就是说，最简单的模型仍然是 C++ 模块简介中显示的值语义模型。 对于小的，简单的脚本，您也可以摆脱它。 但是，由于技术原因，您迟早会发现它并不总是受支持。 例如，序列化 API（torch::save和torch::load）仅支持模块支架（或普通shared_ptr）。 因此，建议使用模块持有人 API 和 C++ 前端定义模块，此后我们将在本教程中使用此 API。 定义 DCGAN 模块 现在，我们有必要的背景和简介来定义我们要在本文中解决的机器学习任务的模块。 回顾一下：我们的任务是从 MNIST 数据集生成数字图像。 我们想使用生成对抗网络（GAN）解决此任务。 特别是，我们将使用 DCGAN 架构，这是同类中最早，最简单的架构之一，但完全可以完成此任务。 小费 您可以在存储库中找到本教程中提供的完整源代码。 什么是 GAN aGAN？ GAN 由两个不同的神经网络模型组成：生成器和判别器。 生成器从噪声分布中接收样本，其目的是将每个噪声样本转换为类似于目标分布的图像（在我们的情况下为 MNIST 数据集）。 判别器又从 MNIST 数据集接收实际图像，或从生成器接收假图像。 要求发出一个概率来判断特定图像的真实程度（接近1）或伪造（接近0）。 来自判别器的关于由生成器产生的图像有多真实的反馈被用来训练生成器。 判别器对真实性有多好的反馈将用于优化判别器。 从理论上讲，生成器和判别器之间的微妙平衡使它们连接起来得到改善，从而导致生成器生成与目标分布无法区分的图像，从而使判别器（那时）的敏锐眼睛冒出了散发0.5的真实和真实可能性。 假图片。 对我们来说，最终结果是一台接收噪声作为输入并生成逼真的数字图像作为其输出的机器。 生成器模块 我们首先定义生成器模块，该模块由一系列转置的 2D 卷积，批量归一化和 ReLU 激活单元组成。 我们在定义自己的模块的forward()方法中显式地（在功能上）在模块之间传递输入： struct DCGANGeneratorImpl : nn::Module { DCGANGeneratorImpl(int kNoiseSize) : conv1(nn::ConvTranspose2dOptions(kNoiseSize, 256, 4) .bias(false)), batch_norm1(256), conv2(nn::ConvTranspose2dOptions(256, 128, 3) .stride(2) .padding(1) .bias(false)), batch_norm2(128), conv3(nn::ConvTranspose2dOptions(128, 64, 4) .stride(2) .padding(1) .bias(false)), batch_norm3(64), conv4(nn::ConvTranspose2dOptions(64, 1, 4) .stride(2) .padding(1) .bias(false)) { // register_module() is needed if we want to use the parameters() method later on register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); register_module(\"conv3\", conv3); register_module(\"conv4\", conv4); register_module(\"batch_norm1\", batch_norm1); register_module(\"batch_norm2\", batch_norm2); register_module(\"batch_norm3\", batch_norm3); } torch::Tensor forward(torch::Tensor x) { x = torch::relu(batch_norm1(conv1(x))); x = torch::relu(batch_norm2(conv2(x))); x = torch::relu(batch_norm3(conv3(x))); x = torch::tanh(conv4(x)); return x; } nn::ConvTranspose2d conv1, conv2, conv3, conv4; nn::BatchNorm2d batch_norm1, batch_norm2, batch_norm3; }; TORCH_MODULE(DCGANGenerator); DCGANGenerator generator(kNoiseSize); 现在我们可以在DCGANGenerator上调用forward()将噪声样本映射到图像。 选择的特定模块，例如nn::ConvTranspose2d和nn::BatchNorm2d，遵循前面概述的结构。 kNoiseSize常数确定输入噪声向量的大小，并将其设置为100。 当然，超参数是通过研究生的血统发现的。 Attention No grad students were harmed in the discovery of hyperparameters. They were fed Soylent regularly. Note A brief word on the way options are passed to built-in modules like Conv2d in the C++ frontend: Every module has some required options, like the number of features for BatchNorm2d. If you only need to configure the required options, you can pass them directly to the module’s constructor, like BatchNorm2d(128) or Dropout(0.5) or Conv2d(8, 4, 2) (for input channel count, output channel count, and kernel size). If, however, you need to modify other options, which are normally defaulted, such as bias for Conv2d, you need to construct and pass an options object. Every module in the C++ frontend has an associated options struct, called ModuleOptions where Module is the name of the module, like LinearOptions for Linear. This is what we do for the Conv2d modules above. 判别器模块 The discriminator is similarly a sequence of convolutions, batch normalizations and activations. However, the convolutions are now regular ones instead of transposed, and we use a leaky ReLU with an alpha value of 0.2 instead of a vanilla ReLU. Also, the final activation becomes a Sigmoid, which squashes values into a range between 0 and 1. We can then interpret these squashed values as the probabilities the discriminator assigns to images being real. To build the discriminator, we will try something different: a Sequential module. Like in Python, PyTorch here provides two APIs for model definition: a functional one where inputs are passed through successive functions (e.g. the generator module example), and a more object-oriented one where we build a Sequential module containing the entire model as submodules. Using Sequential, the discriminator would look like: nn::Sequential discriminator( // Layer 1 nn::Conv2d( nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).bias(false)), nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)), // Layer 2 nn::Conv2d( nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).bias(false)), nn::BatchNorm2d(128), nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)), // Layer 3 nn::Conv2d( nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).bias(false)), nn::BatchNorm2d(256), nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)), // Layer 4 nn::Conv2d( nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).bias(false)), nn::Sigmoid()); Tip A Sequential module simply performs function composition. The output of the first submodule becomes the input of the second, the output of the third becomes the input of the fourth and so on. 加载数据 Now that we have defined the generator and discriminator model, we need some data we can train these models with. The C++ frontend, like the Python one, comes with a powerful parallel data loader. This data loader can read batches of data from a dataset (which you can define yourself) and provides many configuration knobs. 注意 While the Python data loader uses multi-processing, the C++ data loader is truly multi-threaded and does not launch any new processes. The data loader is part of the C++ frontend’s data api, contained in the torch::data:: namespace. This API consists of a few different components: 数据加载器类， 用于定义数据集的 API， 用于定义转换的 API，可以将其应用于数据集， 用于定义采样器的 API，该采样器会生成用于对数据集建立索引的索引， 现有数据集，变换和采样器的库。 For this tutorial, we can use the MNIST dataset that comes with the C++ frontend. Let’s instantiate a torch::data::datasets::MNIST for this, and apply two transformations: First, we normalize the images so that they are in the range of -1 to +1 (from an original range of 0 to 1). Second, we apply the Stack collation, which takes a batch of tensors and stacks them into a single tensor along the first dimension: auto dataset = torch::data::datasets::MNIST(\"./mnist\") .map(torch::data::transforms::Normalize<>(0.5, 0.5)) .map(torch::data::transforms::Stack<>()); Note that the MNIST dataset should be located in the ./mnist directory relative to wherever you execute the training binary from. You can use this script to download the MNIST dataset. 接下来，我们创建一个数据加载器并将其传递给该数据集。 为了创建一个新的数据加载器，我们使用torch::data::make_data_loader，它返回正确类型的std::unique_ptr（取决于数据集的类型，采样器的类型以及其他一些实现细节）： auto data_loader = torch::data::make_data_loader(std::move(dataset)); 数据加载器确实提供了很多选项。 您可以在这里检查全套。 例如，为了加快数据加载速度，我们可以增加工作器的数量。 默认数字为零，这意味着将使用主线程。 如果将workers设置为2，将产生两个线程并发加载数据。 我们还应该将批量大小从其默认值1增加到更合理的值，例如64（kBatchSize的值）。 因此，让我们创建一个DataLoaderOptions对象并设置适当的属性： auto data_loader = torch::data::make_data_loader( std::move(dataset), torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2)); 现在，我们可以编写一个循环来加载批量数据，目前我们仅将其打印到控制台： for (torch::data::Example<>& batch : *data_loader) { std::cout () 在这种情况下，数据加载器返回的类型为torch::data::Example。 此类型是一种简单的结构，其中的data字段用于数据，而target字段用于标签。 因为我们之前应用了Stack归类，所以数据加载器仅返回一个这样的示例。 如果我们未应用排序规则，则数据加载器将改为生成std::vector>，批量中每个示例包含一个元素。 如果重建并运行此代码，则应看到类似以下内容的内容： root@fa350df05ecf:/home/build# make Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan root@fa350df05ecf:/home/build# make [100%] Built target dcgan root@fa350df05ecf:/home/build# ./dcgan Batch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9 Batch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2 Batch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0 Batch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7 Batch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0 Batch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2 Batch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7 Batch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3 Batch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7 Batch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4 Batch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6 ... 这意味着我们能够成功地从 MNIST 数据集中加载数据。 编写训练循环 现在，让我们完成示例的算法部分，并实现生成器和判别器之间的精妙舞蹈。 首先，我们将创建两个优化器，一个用于生成器，一个用于判别器。 我们使用的优化程序实现了 Adam 算法： torch::optim::Adam generator_optimizer( generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam discriminator_optimizer( discriminator->parameters(), torch::optim::AdamOptions(5e-4).beta1(0.5)); 注意 在撰写本文时，C++ 前端提供了实现 Adagrad，Adam，LBBFG，RMSprop 和 SGD 的优化器。 文档具有最新列表。 接下来，我们需要更新我们的训练循环。 我们将添加一个外循环以在每个周期耗尽数据加载器，然后编写 GAN 训练代码： for (int64_t epoch = 1; epoch & batch : *data_loader) { // Train discriminator with real images. discriminator->zero_grad(); torch::Tensor real_images = batch.data; torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0); torch::Tensor real_output = discriminator->forward(real_images); torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels); d_loss_real.backward(); // Train discriminator with fake images. torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1}); torch::Tensor fake_images = generator->forward(noise); torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); torch::Tensor fake_output = discriminator->forward(fake_images.detach()); torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels); d_loss_fake.backward(); torch::Tensor d_loss = d_loss_real + d_loss_fake; discriminator_optimizer.step(); // Train generator. generator->zero_grad(); fake_labels.fill_(1); fake_output = discriminator->forward(fake_images); torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels); g_loss.backward(); generator_optimizer.step(); std::printf( \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\", epoch, kNumberOfEpochs, ++batch_index, batches_per_epoch, d_loss.item(), g_loss.item()); } } 上面，我们首先在真实图像上评估判别器，为此应为其分配较高的概率。 为此，我们使用torch::empty(batch.data.size(0)).uniform_(0.8, 1.0)作为目标概率。 注意 我们选择均匀分布在 0.8 到 1.0 之间的随机值，而不是各处的 1.0，以使判别器训练更可靠。 此技巧称为标签平滑。 在评估判别器之前，我们将其参数的梯度归零。 计算完损失后，我们通过调用d_loss.backward()来计算新的梯度，从而在网络中反向传播。 我们对虚假图像重复此步骤。 我们不使用数据集中的图像，而是让生成器通过为它提供一批随机噪声来为此创建伪造图像。 然后，我们将这些伪造图像转发给判别器。 这次，我们希望判别器发出低概率，最好是全零。 一旦计算了一批真实图像和一批伪造图像的判别器损失，我们就可以一步一步地进行判别器的优化程序，以更新其参数。 为了训练生成器，我们再次首先将其梯度归零，然后在伪图像上重新评估判别器。 但是，这一次，我们希望判别器将概率分配为非常接近的概率，这将表明生成器可以生成使判别器认为它们实际上是真实的图像（来自数据集）。 为此，我们用全部填充fake_labels张量。 最后，我们逐步使用生成器的优化器来更新其参数。 现在，我们应该准备在 CPU 上训练我们的模型。 我们还没有任何代码可以捕获状态或示例输出，但是我们稍后会添加。 现在，让我们观察一下我们的模型正在做某事 –我们稍后将根据生成的图像来验证这是否有意义。 重建和运行应打印如下内容： root@3c0711f20896:/home/build# make && ./dcgan Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcga [ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304 [ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101 [ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626 [ 1/10][400/938] D_loss: 0.8057 | G_loss: 2.2795 [ 1/10][500/938] D_loss: 0.3531 | G_loss: 4.4452 [ 1/10][600/938] D_loss: 0.3501 | G_loss: 5.0811 [ 1/10][700/938] D_loss: 0.3581 | G_loss: 4.5623 [ 1/10][800/938] D_loss: 0.6423 | G_loss: 1.7385 [ 1/10][900/938] D_loss: 0.3592 | G_loss: 4.7333 [ 2/10][100/938] D_loss: 0.4660 | G_loss: 2.5242 [ 2/10][200/938] D_loss: 0.6364 | G_loss: 2.0886 [ 2/10][300/938] D_loss: 0.3717 | G_loss: 3.8103 [ 2/10][400/938] D_loss: 1.0201 | G_loss: 1.3544 [ 2/10][500/938] D_loss: 0.4522 | G_loss: 2.6545 ... 移至 GPU 虽然我们当前的脚本可以在 CPU 上正常运行，但是我们都知道卷积在 GPU 上要快得多。 让我们快速讨论如何将训练转移到 GPU 上。 为此，我们需要做两件事：将 GPU 设备规范传递给我们分配给自己的张量，并通过to()方法将所有其他张量明确复制到 C++ 前端中的所有张量和模块上。 实现这两者的最简单方法是在我们的训练脚本的顶层创建torch::Device的实例，然后将该设备传递给张量工厂函数，例如torch::zeros和to()方法。 我们可以从使用 CPU 设备开始： // Place this somewhere at the top of your training script. torch::Device device(torch::kCPU); 新的张量分配，例如 torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); 应该更新为以device作为最后一个参数： torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device); 对于那些不在我们手中的张量，例如来自 MNIST 数据集的张量，我们必须插入显式的to()调用。 这表示 torch::Tensor real_images = batch.data; 变成 torch::Tensor real_images = batch.data.to(device); 并且我们的模型参数也应该移到正确的设备上： generator->to(device); discriminator->to(device); 注意 如果张量已经存在于提供给to()的设备上，则该调用为空操作。 没有多余的副本。 至此，我们已经使之前的 CPU 代码更加明确了。 但是，现在将设备更改为 CUDA 设备也非常容易： torch::Device device(torch::kCUDA) 现在，所有张量都将驻留在 GPU 上，并调用快速 CUDA 内核进行所有操作，而无需我们更改任何下游代码。 如果我们想指定一个特定的设备索引，则可以将其作为第二个参数传递给Device构造器。 如果我们希望不同的张量驻留在不同的设备上，则可以传递单独的设备实例（例如，一个在 CUDA 设备 0 上，另一个在 CUDA 设备 1 上）。 我们甚至可以动态地进行此配置，这通常对于使我们的训练脚本更具可移植性很有用： torch::Device device = torch::kCPU; if (torch::cuda::is_available()) { std::cout 甚至 torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU); 检查点和恢复训练状态 我们应该对训练脚本进行的最后扩充是定期保存模型参数的状态，优化器的状态以及一些生成的图像样本。 如果我们的计算机在训练过程中崩溃，则前两个将使我们能够恢复训练状态。 对于长期的训练过程，这是绝对必要的。 幸运的是，C++ 前端提供了一个 API，用于对模型和优化器状态以及单个张量进行序列化和反序列化。 为此的核心 API 是torch::save(thing,filename)和torch::load(thing,filename)，其中thing可以是torch::nn::Module子类，也可以是优化脚本实例，例如我们在训练脚本中拥有的Adam对象。 让我们更新训练循环，以一定间隔检查模型和优化器状态： if (batch_index % kCheckpointEvery == 0) { // Checkpoint the model and optimizer state. torch::save(generator, \"generator-checkpoint.pt\"); torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::save(discriminator, \"discriminator-checkpoint.pt\"); torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); // Sample the generator and save the images. torch::Tensor samples = generator->forward(torch::randn({8, kNoiseSize, 1, 1}, device)); torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\")); std::cout checkpoint \" 其中kCheckpointEvery是设置为类似于100之类的整数，用于每批100批量检查点，而checkpoint_counter是每次创建检查点时都会增加的计数器。 要恢复训练状态，可以在创建所有模型和优化器之后但在训练循环之前添加如下代码： torch::optim::Adam generator_optimizer( generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam discriminator_optimizer( discriminator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); if (kRestoreFromCheckpoint) { torch::load(generator, \"generator-checkpoint.pt\"); torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::load(discriminator, \"discriminator-checkpoint.pt\"); torch::load( discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); } int64_t checkpoint_counter = 0; for (int64_t epoch = 1; epoch & batch : *data_loader) { 检查生成的图像 我们的训练脚本现已完成。 我们准备在 CPU 或 GPU 上训练 GAN。 为了检查我们训练过程的中间输出，为此我们添加了将代码样本定期保存到\"dcgan-sample-xxx.pt\"文件的代码，我们可以编写一个小的 Python 脚本来加载张量并使用 matplotlib 显示它们： from __future__ import print_function from __future__ import unicode_literals import argparse import matplotlib.pyplot as plt import torch parser = argparse.ArgumentParser() parser.add_argument(\"-i\", \"--sample-file\", required=True) parser.add_argument(\"-o\", \"--out-file\", default=\"out.png\") parser.add_argument(\"-d\", \"--dimension\", type=int, default=3) options = parser.parse_args() module = torch.jit.load(options.sample_file) images = list(module.parameters())[0] for index in range(options.dimension * options.dimension): image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8) array = image.numpy() axis = plt.subplot(options.dimension, options.dimension, 1 + index) plt.imshow(array, cmap=\"gray\") axis.get_xaxis().set_visible(False) axis.get_yaxis().set_visible(False) plt.savefig(options.out_file) print(\"Saved \", options.out_file) 现在，让我们训练模型约 30 个周期： root@3c0711f20896:/home/build# make && ./dcgan 10:17:57 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan CUDA is available! Training on GPU. [ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195 -> checkpoint 1 [ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148 -> checkpoint 2 [ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760 -> checkpoint 3 [ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250 -> checkpoint 4 [ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790 -> checkpoint 5 [ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315 ... -> checkpoint 120 [30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084 并在图中显示图像： root@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt Saved out.png 应该看起来像这样： 数字！ 万岁！ 现在，事情就在您的球场上了：您可以改进模型以使数字看起来更好吗？ 总结 希望本教程为您提供了 PyTorch C++ 前端的可摘要。 像 PyTorch 这样的机器学习库必然具有非常广泛的 API。 因此，有许多概念我们没有时间或空间来讨论。 但是，我建议您尝试一下 API，并在遇到问题时查阅我们的文档，尤其是库 API 部分。 另外，请记住，只要我们能够做到，就可以期望 C++ 前端遵循 Python 前端的设计和语义，因此您可以利用这一事实来提高学习率。 小费 您可以在存储库中找到本教程中提供的完整源代码。 与往常一样，如果您遇到任何问题或疑问，可以使用我们的论坛或 GitHub ISSUE 进行联系。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"45.html":{"url":"45.html","title":"自定义 C++ 和 CUDA 扩展","keywords":"","body":"自定义 C++ 和 CUDA 扩展 原文：https://pytorch.org/tutorials/advanced/cpp_extension.html 作者： Peter Goldsborough PyTorch 提供了与神经网络，任意张量代数，数据整理和其他目的有关的大量操作。 但是，您仍然可能发现自己需要更多的自定义操作。 例如，您可能想使用论文中发现的新颖的激活函数，或者实现您在研究过程中开发的操作。 在 PyTorch 中集成这样的自定义操作的最简单方法是通过扩展此处概述的Function和Module来用 Python 编写它。 这为您提供了自动微分的全部功能（使您不必编写导函数）以及 Python 的通常表达能力。 但是，有时您的操作可以用 C++ 更好地实现。 例如，您的代码可能确实需要速度，因为在模型中它经常被调用，或者即使很少调用也很昂贵。 另一个合理的原因是它依赖于其他 C 或 C++ 库或与之交互。 为了解决这种情况，PyTorch 提供了一种非常简单的方式来编写自定义 C++ 扩展。 C++ 扩展是我们开发的一种机制，允许用户（您）创建源外定义的 PyTorch 运算符，即与 PyTorch 后端分开。 该方法不同于本机 PyTorch 操作的实现方式。 C++ 扩展旨在为您节省大量与将操作与 PyTorch 后端集成在一起相关的样板，同时为基于 PyTorch 的项目提供高度的灵活性。 但是，一旦将操作定义为 C++ 扩展，将其转换为本地 PyTorch 函数在很大程度上取决于代码组织，如果您决定在上游进行操作，则可以解决此问题。 动机和示例 本说明的其余部分将逐步介绍编写和使用 C++（和 CUDA）扩展的实际示例。 如果您被追捕，或者在一天结束前仍未完成该操作，就会有人开除您，则可以跳过本节，直接进入下一部分的实现细节。 假设您想出了一种新型的循环装置，发现与现有技术相比，它具有更好的表现。 该循环单元类似于 LSTM，但不同之处在于它缺少遗忘门，并使用指数线性单元（ELU）作为其内部激活函数。 由于此设备永远不会忘记，因此我们将其称为 LLTM 或长期记忆单元。 LLTM 与普通 LSTM 的两种区别非常重要，以至于我们无法为自己的目的配置 PyTorch 的LSTMCell，因此我们必须创建一个自定义单元。 这样做的第一个也是最简单的方法，并且在所有情况下都可能是一个好的第一步，是使用 Python 在纯 PyTorch 中实现我们所需的功能。 为此，我们需要子类torch.nn.Module并实现 LLTM 的正向传播。 看起来像这样： class LLTM(torch.nn.Module): def __init__(self, input_features, state_size): super(LLTM, self).__init__() self.input_features = input_features self.state_size = state_size # 3 * state_size for input gate, output gate and candidate cell gate. # input_features + state_size because we will multiply with [input, h]. self.weights = torch.nn.Parameter( torch.empty(3 * state_size, input_features + state_size)) self.bias = torch.nn.Parameter(torch.empty(3 * state_size)) self.reset_parameters() def reset_parameters(self): stdv = 1.0 / math.sqrt(self.state_size) for weight in self.parameters(): weight.data.uniform_(-stdv, +stdv) def forward(self, input, state): old_h, old_cell = state X = torch.cat([old_h, input], dim=1) # Compute the input, output and candidate cell gates with one MM. gate_weights = F.linear(X, self.weights, self.bias) # Split the combined gate weight matrix into its components. gates = gate_weights.chunk(3, dim=1) input_gate = torch.sigmoid(gates[0]) output_gate = torch.sigmoid(gates[1]) # Here we use an ELU instead of the usual tanh. candidate_cell = F.elu(gates[2]) # Compute the new cell state. new_cell = old_cell + candidate_cell * input_gate # Compute the new hidden state and output. new_h = torch.tanh(new_cell) * output_gate return new_h, new_cell 然后我们可以按预期使用： import torch X = torch.randn(batch_size, input_features) h = torch.randn(batch_size, state_size) C = torch.randn(batch_size, state_size) rnn = LLTM(input_features, state_size) new_h, new_C = rnn(X, (h, C)) 自然，如果可能的话，您应该使用这种方法扩展 PyTorch。 由于 PyTorch 对 CPU 和 GPU 的操作进行了高度优化的实现，并由 NVIDIA cuDNN，Intel MKL 或 NNPACK 等库提供支持 ，上面的 PyTorch 代码通常会足够快。 但是，我们还可以看到为什么在某些情况下还有进一步改进性能的空间。 最明显的原因是 PyTorch 不了解您要实现的算法。 它仅知道您用于组成算法的单个操作。 因此，PyTorch 必须一个接一个地执行您的操作。 由于对操作的实现（或核）的每个单独调用（可能涉及 CUDA 内核的启动）都具有一定的开销，因此该开销在许多函数调用中可能变得很重要。 此外，运行我们的代码的 Python 解释器本身可能会使我们的程序变慢。 因此，一种确定的加速方法是用 C++（或 CUDA）和熔断特定操作组来重写零件。 融合是指将许多功能的实现组合为一个功能，这可以从更少的内核启动以及我们可以提高全局数据流可见性的情况下执行的其他优化中获利。 让我们看看如何使用 C++ 扩展来实现 LLTM 的融合版本。 首先，我们使用 ATen 库以普通的 C++ 语言编写代码，该库为 PyTorch 的许多后端提供了强大的支持，并了解它如何使我们轻松转换 Python 代码。 然后，我们将模型的某些部分移至 CUDA 内核，以从 GPU 提供的大量并行处理中受益，从而进一步加快处理速度。 编写 C++ 扩展 C++ 扩展有两种形式：它们可以使用setuptools提前构建，也可以通过torch.utils.cpp_extension.load()适时构建。 我们将从第一种方法开始，稍后再讨论后者。 使用setuptools构建 为了“提前”，我们通过编写一个setup.py脚本来构建 C++ 扩展，该脚本使用setuptools编译我们的 C++ 代码。 对于 LLTM，它看起来像这样简单： from setuptools import setup, Extension from torch.utils import cpp_extension setup(name='lltm_cpp', ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])], cmdclass={'build_ext': cpp_extension.BuildExtension}) 在此代码中，CppExtension是setuptools.Extension的便利包装，它传递正确的包含路径并将扩展的语言设置为 C++。 等效的setuptools原始代码如下： Extension( name='lltm_cpp', sources=['lltm.cpp'], include_dirs=cpp_extension.include_paths(), language='c++') BuildExtension执行许多必需的配置步骤，并检查和管理混合 C++/CUDA 扩展的混合编译。 这就是我们现在真正需要了解的有关构建 C++ 扩展的全部信息！ 现在让我们看一下lltm.cpp中 C++ 扩展的实现。 编写 C++ 操作 让我们开始以 C++ 实现 LLTM！ 我们需要反向传播的一项函数是 Sigmoid 导数。 这是一小段代码，用于讨论编写 C++ 扩展时可供我们使用的总体环境： #include #include torch::Tensor d_sigmoid(torch::Tensor z) { auto s = torch::sigmoid(z); return (1 - s) * s; } 是一站式标头，包括编写 C++ 扩展的所有必需的 PyTorch 位。 这包括： ATen 库，这是我们用于张量计算的主要 API， pybind11 ，这是我们为 C++ 代码创建 Python 绑定的方式， 标头，用于管理 ATen 与pybind11之间的交互的详细信息。 d_sigmoid()的实现显示了如何使用 ATen API。 PyTorch 的张量和变量接口是从 ATen 库自动生成的，因此我们可以或多或少地将 Python 实现 1:1 转换为 C++。 我们用于所有计算的主要数据类型将为torch::Tensor。 可以在此处检查其完整的 API。 还要注意，我们可以包括或任何其他 C 或 C++ 头文件 –我们拥有 C++ 11 的全部功能。 正向传播 接下来，我们可以将整个正向传播到 C++： #include std::vector lltm_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { auto X = torch::cat({old_h, input}, /*dim=*/1); auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1)); auto gates = gate_weights.chunk(3, /*dim=*/1); auto input_gate = torch::sigmoid(gates[0]); auto output_gate = torch::sigmoid(gates[1]); auto candidate_cell = torch::elu(gates[2], /*alpha=*/1.0); auto new_cell = old_cell + candidate_cell * input_gate; auto new_h = torch::tanh(new_cell) * output_gate; return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gate_weights}; } 反向传播 C++ 扩展 API 当前不提供为我们自动生成向后函数的方法。 因此，我们还必须实现 LLTM 的后向传递，它计算相对于正向传播的每个输入的损失导数。 最终，我们将正向和反向函数放入torch.autograd.Function中，以创建一个不错的 Python 绑定。 向后函数的作用稍大一些，因此我们将不深入研究代码（如果您有兴趣，请阅读 Alex Graves 的论文，以获取有关此方面的更多信息）： // tanh'(z) = 1 - tanh^2(z) torch::Tensor d_tanh(torch::Tensor z) { return 1 - z.tanh().pow(2); } // elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) 0).type_as(z) + mask.type_as(z) * (alpha * e); } std::vector lltm_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gate_weights, torch::Tensor weights) { auto d_output_gate = torch::tanh(new_cell) * grad_h; auto d_tanh_new_cell = output_gate * grad_h; auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell; auto d_old_cell = d_new_cell; auto d_candidate_cell = input_gate * d_new_cell; auto d_input_gate = candidate_cell * d_new_cell; auto gates = gate_weights.chunk(3, /*dim=*/1); d_input_gate *= d_sigmoid(gates[0]); d_output_gate *= d_sigmoid(gates[1]); d_candidate_cell *= d_elu(gates[2]); auto d_gates = torch::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1); auto d_weights = d_gates.t().mm(X); auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true); auto d_X = d_gates.mm(weights); const auto state_size = grad_h.size(1); auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size); auto d_input = d_X.slice(/*dim=*/1, state_size); return {d_old_h, d_input, d_weights, d_bias, d_old_cell}; } 绑定到 Python 一旦用 C++ 和 ATen 编写了操作，就可以使用pybind11以非常简单的方式将 C++ 函数或类绑定到 Python 中。 您对 PyTorch C++ 扩展部分的疑问或问题将在pybind11文档中得到解决。 对于我们的扩展，必要的绑定代码仅跨越四行： PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(\"forward\", &lltm_forward, \"LLTM forward\"); m.def(\"backward\", &lltm_backward, \"LLTM backward\"); } 这里要注意的一点是宏TORCH_EXTENSION_NAME。 火炬扩展程序构建会将其定义为您在setup.py脚本中为扩展程序指定的名称。 在这种情况下，TORCH_EXTENSION_NAME的值为lltm。 这是为了避免在两个位置（构建脚本和 C++ 代码）都保留扩展名，因为两者之间的不匹配会导致令人讨厌且难以跟踪的问题。 使用扩展程序 现在，我们准备将扩展名导入 PyTorch 中。 此时，您的目录结构可能如下所示： pytorch/ lltm-extension/ lltm.cpp setup.py 现在，运行python setup.py install来构建和安装扩展程序。 看起来应该像这样： running install running bdist_egg running egg_info creating lltm_cpp.egg-info writing lltm_cpp.egg-info/PKG-INFO writing dependency_links to lltm_cpp.egg-info/dependency_links.txt writing top-level names to lltm_cpp.egg-info/top_level.txt writing manifest file 'lltm_cpp.egg-info/SOURCES.txt' reading manifest file 'lltm_cpp.egg-info/SOURCES.txt' writing manifest file 'lltm_cpp.egg-info/SOURCES.txt' installing library code to build/bdist.linux-x86_64/egg running install_lib running build_ext building 'lltm_cpp' extension creating build creating build/temp.linux-x86_64-3.7 gcc -pthread -B ~/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.7/site-packages/torch/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/TH -I~/local/miniconda/lib/python3.7/site-packages/torch/include/THC -I~/local/miniconda/include/python3.7m -c lltm.cpp -o build/temp.linux-x86_64-3.7/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm_cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11 cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ creating build/lib.linux-x86_64-3.7 g++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/lltm.o -o build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so creating build/bdist.linux-x86_64 creating build/bdist.linux-x86_64/egg copying build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg creating stub loader for lltm_cpp.cpython-37m-x86_64-linux-gnu.so byte-compiling build/bdist.linux-x86_64/egg/lltm_cpp.py to lltm_cpp.cpython-37.pyc creating build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt zip_safe flag not set; analyzing archive contents... __pycache__.lltm_cpp.cpython-37: module references __file__ creating 'dist/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it removing 'build/bdist.linux-x86_64/egg' (and everything under it) Processing lltm_cpp-0.0.0-py3.7-linux-x86_64.egg removing '~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' (and everything under it) creating ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg Extracting lltm_cpp-0.0.0-py3.7-linux-x86_64.egg to ~/local/miniconda/lib/python3.7/site-packages lltm-cpp 0.0.0 is already the active version in easy-install.pth Installed ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg Processing dependencies for lltm-cpp==0.0.0 Finished processing dependencies for lltm-cpp==0.0.0 关于编译器的小提示：由于 ABI 版本问题，用于构建 C++ 扩展的编译器必须与 PyTorch 编译器兼容。 实际上，这意味着您必须在 Linux 上使用 GCC 4.9 及更高版本。 对于 Ubuntu 16.04 和其他较新的 Linux 发行版，这应该已经是默认的编译器。 在 MacOS 上，您必须使用 clang（它没有任何 ABI 版本控制问题）。 在最坏的情况下，您可以使用编译器从源代码构建 PyTorch，然后使用相同的编译器构建扩展。 扩展程序构建完成后，您可以使用setup.py脚本中指定的名称，将其简单地导入 Python。 只需确保先import torch，因为这将解决动态链接器必须看到的一些符号： In [1]: import torch In [2]: import lltm_cpp In [3]: lltm_cpp.forward Out[3]: 如果我们在函数或模块上调用help()，则可以看到其签名与我们的 C++ 代码匹配： In[4] help(lltm_cpp.forward) forward(...) method of builtins.PyCapsule instance forward(arg0: torch::Tensor, arg1: torch::Tensor, arg2: torch::Tensor, arg3: torch::Tensor, arg4: torch::Tensor) -> List[torch::Tensor] LLTM forward 由于我们现在可以从 Python 调用 C++ 函数，因此可以将它们包装为torch.autograd.Function和torch.nn.Module以使其成为 PyTorch 的一等公民： import math import torch # Our module! import lltm_cpp class LLTMFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weights, bias, old_h, old_cell): outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell) new_h, new_cell = outputs[:2] variables = outputs[1:] + [weights] ctx.save_for_backward(*variables) return new_h, new_cell @staticmethod def backward(ctx, grad_h, grad_cell): outputs = lltm_cpp.backward( grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_variables) d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs return d_input, d_weights, d_bias, d_old_h, d_old_cell class LLTM(torch.nn.Module): def __init__(self, input_features, state_size): super(LLTM, self).__init__() self.input_features = input_features self.state_size = state_size self.weights = torch.nn.Parameter( torch.empty(3 * state_size, input_features + state_size)) self.bias = torch.nn.Parameter(torch.empty(3 * state_size)) self.reset_parameters() def reset_parameters(self): stdv = 1.0 / math.sqrt(self.state_size) for weight in self.parameters(): weight.data.uniform_(-stdv, +stdv) def forward(self, input, state): return LLTMFunction.apply(input, self.weights, self.bias, *state) 性能比较 现在我们已经可以使用并从 PyTorch 调用 C++ 代码了，我们可以运行一个小型基准测试，以查看通过用 C++ 重写操作所获得的性能。 我们将向前和向后运行 LLTM 几次，并测量持续时间： import time import torch batch_size = 16 input_features = 32 state_size = 128 X = torch.randn(batch_size, input_features) h = torch.randn(batch_size, state_size) C = torch.randn(batch_size, state_size) rnn = LLTM(input_features, state_size) forward = 0 backward = 0 for _ in range(100000): start = time.time() new_h, new_C = rnn(X, (h, C)) forward += time.time() - start start = time.time() (new_h.sum() + new_C.sum()).backward() backward += time.time() - start print('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5)) 如果我们使用本文开头用纯 Python 编写的原始 LLTM 运行此代码，则会得到以下数字（在我的机器上）： Forward: 506.480 us | Backward 444.694 us 以及我们的新 C++ 版本： Forward: 349.335 us | Backward 443.523 us 我们已经可以看到正向函数的明显提速（超过 30%）。 对于反向函数，可以看到加速，尽管不是很大。 我在上面编写的后向通行证没有特别优化，并且肯定可以改进。 而且，PyTorch 的自动微分引擎可以自动并行化计算图，可以整体上使用更高效的操作流程，并且也可以用 C++ 来实现，因此有望实现更快的速度。 不过，这是一个良好的开始。 GPU 设备上的性能 关于 PyTorch 的 ATen 后端的一个奇妙事实是，它抽象了您正在运行的计算设备。 这意味着我们为 CPU 编写的同一代码也可以在 GPU 上运行，并且各个操作将相应地分派到 GPU 优化的实现。 对于某些运算，例如矩阵乘法（例如mm或addmm），这是一个很大的胜利。 让我们看一下使用 CUDA 张量运行 C++ 代码所获得的性能。 无需更改实现，我们只需要将张量从 Python 放入 GPU 内存，即可在创建时添加device=cuda_device参数，或者在创建后使用.to(cuda_device)： import torch assert torch.cuda.is_available() cuda_device = torch.device(\"cuda\") # device object representing GPU batch_size = 16 input_features = 32 state_size = 128 # Note the device=cuda_device arguments here X = torch.randn(batch_size, input_features, device=cuda_device) h = torch.randn(batch_size, state_size, device=cuda_device) C = torch.randn(batch_size, state_size, device=cuda_device) rnn = LLTM(input_features, state_size).to(cuda_device) forward = 0 backward = 0 for _ in range(100000): start = time.time() new_h, new_C = rnn(X, (h, C)) torch.cuda.synchronize() forward += time.time() - start start = time.time() (new_h.sum() + new_C.sum()).backward() torch.cuda.synchronize() backward += time.time() - start print('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5)) 再次将普通的 PyTorch 代码与 C++ 版本（现在都在 CUDA 设备上运行）进行比较，我们再次看到了性能提升。 对于 Python/PyTorch： Forward: 187.719 us | Backward 410.815 us 和 C++/ATen： Forward: 149.802 us | Backward 393.458 us 与非 CUDA 代码相比，这可以大大提高整体速度。 但是，通过编写自定义 CUDA 内核，我们可以从 C++ 代码中获得更多性能，我们将很快深入其中。 在此之前，让我们讨论构建 C++ 扩展的另一种方法。 JIT 编译扩展 之前，我提到过有两种构建 C++ 扩展的方法：使用setuptools或即时（JIT）。 在介绍了前者之后，让我们详细介绍后者。 JIT 编译机制通过调用 PyTorch API 中称为torch.utils.cpp_extension.load()的简单函数，为您提供了一种动态编译和加载扩展的方式。 对于 LLTM，这看起来像这样简单： from torch.utils.cpp_extension import load lltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"]) 在此，我们为函数提供与setuptools相同的信息。 在后台，这将执行以下操作： 创建一个临时目录/tmp/torch_extensions/lltm， 将 Ninja 构建文件发送到该临时目录中， 将您的源文件编译到共享库中， 将此共享库导入为 Python 模块。 实际上，如果将verbose=True传递给cpp_extension.load()，则会通知您有关过程： Using /tmp/torch_extensions as PyTorch extensions root... Emitting ninja build file /tmp/torch_extensions/lltm_cpp/build.ninja... Building extension module lltm_cpp... Loading extension module lltm_cpp... 生成的 Python 模块将与setuptools生成的模块完全相同，但是消除了必须维护单独的setup.py构建文件的要求。 如果您的设置更为复杂，并且确实需要setuptools的全部功能，则可以编写自己的setup.py –但是在许多情况下，这种 JIT 技术就可以了。 第一次运行此行时，将需要一些时间，因为扩展程序是在后台编译的。 由于我们使用 Ninja 构建系统来构建您的源代码，因此重新编译是增量的，因此在您第二次运行 Python 模块时重新加载扩展程序非常快捷，而且如果您不更改扩展程序的源文件，开销也很低。 编写混合的 C++/CUDA 扩展 为了将实现真正提升到一个新的水平，我们可以使用自定义 CUDA 内核来手写前进和后退通道的一部分。 对于 LLTM，这具有特别有效的前景，因为按顺序有大量的逐点运算，这些运算都可以在单个 CUDA 内核中融合和并行化。 让我们看看如何编写这种 CUDA 内核，并使用此扩展机制将其与 PyTorch 集成。 编写 CUDA 扩展的一般策略是首先编写一个 C++ 文件，该文件定义将从 Python 调用的函数，然后使用pybind11将这些函数绑定到 Python。 此外，此文件还将声明在 CUDA（.cu）文件中定义的函数。 然后，C++ 函数将进行一些检查，并最终将其调用转发给 CUDA 函数。 在 CUDA 文件中，我们编写了实际的 CUDA 内核。 然后cpp_extension包将负责使用gcc等 C++ 编译器来编译 C++ 源代码，并使用 NVIDIA 的nvcc编译器来编译 CUDA 源。 这样可以确保每个编译器都照顾最了解要编译的文件。 最终，它们将被链接到一个共享库中，该库可从 Python 代码中获得。 我们将从 C++ 文件开始，我们将其称为lltm_cuda.cpp，例如： #include #include // CUDA forward declarations std::vector lltm_cuda_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell); std::vector lltm_cuda_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gate_weights, torch::Tensor weights); // C++ interface #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\") #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\") #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x) std::vector lltm_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { CHECK_INPUT(input); CHECK_INPUT(weights); CHECK_INPUT(bias); CHECK_INPUT(old_h); CHECK_INPUT(old_cell); return lltm_cuda_forward(input, weights, bias, old_h, old_cell); } std::vector lltm_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gate_weights, torch::Tensor weights) { CHECK_INPUT(grad_h); CHECK_INPUT(grad_cell); CHECK_INPUT(input_gate); CHECK_INPUT(output_gate); CHECK_INPUT(candidate_cell); CHECK_INPUT(X); CHECK_INPUT(gate_weights); CHECK_INPUT(weights); return lltm_cuda_backward( grad_h, grad_cell, new_cell, input_gate, output_gate, candidate_cell, X, gate_weights, weights); } PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(\"forward\", &lltm_forward, \"LLTM forward (CUDA)\"); m.def(\"backward\", &lltm_backward, \"LLTM backward (CUDA)\"); } 如您所见，它主要是样板文件，检查并转发到我们将在 CUDA 文件中定义的功能。 我们将此文件命名为lltm_cuda_kernel.cu（请注意.cu扩展名！）。 NVCC 可以合理地编译 C++ 11，因此我们仍然可以使用 ATen 和 C++ 标准库（但不能使用torch.h）。 请注意，setuptools无法处理具有相同名称但扩展名不同的文件，因此，如果您使用setup.py方法而不是 JIT 方法，则必须给 CUDA 文件指定一个与 C++ 文件不同的名称（对于 JIT 方法， lltm.cpp和lltm.cu可以正常工作）。 让我们看一下该文件的外观： #include #include #include #include template __device__ __forceinline__ scalar_t sigmoid(scalar_t z) { return 1.0 / (1.0 + exp(-z)); } 在这里，我们看到了我刚刚描述的标头，以及我们正在使用特定于 CUDA 的声明，例如__device__和__forceinline__以及类似exp的事实。 让我们继续一些我们需要的辅助功​​能： template __device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) { const auto s = sigmoid(z); return (1.0 - s) * s; } template __device__ __forceinline__ scalar_t d_tanh(scalar_t z) { const auto t = tanh(z); return 1 - (t * t); } template __device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) { return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0)); } template __device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) { const auto e = exp(z); const auto d_relu = z 现在，要真正实现一个函数，我们再次需要两件事：一个函数执行我们不想手工明确编写的操作并调用 CUDA 内核，然后是要加速的部分的实际 CUDA 内核。 。 对于正向传播，第一个函数应如下所示： std::vector lltm_cuda_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { auto X = torch::cat({old_h, input}, /*dim=*/1); auto gates = torch::addmm(bias, X, weights.transpose(0, 1)); const auto batch_size = old_cell.size(0); const auto state_size = old_cell.size(1); auto new_h = torch::zeros_like(old_cell); auto new_cell = torch::zeros_like(old_cell); auto input_gate = torch::zeros_like(old_cell); auto output_gate = torch::zeros_like(old_cell); auto candidate_cell = torch::zeros_like(old_cell); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&] { lltm_cuda_forward_kernel>>( gates.data(), old_cell.data(), new_h.data(), new_cell.data(), input_gate.data(), output_gate.data(), candidate_cell.data(), state_size); })); return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates}; } 这里的主要关注点是AT_DISPATCH_FLOATING_TYPES宏和内核启动（由>>指示）。 尽管 ATen 提取了我们处理过的张量的设备和数据类型，但张量在运行时仍将由具体设备上具体类型的内存支持。 因此，我们需要一种在运行时确定张量是什么类型，然后有选择地调用具有相应正确类型签名的函数的方法。 手动完成后，（在概念上）将如下所示： switch (tensor.type().scalarType()) { case torch::ScalarType::Double: return function(tensor.data()); case torch::ScalarType::Float: return function(tensor.data()); ... } AT_DISPATCH_FLOATING_TYPES的目的是为我们处理此调度。 它需要一个类型（在我们的例子中为gates.type()），一个名称（用于错误消息）和一个 lambda 函数。 在此 lambda 函数内部，类型别名scalar_t可用，并且定义为该上下文中张量实际上在运行时的类型。 这样，如果我们有一个模板函数（CUDA 内核将使用该模板函数），则可以使用此scalar_t别名实例化它，然后将调用正确的函数。 在这种情况下，我们还希望检索张量的数据指针作为scalar_t类型的指针。 如果您想分派所有类型而不只是浮点类型（Float和Double），则可以使用AT_DISPATCH_ALL_TYPES。 请注意，我们使用普通的 ATen 执行一些操作。 这些操作仍将在 GPU 上运行，但使用 ATen 的默认实现。 这是有道理的，因为 ATen 会针对矩阵乘法（例如addmm）或卷积使用高度优化的例程，而这将很难实现和改善。 至于内核启动本身，我们在这里指定每个 CUDA 块将具有 1024 个线程，并且将整个 GPU 网格分为所需的1 x 1024线程块，以便用每个组件一个线程填充矩阵。 例如，如果我们的状态大小为 2048，批量大小为 4，则我们将以每个 1024 个线程总共启动4 x 2 = 8块。 如果您以前从未听说过 CUDA 的“障碍”或“网格”，那么 CUDA 简介可能会有所帮助。 实际的 CUDA 内核非常简单（如果您曾经编程过 GPU）： template __global__ void lltm_cuda_forward_kernel( const scalar_t* __restrict__ gates, const scalar_t* __restrict__ old_cell, scalar_t* __restrict__ new_h, scalar_t* __restrict__ new_cell, scalar_t* __restrict__ input_gate, scalar_t* __restrict__ output_gate, scalar_t* __restrict__ candidate_cell, size_t state_size) { const int column = blockIdx.x * blockDim.x + threadIdx.x; const int index = blockIdx.y * state_size + column; const int gates_row = blockIdx.y * (state_size * 3); if (column 这里最有趣的是，我们能够为门矩阵中的每个单独的组件完全并行地计算所有这些逐点运算。 如果您想象必须用一个串行的百万个元素的for大型循环来执行此操作，那么您会明白为什么这样做会更快。 使用访问器 您可以在 CUDA 内核中看到，我们直接处理正确类型的指针。 实际上，直接在 cuda 内核内部使用高级类型不可知张量会非常低效。 但是，这是以易于使用和可读性为代价的，尤其是对于高维数据。 在我们的示例中，例如，我们知道连续的gates张量具有 3 个维度： 批量，batch_size的大小和3*state_size的步幅 3的行，大小和state_size的步幅 指数，state_size的大小和1的步幅 那么我们如何访问内核中的元素gates[n][row][column]？ 事实证明，您需要通过一些简单的算法就可以大步访问元素。 gates.data()[n`3`state_size + row*state_size + column] 除了冗长之外，此表达式还需要跨步才能明确知道，并因此在其参数内传递给内核函数。 您会看到，在内核函数接受具有不同大小的多个张量的情况下，您将得到很长的参数列表。 对我们来说幸运的是，ATen 提供了通过动态检查张量是维度的类型和数量而创建的访问器。 然后，访问器公开一个 API，可以有效地访问张量元素，而不必转换为单个指针： torch::Tensor foo = torch::rand({12, 12}); // assert foo is 2-dimensional and holds floats. auto foo_a = foo.accessor(); float trace = 0; for(int i = 0; i 访问器对象具有相对较高级别的接口，具有.size()和.stride()方法以及多维索引。 .accessor<>接口旨在在 CPU 张量上有效访问数据。 CUDA 张量的等效项是packed_accessor64<>和packed_accessor32<>，它们产生具有 64 位或 32 位整数索引的压缩访问器。 与访问器的根本区别在于，打包的访问器在其结构内部复制大小和跨度数据，而不是指向它。 它允许我们将其传递给 CUDA 内核函数并在其中使用其接口。 我们可以设计一个使用压缩访问器而不是指针的函数。 __global__ void lltm_cuda_forward_kernel( const torch::PackedTensorAccessor32 gates, const torch::PackedTensorAccessor32 old_cell, torch::PackedTensorAccessor32 new_h, torch::PackedTensorAccessor32 new_cell, torch::PackedTensorAccessor32 input_gate, torch::PackedTensorAccessor32 output_gate, torch::PackedTensorAccessor32 candidate_cell) 让我们分解一下这里使用的模板。 前两个参数scalar_t和2与常规访问器相同。 参数torch::RestrictPtrTraits指示必须使用__restrict__关键字。 另请注意，我们使用了PackedAccessor32变体，将变体和步幅存储在int32_t中。 这很重要，因为使用 64 位变体（PackedAccessor64）会使内核变慢。 函数声明变为 template __global__ void lltm_cuda_forward_kernel( const torch::PackedTensorAccessor32 gates, const torch::PackedTensorAccessor32 old_cell, torch::PackedTensorAccessor32 new_h, torch::PackedTensorAccessor32 new_cell, torch::PackedTensorAccessor32 input_gate, torch::PackedTensorAccessor32 output_gate, torch::PackedTensorAccessor32 candidate_cell) { //batch index const int n = blockIdx.y; // column index const int c = blockIdx.x * blockDim.x + threadIdx.x; if (c 该实现更具可读性！ 然后，通过在主机函数内使用.packed_accessor32<>方法创建压缩访问器来调用此函数。 std::vector lltm_cuda_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { auto X = torch::cat({old_h, input}, /*dim=*/1); auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1)); const auto batch_size = old_cell.size(0); const auto state_size = old_cell.size(1); auto gates = gate_weights.reshape({batch_size, 3, state_size}); auto new_h = torch::zeros_like(old_cell); auto new_cell = torch::zeros_like(old_cell); auto input_gate = torch::zeros_like(old_cell); auto output_gate = torch::zeros_like(old_cell); auto candidate_cell = torch::zeros_like(old_cell); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&] { lltm_cuda_forward_kernel>>( gates.packed_accessor32(), old_cell.packed_accessor32(), new_h.packed_accessor32(), new_cell.packed_accessor32(), input_gate.packed_accessor32(), output_gate.packed_accessor32(), candidate_cell.packed_accessor32()); })); return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates}; } 反向传播遵循相同的模式，在此我不再赘述： template __global__ void lltm_cuda_backward_kernel( torch::PackedTensorAccessor32 d_old_cell, torch::PackedTensorAccessor32 d_gates, const torch::PackedTensorAccessor32 grad_h, const torch::PackedTensorAccessor32 grad_cell, const torch::PackedTensorAccessor32 new_cell, const torch::PackedTensorAccessor32 input_gate, const torch::PackedTensorAccessor32 output_gate, const torch::PackedTensorAccessor32 candidate_cell, const torch::PackedTensorAccessor32 gate_weights) { //batch index const int n = blockIdx.y; // column index const int c = blockIdx.x * blockDim.x + threadIdx.x; if (c lltm_cuda_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gates, torch::Tensor weights) { auto d_old_cell = torch::zeros_like(new_cell); auto d_gates = torch::zeros_like(gates); const auto batch_size = new_cell.size(0); const auto state_size = new_cell.size(1); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(X.type(), \"lltm_forward_cuda\", ([&] { lltm_cuda_backward_kernel>>( d_old_cell.packed_accessor32(), d_gates.packed_accessor32(), grad_h.packed_accessor32(), grad_cell.packed_accessor32(), new_cell.packed_accessor32(), input_gate.packed_accessor32(), output_gate.packed_accessor32(), candidate_cell.packed_accessor32(), gates.packed_accessor32()); })); auto d_gate_weights = d_gates.reshape({batch_size, 3*state_size}); auto d_weights = d_gate_weights.t().mm(X); auto d_bias = d_gate_weights.sum(/*dim=*/0, /*keepdim=*/true); auto d_X = d_gate_weights.mm(weights); auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size); auto d_input = d_X.slice(/*dim=*/1, state_size); return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates}; } 将 C++/CUDA 操作与 PyTorch 集成 同样，将支持 CUDA 的操作与 PyTorch 集成非常简单。 如果要编写setup.py脚本，它可能看起来像这样： from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CUDAExtension setup( name='lltm', ext_modules=[ CUDAExtension('lltm_cuda', [ 'lltm_cuda.cpp', 'lltm_cuda_kernel.cu', ]) ], cmdclass={ 'build_ext': BuildExtension }) 现在，我们使用CUDAExtension()代替CppExtension()。 我们只需要指定.cu文件和.cpp文件即可–该库将为您解决所有麻烦。 JIT 机制甚至更简单： from torch.utils.cpp_extension import load lltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu']) 性能比较 我们的希望是，将我们的代码的逐点操作与 CUDA 并行化和融合，将改善 LLTM 的性能。 让我们看看这是否成立。 我们可以运行前面列出的代码来运行基准测试。 我们之前最快的版本是基于 CUDA 的 C++ 代码： Forward: 149.802 us | Backward 393.458 us 现在使用我们的自定义 CUDA 内核： Forward: 129.431 us | Backward 304.641 us 更多性能提升！ 总结 现在，您应该对 PyTorch 的 C++ 扩展机制有了一个很好的了解，并有使用它们的动机。 您可以在此处找到本说明中显示的代码示例。 如有疑问，请使用论坛。 另外，请务必查看我们的常见问题解答，以防遇到任何问题。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"46.html":{"url":"46.html","title":"使用自定义 C++ 运算符扩展 TorchScript","keywords":"","body":"使用自定义 C++ 运算符扩展 TorchScript 原文：https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html PyTorch 1.0 版本向 PyTorch 引入了一种新的编程模型，称为 TorchScript 。 TorchScript 是 Python 编程语言的子集，可以通过 TorchScript 编译器进行解析，编译和优化。 此外，已编译的 TorchScript 模型可以选择序列化为磁盘文件格式，然后可以从纯 C++（以及 Python）加载并运行该文件格式以进行推理。 TorchScript 支持torch包提供的大量操作子集，使您可以纯粹表示为 PyTorch 的“标准库”中的一系列张量操作来表示多种复杂模型。 但是，有时您可能需要使用自定义 C++ 或 CUDA 函数扩展 TorchScript。 虽然我们建议您仅在无法（简单有效地）将您的想法表达为简单的 Python 函数时才诉诸该选项，但我们确实提供了一个非常友好且简单的接口，用于使用 ATen 定义自定义 C++ 和 CUDA 内核。 ，PyTorch 的高性能 C++ 张量库。 绑定到 TorchScript 后，您可以将这些自定义内核（或“操作”）嵌入到 TorchScript 模型中，并以 Python 或直接以 C++ 的序列化形式执行它们。 以下段落提供了一个编写 TorchScript 自定义操作以调用 OpenCV （使用 C++ 编写的计算机视觉库）的示例。 我们将讨论如何在 C++ 中使用张量，如何有效地将它们转换为第三方张量格式（在这种情况下为 OpenCV Mat），如何在 TorchScript 运行时中注册您的运算符，以及最后如何编译该运算符并在 Python 和 C++ 中使用它。 在 C++ 中实现自定义运算符 在本教程中，我们将公开warpPerspective函数，该函数将透视转换应用于图像，从 OpenCV 到 TorchScript 作为自定义运算符。 第一步是用 C++ 编写自定义运算符的实现。 让我们将此实现的文件称为op.cpp，并使其如下所示： torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) { // BEGIN image_mat cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data_ptr()); // END image_mat // BEGIN warp_mat cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data_ptr()); // END warp_mat // BEGIN output_mat cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8}); // END output_mat // BEGIN output_tensor torch::Tensor output = torch::from_blob(output_mat.ptr(), /*sizes=*/{8, 8}); return output.clone(); // END output_tensor } 该运算符的代码很短。 在文件顶部，我们包含 OpenCV 标头文件opencv2/opencv.hpp和torch/script.h标头，该标头暴露了 PyTorch C++ API 中所有需要编写自定义 TorchScript 运算符的必要特性。 我们的函数warp_perspective有两个参数：输入image和我们希望应用于图像的warp变换矩阵。 这些输入的类型是torch::Tensor，这是 C++ 中 PyTorch 的张量类型（也是 Python 中所有张量的基础类型）。 我们的warp_perspective函数的返回类型也将是torch::Tensor。 小费 有关 ATen 的更多信息，请参见本说明，ATen 是为 PyTorch 提供Tensor类的库。 此外，本教程描述了如何在 C++ 中分配和初始化新的张量对象（此运算符不需要）。 注意 TorchScript 编译器了解固定数量的类型。 只有这些类型可以用作自定义运算符的参数。 当前这些类型是：torch::Tensor，torch::Scalar，double，int64_t和这些类型的std::vector。 请注意，只支持double而不是float，只支持int64_t而不是其他整数类型，例如int，short或long。 在函数内部，我们要做的第一件事就是将 PyTorch 张量转换为 OpenCV 矩阵，因为 OpenCV 的warpPerspective期望cv::Mat对象作为输入。 幸运的是，有一种方法可以执行它，而无需复制任何数据。 在前几行中 cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data_ptr()); 我们正在调用 OpenCV Mat类的构造器，将张量转换为Mat对象。 我们向其传递原始image张量的行数和列数，数据类型（在此示例中，我们将其固定为float32），最后是指向基础数据的原始指针– float*。 Mat类的此构造器的特殊之处在于它不会复制输入数据。 取而代之的是，它将简单地引用此存储器来执行Mat上的所有操作。 如果在image_mat上执行原地操作，这将反映在原始image张量中（反之亦然）。 即使我们实际上将数据存储在 PyTorch 张量中，这也使我们能够使用库的本机矩阵类型调用后续的 OpenCV 例程。 我们重复此过程将warp PyTorch 张量转换为warp_mat OpenCV 矩阵： cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data_ptr()); 接下来，我们准备调用我们渴望在 TorchScript 中使用的 OpenCV 函数：warpPerspective。 为此，我们将image_mat和warp_mat矩阵以及称为output_mat的空输出矩阵传递给 OpenCV 函数。 我们还指定了我们希望输出矩阵（图像）为dsize的大小。 对于此示例，它被硬编码为8 x 8： cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8}); 我们的自定义运算符实现的最后一步是将output_mat转换回 PyTorch 张量，以便我们可以在 PyTorch 中进一步使用它。 这与我们先前在另一个方向进行转换的操作极为相似。 在这种情况下，PyTorch 提供了一种torch::from_blob方法。 在这种情况下， blob 的意思是指向我们要解释为 PyTorch 张量的不透明平面指针。 对torch::from_blob的调用如下所示： torch::Tensor output = torch::from_blob(output_mat.ptr(), /*sizes=*/{8, 8}); return output.clone(); 我们在 OpenCV Mat类上使用.ptr()方法来获取指向基础数据的原始指针（就像之前的 PyTorch 张量的.data_ptr()一样）。 我们还指定了张量的输出形状，我们将其硬编码为8 x 8。 然后torch::from_blob的输出是torch::Tensor，指向 OpenCV 矩阵拥有的内存。 从我们的运算符实现返回此张量之前，我们必须在张量上调用.clone()以执行基础数据的存储副本。 这样做的原因是torch::from_blob返回了一个不拥有其数据的张量。 那时，数据仍归 OpenCV 矩阵所有。 但是，此 OpenCV 矩阵将超出范围，并在函数末尾重新分配。 如果我们按原样返回output张量，那么当我们在函数外部使用它时，它将指向无效的内存。 调用.clone()会返回一个新张量，其中包含新张量自己拥有的原始数据的副本。 因此，返回外部世界是安全的。 使用 TorchScript 注册自定义运算符 现在，已经在 C++ 中实现了自定义运算符，我们需要在 T​​orchScript 运行时和编译器中将其注册。 这将使 TorchScript 编译器可以在 TorchScript 代码中解析对我们自定义运算符的引用。 如果您曾经使用过pybind11库，则我们的注册语法非常类似于pybind11语法。 要注册一个函数，我们编写： TORCH_LIBRARY(my_ops, m) { m.def(\"warp_perspective\", warp_perspective); } 在op.cpp文件顶层的某个位置。 TORCH_LIBRARY宏创建一个在程序启动时将被调用的函数。 库的名称（my_ops）作为第一个参数给出（不应用引号引起来）。 第二个参数（m）定义了torch::Library类型的变量，该变量是注册运算符的主要接口。 方法Library::def实际上创建了一个名为warp_perspective的运算符，将其同时暴露给 Python 和 TorchScript。 您可以通过多次调用def来定义任意数量的运算符。 在后台，def函数实际上正在做大量工作：它正在使用模板元编程来检查函数的类型签名，并将其转换为可在 TorchScript 的类型系统中指定操作符类型的操作符架构。 构建自定义运算符 现在，我们已经用 C++ 实现了自定义运算符并编写了其注册代码，是时候将该运算符构建到一个（共享的）库中了，可以将其加载到 Python 中进行研究和实验，或者加载到 C++ 中以在非 Python 中进行推理。 环境。 有多种方法可以使用纯 CMake 或setuptools之类的 Python 替代方法来构建我们的运算符。 为简洁起见，以下段落仅讨论 CMake 方法。 本教程的附录将深入探讨其他替代方法。 环境设置 我们需要安装 PyTorch 和 OpenCV。 实现这两者的最简单，最独立于平台的方法是通过 Conda： conda install -c pytorch pytorch conda install opencv 将 CMake 用于构建 为了使用 CMake 构建系统将自定义运算符构建到共享库中，我们需要编写一个简短的CMakeLists.txt文件并将其与我们先前的op.cpp文件放置在一起。 为此，让我们就一个看起来像这样的目录结构达成一致： warp-perspective/ op.cpp CMakeLists.txt 我们的CMakeLists.txt文件的内容应为以下内容： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(warp_perspective) find_package(Torch REQUIRED) find_package(OpenCV REQUIRED) # Define our library target add_library(warp_perspective SHARED op.cpp) # Enable C++14 target_compile_features(warp_perspective PRIVATE cxx_std_14) # Link against LibTorch target_link_libraries(warp_perspective \"${TORCH_LIBRARIES}\") # Link against OpenCV target_link_libraries(warp_perspective opencv_core opencv_imgproc) 现在要构建我们的运算符，我们可以从warp_perspective文件夹中运行以下命令： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /warp_perspective/build $ make -j Scanning dependencies of target warp_perspective [ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o [100%] Linking CXX shared library libwarp_perspective.so [100%] Built target warp_perspective 这会将libwarp_perspective.so共享库文件放置在build文件夹中。 在上面的cmake命令中，我们使用帮助程序变量torch.utils.cmake_prefix_path方便地告诉我们 PyTorch 安装的 cmake 文件在哪里。 我们将在下面进一步探讨如何使用和调用我们的运算符，但为了早日获得成功，我们可以尝试在 Python 中运行以下代码： import torch torch.ops.load_library(\"build/libwarp_perspective.so\") print(torch.ops.my_ops.warp_perspective) 如果一切顺利，则应打印如下内容： 这是我们稍后将用来调用自定义运算符的 Python 函数。 在 Python 中使用 TorchScript 自定义运算符 将我们的自定义运算符构建到共享库后，我们就可以在 Python 的 TorchScript 模型中使用此运算符了。 这有两个部分：首先将运算符加载到 Python 中，其次在 TorchScript 代码中使用运算符。 您已经了解了如何将运算符导入 Python：torch.ops.load_library()。 此函数采用包含自定义运算符的共享库的路径，并将其加载到当前进程中。 加载共享库也将执行TORCH_LIBRARY块。 这将在 TorchScript 编译器中注册我们的自定义运算符，并允许我们在 TorchScript 代码中使用该运算符。 您可以将已加载的运算符称为torch.ops..，其中是运算符名称的名称空间部分，而是运算符的函数名称。 对于我们上面编写的运算符，名称空间为my_ops，函数名称为warp_perspective，这意味着我们的运算符可以作为torch.ops.my_ops.warp_perspective使用。 虽然可以在脚本化或跟踪的 TorchScript 模块中使用此函数，但我们也可以仅在急切的 PyTorch 中使用它，并将其传递给常规 PyTorch 张量： import torch torch.ops.load_library(\"build/libwarp_perspective.so\") print(torch.ops.my_ops.warp_perspective(torch.randn(32, 32), torch.rand(3, 3))) 生产： tensor([[0.0000, 0.3218, 0.4611, ..., 0.4636, 0.4636, 0.4636], [0.3746, 0.0978, 0.5005, ..., 0.4636, 0.4636, 0.4636], [0.3245, 0.0169, 0.0000, ..., 0.4458, 0.4458, 0.4458], ..., [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000], [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000], [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000]]) 注意 幕后发生的事情是，您第一次使用 Python 访问torch.ops.namespace.function时，TorchScript 编译器（在 C++ 领域）将查看是否已注册函数namespace::function，如果已注册，则将 Python 句柄返回给该函数， 我们随后可以使用它从 Python 调用我们的 C++ 运算符实现。 这是 TorchScript 自定义运算符和 C++ 扩展之间的一个值得注意的区别：C++ 扩展是使用pybind11手动绑定的，而 TorchScript 自定义操作则是由 PyTorch 自己动态绑定的。pybind11在绑定到 Python 的类型和类方面为您提供了更大的灵活性，因此建议将其用于纯粹渴望的代码，但 TorchScript 操作不支持它。 从这里开始，您可以在脚本或跟踪代码中使用自定义运算符，就像torch包中的其他函数一样。 实际上，诸如torch.matmul之类的“标准库”函数在很大程度上与自定义运算符使用相同的注册路径，这使得自定义运算符在 TorchScript 中的使用方式和位置方面真正成为一流公民。 （但是，区别之一是标准库函数具有自定义的 Python 自变量解析逻辑，与torch.ops自变量解析不同。） 在跟踪中使用自定义运算符 首先，将我们的运算符嵌入到跟踪函数中。 回想一下，为了进行跟踪，我们从一些原始的 Pytorch 代码开始： def compute(x, y, z): return x.matmul(y) + torch.relu(z) 然后在其上调用torch.jit.trace。 我们进一步传递torch.jit.trace一些示例输入，它将输入到我们的实现中，以记录输入流过其中时发生的操作顺序。 这样的结果实际上是渴望的 PyTorch 程序的“冻结”版本，TorchScript 编译器可以对其进行进一步的分析，优化和序列化： inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(4, 5)] trace = torch.jit.trace(compute, inputs) print(trace.graph) 生产： graph(%x : Float(4:8, 8:1), %y : Float(8:5, 5:1), %z : Float(4:5, 5:1)): %3 : Float(4:5, 5:1) = aten::matmul(%x, %y) # test.py:10:0 %4 : Float(4:5, 5:1) = aten::relu(%z) # test.py:10:0 %5 : int = prim::Constant[value=1]() # test.py:10:0 %6 : Float(4:5, 5:1) = aten::add(%3, %4, %5) # test.py:10:0 return (%6) 现在，令人兴奋的启示是，我们可以简单地将自定义运算符放到 PyTorch 跟踪中，就好像它是torch.relu或任何其他torch函数一样： def compute(x, y, z): x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + torch.relu(z) 然后像以前一样跟踪它： inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(8, 5)] trace = torch.jit.trace(compute, inputs) print(trace.graph) 生产： graph(%x.1 : Float(4:8, 8:1), %y : Float(8:5, 5:1), %z : Float(8:5, 5:1)): %3 : int = prim::Constant[value=3]() # test.py:25:0 %4 : int = prim::Constant[value=6]() # test.py:25:0 %5 : int = prim::Constant[value=0]() # test.py:25:0 %6 : Device = prim::Constant[value=\"cpu\"]() # test.py:25:0 %7 : bool = prim::Constant[value=0]() # test.py:25:0 %8 : Float(3:3, 3:1) = aten::eye(%3, %4, %5, %6, %7) # test.py:25:0 %x : Float(8:8, 8:1) = my_ops::warp_perspective(%x.1, %8) # test.py:25:0 %10 : Float(8:5, 5:1) = aten::matmul(%x, %y) # test.py:26:0 %11 : Float(8:5, 5:1) = aten::relu(%z) # test.py:26:0 %12 : int = prim::Constant[value=1]() # test.py:26:0 %13 : Float(8:5, 5:1) = aten::add(%10, %11, %12) # test.py:26:0 return (%13) 如此简单地将 TorchScript 自定义操作集成到跟踪的 PyTorch 代码中！ 将自定义运算符与脚本一起使用 除了跟踪之外，获得 PyTorch 程序的 TorchScript 表示形式的另一种方法是直接在 TorchScript 中编写代码。 TorchScript 在很大程度上是 Python 语言的子集，它具有一些限制，使 TorchScript 编译器更容易推理程序。 您可以使用@torch.jit.script标记自由函数，使用@torch.jit.script_method标记类中的方法（也必须从torch.jit.ScriptModule派生），将常规 PyTorch 代码转换为 TorchScript。 有关 TorchScript 注解的更多详细信息，请参见此处。 使用 TorchScript 而不是跟踪的一个特殊原因是，跟踪无法捕获 PyTorch 代码中的控制流。 因此，让我们考虑使用控制流的此函数： def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 return x.matmul(y) + z 要将此函数从原始 PyTorch 转换为 TorchScript，我们用@torch.jit.script对其进行注解： @torch.jit.script def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 return x.matmul(y) + z 这将及时将compute函数编译成图形表示形式，我们可以在compute.graph属性中进行检查： >>> compute.graph graph(%x : Dynamic %y : Dynamic) { %14 : int = prim::Constant[value=1]() %2 : int = prim::Constant[value=0]() %7 : int = prim::Constant[value=42]() %z.1 : int = prim::Constant[value=5]() %z.2 : int = prim::Constant[value=10]() %4 : Dynamic = aten::select(%x, %2, %2) %6 : Dynamic = aten::select(%4, %2, %2) %8 : Dynamic = aten::eq(%6, %7) %9 : bool = prim::TensorToBool(%8) %z : int = prim::If(%9) block0() { -> (%z.1) } block1() { -> (%z.2) } %13 : Dynamic = aten::matmul(%x, %y) %15 : Dynamic = aten::add(%13, %z, %14) return (%15); } 现在，就像以前一样，我们可以像脚本代码中的任何其他函数一样使用自定义运算符： torch.ops.load_library(\"libwarp_perspective.so\") @torch.jit.script def compute(x, y): if bool(x[0] == 42): z = 5 else: z = 10 x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + z 当 TorchScript 编译器看到对torch.ops.my_ops.warp_perspective的引用时，它将找到我们通过 C++ 中的TORCH_LIBRARY函数注册的实现，并将其编译为图形表示形式： >>> compute.graph graph(%x.1 : Dynamic %y : Dynamic) { %20 : int = prim::Constant[value=1]() %16 : int[] = prim::Constant[value=[0, -1]]() %14 : int = prim::Constant[value=6]() %2 : int = prim::Constant[value=0]() %7 : int = prim::Constant[value=42]() %z.1 : int = prim::Constant[value=5]() %z.2 : int = prim::Constant[value=10]() %13 : int = prim::Constant[value=3]() %4 : Dynamic = aten::select(%x.1, %2, %2) %6 : Dynamic = aten::select(%4, %2, %2) %8 : Dynamic = aten::eq(%6, %7) %9 : bool = prim::TensorToBool(%8) %z : int = prim::If(%9) block0() { -> (%z.1) } block1() { -> (%z.2) } %17 : Dynamic = aten::eye(%13, %14, %2, %16) %x : Dynamic = my_ops::warp_perspective(%x.1, %17) %19 : Dynamic = aten::matmul(%x, %y) %21 : Dynamic = aten::add(%19, %z, %20) return (%21); } 请特别注意图末尾对my_ops::warp_perspective的引用。 注意 TorchScript 图形表示仍可能更改。 不要依靠它看起来像这样。 在 Python 中使用自定义运算符时，确实如此。 简而言之，您可以使用torch.ops.load_library导入包含您的运算符的库，并像其他任何torch运算符一样，从跟踪或编写脚本的 TorchScript 代码中调用自定义操作。 在 C++ 中使用 TorchScript 自定义运算符 TorchScript 的一项有用功能是能够将模型序列化到磁盘文件中。 该文件可以通过有线方式发送，存储在文件系统中，或者更重要的是，可以动态反序列化和执行，而无需保留原始源代码。 这在 Python 中是可能的，但在 C++ 中也是可能的。 为此，PyTorch 提供了纯 C++ API，用于反序列化以及执行 TorchScript 模型。 如果您还没有的话，请阅读在 C++ 中加载和运行序列化 TorchScript 模型的教程，接下来的几段将基于该教程构建。 简而言之，即使从文件反序列化并以 C++ 运行，也可以像常规torch运算符一样执行自定义运算符。 唯一的要求是将我们先前构建的自定义运算符共享库与执行模型的 C++ 应用链接。 在 Python 中，只需调用torch.ops.load_library即可。 在 C++ 中，您需要在使用的任何构建系统中将共享库与主应用链接。 下面的示例将使用 CMake 展示这一点。 注意 从技术上讲，您还可以在运行时将共享库动态加载到 C++ 应用中，就像在 Python 中一样。 在 Linux 上，可以使用dlopen来执行此操作。 在其他平台上也存在等效项。 在上面链接的 C++ 执行教程的基础上，让我们从一个最小的 C++ 应用开始，在与自定义运算符不同的文件夹中的main.cpp文件中，该文件加载并执行序列化的 TorchScript 模型： #include // One-stop header. #include #include int main(int argc, const char* argv[]) { if (argc != 2) { std::cerr \\n\"; return -1; } // Deserialize the ScriptModule from a file using torch::jit::load(). std::shared_ptr module = torch::jit::load(argv[1]); std::vector inputs; inputs.push_back(torch::randn({4, 8})); inputs.push_back(torch::randn({8, 5})); torch::Tensor output = module->forward(std::move(inputs)).toTensor(); std::cout 以及一个小的CMakeLists.txt文件： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(example_app) find_package(Torch REQUIRED) add_executable(example_app main.cpp) target_link_libraries(example_app \"${TORCH_LIBRARIES}\") target_compile_features(example_app PRIVATE cxx_range_for) 在这一点上，我们应该能够构建应用： 并在尚未通过模型的情况下运行它： 接下来，让我们序列化我们之前编写的使用自定义运算符的脚本函数： torch.ops.load_library(\"libwarp_perspective.so\") @torch.jit.script def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + z compute.save(\"example.pt\") 最后一行将脚本函数序列化为一个名为example.pt的文件。 如果我们随后将此序列化模型传递给 C++ 应用，则可以立即运行它： 或者可能不是。 也许还没有。 当然！ 我们尚未将自定义运算符库与我们的应用链接。 让我们立即执行此操作，并正确进行操作，让我们稍微更新一下文件组织，如下所示： example_app/ CMakeLists.txt main.cpp warp_perspective/ CMakeLists.txt op.cpp 这将允许我们将warp_perspective库 CMake 目标添加为应用目标的子目录。 example_app文件夹中的顶级CMakeLists.txt应该如下所示： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(example_app) find_package(Torch REQUIRED) add_subdirectory(warp_perspective) add_executable(example_app main.cpp) target_link_libraries(example_app \"${TORCH_LIBRARIES}\") target_link_libraries(example_app -Wl,--no-as-needed warp_perspective) target_compile_features(example_app PRIVATE cxx_range_for) 基本的 CMake 配置与以前非常相似，只是我们将warp_perspective CMake 构建添加为子目录。 一旦其 CMake 代码运行，我们将example_app应用与warp_perspective共享库链接。 注意 上面的示例中嵌入了一个关键细节：warp_perspective链接行的-Wl,--no-as-needed前缀。 这是必需的，因为我们实际上不会在应用代码中从warp_perspective共享库中调用任何函数。 我们只需要运行TORCH_LIBRARY函数。 麻烦的是，这使链接器感到困惑，并使其认为可以完全跳过与库的链接。 在 Linux 上，-Wl,--no-as-needed标志会强制链接发生（注意：此标志特定于 Linux！）。 还有其他解决方法。 最简单的方法是在运算符库中定义一些函数，您需要从主应用中调用该函数。 这可能就像在某个标头中声明的函数void init();一样简单，然后在运算符库中将其定义为void init() { }。 在主应用中调用此init()函数会给链接器以印象，这是一个值得链接的库。 不幸的是，这超出了我们的控制范围，我们宁愿让您知道其原因和简单的解决方法，而不是让您将一些不透明的宏放入代码中。 现在，由于我们现在在顶层找到了Torch包，因此warp_perspective子目录中的CMakeLists.txt文件可以缩短一些。 它看起来应该像这样： find_package(OpenCV REQUIRED) add_library(warp_perspective SHARED op.cpp) target_compile_features(warp_perspective PRIVATE cxx_range_for) target_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\") target_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo) 让我们重新构建示例应用，该应用还将与自定义运算符库链接。 在顶层example_app目录中： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /warp_perspective/example_app/build $ make -j Scanning dependencies of target warp_perspective [ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o [ 50%] Linking CXX shared library libwarp_perspective.so [ 50%] Built target warp_perspective Scanning dependencies of target example_app [ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o [100%] Linking CXX executable example_app [100%] Built target example_app 如果现在运行example_app二进制文件并将其交给序列化模型，我们应该得出一个圆满的结局： $ ./example_app example.pt 11.4125 5.8262 9.5345 8.6111 12.3997 7.4683 13.5969 9.0850 11.0698 9.4008 7.4597 15.0926 12.5727 8.9319 9.0666 9.4834 11.1747 9.0162 10.9521 8.6269 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 [ Variable[CPUFloatType]{8,5} ] 成功！ 您现在可以推断了。 总结 本教程向您介绍了如何在 C++ 中实现自定义 TorchScript 运算符，如何将其构建到共享库中，如何在 Python 中使用它来定义 TorchScript 模型以及如何将其加载到 C++ 应用中以进行推理工作负载。 现在，您可以使用与第三方 C++ 库进行接口的 C++ 运算符扩展 TorchScript 模型，编写自定义的高性能 CUDA 内核，或实现任何其他需要 Python，TorchScript 和 C++ 之间的界线才能平稳融合的用例。 与往常一样，如果您遇到任何问题或疑问，可以使用我们的论坛或 GitHub ISSUE 进行联系。 另外，我们的常见问题解答（FAQ）页面可能包含有用的信息。 附录 A：建立自定义运算符的更多方法 “构建自定义运算符”一节介绍了如何使用 CMake 将自定义运算符构建到共享库中。 本附录概述了两种进一步的编译方法。 他们俩都使用 Python 作为编译过程的“驱动程序”或“接口”。 此外，两者都重用了现有基础结构。 PyTorch 提供了 C++ 扩展，它们依赖于pybind11用于将函数从 C++ “显式”绑定到 Python。 第一种方法是使用 C++ 扩展程序的方便的即时（JIT）编译接口在您首次运行 PyTorch 脚本时在后台编译代码。 第二种方法依赖于古老的setuptools包，并涉及编写单独的setup.py文件。 这样可以进行更高级的配置，并与其他基于setuptools的项目集成。 我们将在下面详细探讨这两种方法。 使用 JIT 编译的构建 PyTorch C++ 扩展工具包提供的 JIT 编译功能可将自定义运算符的编译直接嵌入到您的 Python 代码中，例如在训练脚本的顶部。 注意 这里的“ JIT 编译”与 TorchScript 编译器中用于优化程序的 JIT 编译无关。 这只是意味着您的自定义运算符 C++ 代码将在您首次导入时在系统/tmp目录下的文件夹中编译，就像您自己事先对其进行编译一样。 此 JIT 编译功能有两种形式。 首先，您仍然将运算符实现保留在单独的文件（op.cpp）中，然后使用torch.utils.cpp_extension.load()编译扩展名。 通常，此函数将返回暴露您的 C++ 扩展的 Python 模块。 但是，由于我们没有将自定义运算符编译到其自己的 Python 模块中，因此我们只想编译一个普通的共享库。 幸运的是，torch.utils.cpp_extension.load()有一个参数is_python_module，可以将其设置为False，以表明我们仅对构建共享库感兴趣，而对 Python 模块不感兴趣。 然后torch.utils.cpp_extension.load()将会编译并将共享库也加载到当前进程中，就像torch.ops.load_library之前所做的那样： import torch.utils.cpp_extension torch.utils.cpp_extension.load( name=\"warp_perspective\", sources=[\"op.cpp\"], extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"], is_python_module=False, verbose=True ) print(torch.ops.my_ops.warp_perspective) 这应该大致打印： JIT 编译的第二种形式使您可以将自定义 TorchScript 运算符的源代码作为字符串传递。 为此，请使用torch.utils.cpp_extension.load_inline： import torch import torch.utils.cpp_extension op_source = \"\"\" #include #include torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) { cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data()); cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data()); cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64}); torch::Tensor output = torch::from_blob(output_mat.ptr(), /*sizes=*/{64, 64}); return output.clone(); } TORCH_LIBRARY(my_ops, m) { m.def(\"warp_perspective\", &warp_perspective); } \"\"\" torch.utils.cpp_extension.load_inline( name=\"warp_perspective\", cpp_sources=op_source, extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"], is_python_module=False, verbose=True, ) print(torch.ops.my_ops.warp_perspective) 自然，最佳实践是仅在源代码相当短的情况下才使用torch.utils.cpp_extension.load_inline。 请注意，如果您在 Jupyter 笔记本中使用此功能，则不应多次执行单元格的注册，因为每次执行都会注册一个新库并重新注册自定义运算符。 如果需要重新执行它，请事先重新启动笔记本的 Python 内核。 使用setuptools构建 从 Python 专门构建自定义运算符的第二种方法是使用setuptools。 这样做的好处是setuptools具有相当强大而广泛的接口，可以用来构建用 C++ 编写的 Python 模块。 但是，由于setuptools实际上是用于构建 Python 模块而不是普通的共享库（它们没有 Python 期望从模块中获得的必要入口点），因此这种方法可能有点古怪。 也就是说，您需要的是一个setup.py文件来代替CMakeLists.txt，该文件看起来像这样： from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CppExtension setup( name=\"warp_perspective\", ext_modules=[ CppExtension( \"warp_perspective\", [\"example_app/warp_perspective/op.cpp\"], libraries=[\"opencv_core\", \"opencv_imgproc\"], ) ], cmdclass={\"build_ext\": BuildExtension.with_options(no_python_abi_suffix=True)}, ) 请注意，我们在底部的BuildExtension中启用了no_python_abi_suffix选项。 这指示setuptools在产生的共享库的名称中省略任何特定于 Python-3 的 ABI 后缀。 否则，例如在 Python 3.7 上，该库可能被称为warp_perspective.cpython-37m-x86_64-linux-gnu.so，其中cpython-37m-x86_64-linux-gnu是 ABI 标签，但我们确实只是希望将其称为warp_perspective.so 如果现在从setup.py所在的文件夹中的终端中运行python setup.py build develop，我们应该会看到类似以下内容的内容： $ python setup.py build develop running build running build_ext building 'warp_perspective' extension creating build creating build/temp.linux-x86_64-3.7 gcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11 cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ creating build/lib.linux-x86_64-3.7 g++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so running develop running egg_info creating warp_perspective.egg-info writing warp_perspective.egg-info/PKG-INFO writing dependency_links to warp_perspective.egg-info/dependency_links.txt writing top-level names to warp_perspective.egg-info/top_level.txt writing manifest file 'warp_perspective.egg-info/SOURCES.txt' reading manifest file 'warp_perspective.egg-info/SOURCES.txt' writing manifest file 'warp_perspective.egg-info/SOURCES.txt' running build_ext copying build/lib.linux-x86_64-3.7/warp_perspective.so -> Creating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .) Adding warp-perspective 0.0.0 to easy-install.pth file Installed /warp_perspective Processing dependencies for warp-perspective==0.0.0 Finished processing dependencies for warp-perspective==0.0.0 这将产生一个名为warp_perspective.so的共享库，我们可以像之前那样将其传递给torch.ops.load_library，以使我们的运算符对 TorchScript 可见： >>> import torch >>> torch.ops.load_library(\"warp_perspective.so\") >>> print(torch.ops.custom.warp_perspective) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"47.html":{"url":"47.html","title":"使用自定义 C++ 类扩展 TorchScript","keywords":"","body":"使用自定义 C++ 类扩展 TorchScript 原文：https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html 本教程是自定义运算符教程的后续教程，并介绍了我们为将 C++ 类同时绑定到 TorchScript 和 Python 而构建的 API。 该 API 与pybind11非常相似，如果您熟悉该系统，则大多数概念都将转移过来。 用 C++ 实现和绑定类 在本教程中，我们将定义一个简单的 C++ 类，该类在成员变量中保持持久状态。 // This header is all you need to do the C++ portions of this // tutorial #include // This header is what defines the custom class registration // behavior specifically. script.h already includes this, but // we include it here so you know it exists in case you want // to look at the API or implementation. #include #include #include template struct MyStackClass : torch::CustomClassHolder { std::vector stack_; MyStackClass(std::vector init) : stack_(init.begin(), init.end()) {} void push(T x) { stack_.push_back(x); } T pop() { auto val = stack_.back(); stack_.pop_back(); return val; } c10::intrusive_ptr clone() const { return c10::make_intrusive(stack_); } void merge(const c10::intrusive_ptr& c) { for (auto& elem : c->stack_) { push(elem); } } }; 有几件事要注意： torch/custom_class.h是您需要使用自定义类扩展 TorchScript 的标头。 注意，无论何时使用自定义类的实例，我们都通过c10::intrusive_ptr<>的实例来实现。 可以将intrusive_ptr视为类似于std::shared_ptr的智能指针，但是引用计数直接存储在对象中，而不是单独的元数据块（如std::shared_ptr中所做的。torch::Tensor内部使用相同的指针类型 ;和自定义类也必须使用此指针类型，以便我们可以一致地管理不同的对象类型。 注意的第二件事是用户定义的类必须继承torch::CustomClassHolder。 这样可以确保自定义类具有存储引用计数的空间。 现在让我们看一下如何使该类对 TorchScript 可见，该过程称为绑定该类： // Notice a few things: // - We pass the class to be registered as a template parameter to // `torch::class_`. In this instance, we've passed the // specialization of the MyStackClass class ``MyStackClass``. // In general, you cannot register a non-specialized template // class. For non-templated classes, you can just pass the // class name directly as the template parameter. // - The arguments passed to the constructor make up the \"qualified name\" // of the class. In this case, the registered class will appear in // Python and C++ as `torch.classes.my_classes.MyStackClass`. We call // the first argument the \"namespace\" and the second argument the // actual class name. TORCH_LIBRARY(my_classes, m) { m.class_>(\"MyStackClass\") // The following line registers the contructor of our MyStackClass // class that takes a single `std::vector` argument, // i.e. it exposes the C++ method `MyStackClass(std::vector init)`. // Currently, we do not support registering overloaded // constructors, so for now you can only `def()` one instance of // `torch::init`. .def(torch::init>()) // The next line registers a stateless (i.e. no captures) C++ lambda // function as a method. Note that a lambda function must take a // `c10::intrusive_ptr` (or some const/ref version of that) // as the first argument. Other arguments can be whatever you want. .def(\"top\", [](const c10::intrusive_ptr>& self) { return self->stack_.back(); }) // The following four lines expose methods of the MyStackClass // class as-is. `torch::class_` will automatically examine the // argument and return types of the passed-in method pointers and // expose these to Python and TorchScript accordingly. Finally, notice // that we must take the *address* of the fully-qualified method name, // i.e. use the unary `&` operator, due to C++ typing rules. .def(\"push\", &MyStackClass::push) .def(\"pop\", &MyStackClass::pop) .def(\"clone\", &MyStackClass::clone) .def(\"merge\", &MyStackClass::merge) ; } 使用 CMake 将示例构建为 C++ 项目 现在，我们将使用 CMake 构建系统来构建上述 C++ 代码。 首先，将到目前为止介绍的所有 C++ 代码放入class.cpp文件中。 然后，编写一个简单的CMakeLists.txt文件并将其放在同一目录中。 CMakeLists.txt应该是这样的： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(custom_class) find_package(Torch REQUIRED) # Define our library target add_library(custom_class SHARED class.cpp) set(CMAKE_CXX_STANDARD 14) # Link against LibTorch target_link_libraries(custom_class \"${TORCH_LIBRARIES}\") 另外，创建一个build目录。 您的文件树应如下所示： custom_class_project/ class.cpp CMakeLists.txt build/ 我们假设您已经按照上一教程中所述的相同方式设置了环境。 继续并调用cmake，然后进行构建项目： $ cd build $ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" .. -- The C compiler identification is GNU 7.3.1 -- The CXX compiler identification is GNU 7.3.1 -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++ -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /torchbind_tutorial/libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /torchbind_tutorial/build $ make -j Scanning dependencies of target custom_class [ 50%] Building CXX object CMakeFiles/custom_class.dir/class.cpp.o [100%] Linking CXX shared library libcustom_class.so [100%] Built target custom_class 您会发现，构建目录中现在有一个动态库文件。 在 Linux 上，它可能名为libcustom_class.so。 因此，文件树应如下所示： custom_class_project/ class.cpp CMakeLists.txt build/ libcustom_class.so 从 Python 和 TorchScript 使用 C++ 类 现在我们已经将我们的类及其注册编译为.so文件，我们可以将.so加载到 Python 中并进行尝试。 这是一个演示脚本的脚本： import torch # `torch.classes.load_library()` allows you to pass the path to your .so file # to load it in and make the custom C++ classes available to both Python and # TorchScript torch.classes.load_library(\"build/libcustom_class.so\") # You can query the loaded libraries like this: print(torch.classes.loaded_libraries) # prints {'/custom_class_project/build/libcustom_class.so'} # We can find and instantiate our custom C++ class in python by using the # `torch.classes` namespace: # # This instantiation will invoke the MyStackClass(std::vector init) # constructor we registered earlier s = torch.classes.my_classes.MyStackClass([\"foo\", \"bar\"]) # We can call methods in Python s.push(\"pushed\") assert s.pop() == \"pushed\" # Returning and passing instances of custom classes works as you'd expect s2 = s.clone() s.merge(s2) for expected in [\"bar\", \"foo\", \"bar\", \"foo\"]: assert s.pop() == expected # We can also use the class in TorchScript # For now, we need to assign the class's type to a local in order to # annotate the type on the TorchScript function. This may change # in the future. MyStackClass = torch.classes.my_classes.MyStackClass @torch.jit.script def do_stacks(s: MyStackClass): # We can pass a custom class instance # We can instantiate the class s2 = torch.classes.my_classes.MyStackClass([\"hi\", \"mom\"]) s2.merge(s) # We can call a method on the class # We can also return instances of the class # from TorchScript function/methods return s2.clone(), s2.top() stack, top = do_stacks(torch.classes.my_classes.MyStackClass([\"wow\"])) assert top == \"wow\" for expected in [\"wow\", \"mom\", \"hi\"]: assert stack.pop() == expected 使用自定义类保存，加载和运行 TorchScript 代码 我们还可以在使用 libtorch 的 C++ 进程中使用自定义注册的 C++ 类。 举例来说，让我们定义一个简单的nn.Module，它实例化并调用MyStackClass类上的方法： import torch torch.classes.load_library('build/libcustom_class.so') class Foo(torch.nn.Module): def __init__(self): super().__init__() def forward(self, s: str) -> str: stack = torch.classes.my_classes.MyStackClass([\"hi\", \"mom\"]) return stack.pop() + s scripted_foo = torch.jit.script(Foo()) print(scripted_foo.graph) scripted_foo.save('foo.pt') 我们文件系统中的foo.pt现在包含我们刚刚定义的序列化 TorchScript 程序。 现在，我们将定义一个新的 CMake 项目，以展示如何加载此模型及其所需的.so文件。 有关如何执行此操作的完整说明，请查看在 C++ 中加载 TorchScript 模型的教程。 与之前类似，让我们创建一个包含以下内容的文件结构： cpp_inference_example/ infer.cpp CMakeLists.txt foo.pt build/ custom_class_project/ class.cpp CMakeLists.txt build/ 请注意，我们已经复制了序列化的foo.pt文件以及上面custom_class_project的源代码树。 我们将把custom_class_project作为依赖项添加到此 C++ 项目中，以便可以将自定义类构建到二进制文件中。 让我们用以下内容填充infer.cpp： #include #include #include int main(int argc, const char* argv[]) { torch::jit::Module module; try { // Deserialize the ScriptModule from a file using torch::jit::load(). module = torch::jit::load(\"foo.pt\"); } catch (const c10::Error& e) { std::cerr inputs = {\"foobarbaz\"}; auto output = module.forward(inputs).toString(); std::cout string() 同样，让我们​​定义CMakeLists.txt文件： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(infer) find_package(Torch REQUIRED) add_subdirectory(custom_class_project) # Define our library target add_executable(infer infer.cpp) set(CMAKE_CXX_STANDARD 14) # Link against LibTorch target_link_libraries(infer \"${TORCH_LIBRARIES}\") # This is where we link in our libcustom_class code, making our # custom class available in our binary. target_link_libraries(infer -Wl,--no-as-needed custom_class) 您知道练习：cd build，cmake和make： $ cd build $ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" .. -- The C compiler identification is GNU 7.3.1 -- The CXX compiler identification is GNU 7.3.1 -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++ -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /local/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /cpp_inference_example/build $ make -j Scanning dependencies of target custom_class [ 25%] Building CXX object custom_class_project/CMakeFiles/custom_class.dir/class.cpp.o [ 50%] Linking CXX shared library libcustom_class.so [ 50%] Built target custom_class Scanning dependencies of target infer [ 75%] Building CXX object CMakeFiles/infer.dir/infer.cpp.o [100%] Linking CXX executable infer [100%] Built target infer 现在我们可以运行令人兴奋的 C++ 二进制文件： $ ./infer momfoobarbaz 难以置信！ 将自定义类移入或移出IValue 也可能需要将自定义类从自定义 C++ 类实例移入或移出IValue, such as when you take or return IValues from TorchScript methods or you want to instantiate a custom class attribute in C++. For creating an IValue： torch::make_custom_class()提供类似于c10::intrusive_ptr的 API，因为它将采用您提供给它的任何参数集，调用与该参数集匹配的T的构造器，并包装该实例，然后返回。 但是，它不仅返回指向自定义类对象的指针，还返回包装对象的IValue。 然后，您可以将此IValue直接传递给 TorchScript。 如果您已经有一个指向类的intrusive_ptr，则可以使用构造器IValue(intrusive_ptr)直接从其构造IValue。 要将IValue转换回自定义类： IValue::toCustomClass()将返回一个intrusive_ptr，指向IValue包含的自定义类。 在内部，此函数正在检查T是否已注册为自定义类，并且IValue实际上确实包含一个自定义类。 您可以通过调用isCustomClass()来手动检查IValue是否包含自定义类。 为自定义 C++ 类定义序列化/反序列化方法 如果您尝试将具有自定义绑定 C++ 类的ScriptModule保存为属性，则会出现以下错误： # export_attr.py import torch torch.classes.load_library('build/libcustom_class.so') class Foo(torch.nn.Module): def __init__(self): super().__init__() self.stack = torch.classes.my_classes.MyStackClass([\"just\", \"testing\"]) def forward(self, s: str) -> str: return self.stack.pop() + s scripted_foo = torch.jit.script(Foo()) scripted_foo.save('foo.pt') loaded = torch.jit.load('foo.pt') print(loaded.stack.pop()) $ python export_attr.py RuntimeError: Cannot serialize custom bound C++ class __torch__.torch.classes.my_classes.MyStackClass. Please define serialization methods via def_pickle for this class. (pushIValueImpl at ../torch/csrc/jit/pickler.cpp:128) 这是因为 TorchScript 无法自动找出 C++ 类中保存的信息。 您必须手动指定。 这样做的方法是使用class_上的特殊def_pickle方法在类上定义__getstate__和__setstate__方法。 注意 TorchScript 中__getstate__和__setstate__的语义与 Python pickle模块的语义相同。 您可以阅读更多有关如何使用这些方法的信息。 这是def_pickle调用的示例，我们可以将其添加到MyStackClass的注册中以包括序列化方法： // class_<>::def_pickle allows you to define the serialization // and deserialization methods for your C++ class. // Currently, we only support passing stateless lambda functions // as arguments to def_pickle .def_pickle( // __getstate__ // This function defines what data structure should be produced // when we serialize an instance of this class. The function // must take a single `self` argument, which is an intrusive_ptr // to the instance of the object. The function can return // any type that is supported as a return value of the TorchScript // custom operator API. In this instance, we've chosen to return // a std::vector as the salient data to preserve // from the class. [](const c10::intrusive_ptr>& self) -> std::vector { return self->stack_; }, // __setstate__ // This function defines how to create a new instance of the C++ // class when we are deserializing. The function must take a // single argument of the same type as the return value of // `__getstate__`. The function must return an intrusive_ptr // to a new instance of the C++ class, initialized however // you would like given the serialized state. [](std::vector state) -> c10::intrusive_ptr> { // A convenient way to instantiate an object and get an // intrusive_ptr to it is via `make_intrusive`. We use // that here to allocate an instance of MyStackClass // and call the single-argument std::vector // constructor with the serialized state. return c10::make_intrusive>(std::move(state)); }); 注意 我们在 Pickle API 中采用与pybind11不同的方法。pybind11作为传递给class_::def()的特殊函数pybind11::pickle()，为此我们有一个单独的方法def_pickle。 这是因为torch::jit::pickle这个名称已经被使用了，我们不想引起混淆。 以这种方式定义（反）序列化行为后，脚本现在可以成功运行： $ python ../export_attr.py testing 定义接受或返回绑定 C++ 类的自定义运算符 定义自定义 C++ 类后，您还可以将该类用作自变量或从自定义运算符返回（即自由函数）。 假设您具有以下自由函数： c10::intrusive_ptr> manipulate_instance(const c10::intrusive_ptr>& instance) { instance->pop(); return instance; } 您可以在TORCH_LIBRARY块中运行以下代码来注册它： m.def( \"foo::manipulate_instance(__torch__.torch.classes.my_classes.MyStackClass x) -> __torch__.torch.classes.my_classes.MyStackClass Y\", manipulate_instance ); 有关注册 API 的更多详细信息，请参见自定义操作教程。 完成此操作后，您可以像以下示例一样使用操作： class TryCustomOp(torch.nn.Module): def __init__(self): super(TryCustomOp, self).__init__() self.f = torch.classes.my_classes.MyStackClass([\"foo\", \"bar\"]) def forward(self): return torch.ops.foo.manipulate_instance(self.f) 注意 注册使用 C++ 类作为参数的运算符时，要求已注册自定义类。 您可以通过确保自定义类注册和您的自由函数定义在同一TORCH_LIBRARY块中，并确保自定义类注册位于第一位来强制实现此操作。 将来，我们可能会放宽此要求，以便可以按任何顺序进行注册。 总结 本教程向您介绍了如何向 TorchScript（以及扩展为 Python）公开 C++ 类，如何注册其方法，如何从 Python 和 TorchScript 使用该类以及如何使用该类保存和加载代码以及运行该代码。 在独立的 C++ 过程中。 现在，您可以使用与第三方 C++ 库连接的 C++ 类扩展 TorchScript 模型，或实现需要 Python，TorchScript 和 C++ 之间的界线平滑融合的任何其他用例。 与往常一样，如果您遇到任何问题或疑问，可以使用我们的论坛或 GitHub ISSUE 进行联系。 另外，我们的常见问题解答（FAQ）页面可能包含有用的信息。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"48.html":{"url":"48.html","title":"TorchScript 中的动态并行性","keywords":"","body":"TorchScript 中的动态并行性 原文：https://pytorch.org/tutorials/advanced/torch-script-parallelism.html 在本教程中，我们介绍在 TorchScript 中执行动态互操作并行化的语法。 此并行性具有以下属性： 动态-创建的并行任务的数量及其工作量可能取决于程序的控制流。 互操作-并行性与并行运行 TorchScript 程序片段有关。 这与运算内部并行化不同，后者涉及拆分单个运算符并并行运行运算符工作的子集。 基本语法 动态并行的两个重要 API 是： torch.jit.fork(fn : Callable[..., T], *args, **kwargs) -> torch.jit.Future[T] torch.jit.wait(fut : torch.jit.Future[T]) -> T 通过示例来演示这些工作原理的好方法： import torch def foo(x): return torch.neg(x) @torch.jit.script def example(x): # Call `foo` using parallelism: # First, we \"fork\" off a task. This task will run `foo` with argument `x` future = torch.jit.fork(foo, x) # Call `foo` normally x_normal = foo(x) # Second, we \"wait\" on the task. Since the task may be running in # parallel, we have to \"wait\" for its result to become available. # Notice that by having lines of code between the \"fork()\" and \"wait()\" # call for a given Future, we can overlap computations so that they # run in parallel. x_parallel = torch.jit.wait(future) return x_normal, x_parallel print(example(torch.ones(1))) # (-1., -1.) fork()接受可调用fn以及该可调用args和kwargs的参数，并创建异步任务来执行fn。 fn可以是函数，方法或模块实例。 fork()返回对此执行结果的值的引用，称为Future。 因为fork在创建异步任务后立即返回，所以在执行fork()调用之后的代码行时可能尚未执行fn。 因此，wait()用于等待异步任务完成并返回值。 这些结构可用于重叠函数内语句的执行（如工作示例部分所示），或与其他语言结构（如循环）组合在一起： import torch from typing import List def foo(x): return torch.neg(x) @torch.jit.script def example(x): futures : List[torch.jit.Future[torch.Tensor]] = [] for _ in range(100): futures.append(torch.jit.fork(foo, x)) results = [] for future in futures: results.append(torch.jit.wait(future)) return torch.sum(torch.stack(results)) print(example(torch.ones([]))) 注意 当我们初始化一个空的期货列表时，我们需要在futures上添加一个显式类型注解。 在 TorchScript 中，空容器默认假定它们包含张量值，因此我们将列表构造器#注解为List[torch.jit.Future[torch.Tensor]]类型 本示例使用fork()启动函数foo的 100 个实例，等待 100 个任务完成，然后对结果求和，返回-100.0。 应用示例：双向 LSTM 的集成 让我们尝试将并行性应用于一个更现实的示例，看看我们可以从中获得什么样的性能。 首先，让我们定义基准模型：双向 LSTM 层的集合。 import torch, time # In RNN parlance, the dimensions we care about are: # # of time-steps (T) # Batch size (B) # Hidden size/number of \"channels\" (C) T, B, C = 50, 50, 1024 # A module that defines a single \"bidirectional LSTM\". This is simply two # LSTMs applied to the same sequence, but one in reverse class BidirectionalRecurrentLSTM(torch.nn.Module): def __init__(self): super().__init__() self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C) self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C) def forward(self, x : torch.Tensor) -> torch.Tensor: # Forward layer output_f, _ = self.cell_f(x) # Backward layer. Flip input in the time dimension (dim 0), apply the # layer, then flip the outputs in the time dimension x_rev = torch.flip(x, dims=[0]) output_b, _ = self.cell_b(torch.flip(x, dims=[0])) output_b_rev = torch.flip(output_b, dims=[0]) return torch.cat((output_f, output_b_rev), dim=2) # An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the # ensemble are run one-by-one on the same input then their results are # stacked and summed together, returning the combined result. class LSTMEnsemble(torch.nn.Module): def __init__(self, n_models): super().__init__() self.n_models = n_models self.models = torch.nn.ModuleList([ BidirectionalRecurrentLSTM() for _ in range(self.n_models)]) def forward(self, x : torch.Tensor) -> torch.Tensor: results = [] for model in self.models: results.append(model(x)) return torch.stack(results).sum(dim=0) # For a head-to-head comparison to what we're going to do with fork/wait, let's # instantiate the model and compile it with TorchScript ens = torch.jit.script(LSTMEnsemble(n_models=4)) # Normally you would pull this input out of an embedding table, but for the # purpose of this demo let's just use random data. x = torch.rand(T, B, C) # Let's run the model once to warm up things like the memory allocator ens(x) x = torch.rand(T, B, C) # Let's see how fast it runs! s = time.time() ens(x) print('Inference took', time.time() - s, ' seconds') 在我的机器上，该网络运行时间为2.05秒。 我们可以做得更好！ 并行化前向和后向层 我们可以做的一个非常简单的事情是在BidirectionalRecurrentLSTM中并行化前进和后退层。 为此，计算结构是静态的，因此我们实际上甚至不需要任何循环。 像这样重写BidirectionalRecurrentLSTM的forward方法： def forward(self, x : torch.Tensor) -> torch.Tensor: # Forward layer - fork() so this can run in parallel to the backward # layer future_f = torch.jit.fork(self.cell_f, x) # Backward layer. Flip input in the time dimension (dim 0), apply the # layer, then flip the outputs in the time dimension x_rev = torch.flip(x, dims=[0]) output_b, _ = self.cell_b(torch.flip(x, dims=[0])) output_b_rev = torch.flip(output_b, dims=[0]) # Retrieve the output from the forward layer. Note this needs to happen # *after* the stuff we want to parallelize with output_f, _ = torch.jit.wait(future_f) return torch.cat((output_f, output_b_rev), dim=2) 在此示例中，forward()将cell_f的执行委派给另一个线程，而它继续执行cell_b。 这导致两个单元的执行彼此重叠。 通过简单的修改再次运行脚本会产生1.71秒的运行时间，从而改进了17%！ 旁注：可视化并行性 我们还没有完成模型的优化，但是值得介绍一下用于可视化性能的工具。 一种重要的工具是 PyTorch 分析器。 让我们将分析器与 Chrome 跟踪导出功能一起使用，以可视化并行模型的性能： 此代码段将写出名为parallel.json的文件。 如果您将 Google Chrome 浏览器导航到chrome://tracing，单击Load按钮，然后加载该 JSON 文件，则应该看到类似以下的时间轴： 时间轴的横轴表示时间，纵轴表示执行线程。 如我们所见，我们一次运行两个lstm实例。 这是我们辛勤工作使双向层平行的结果！ 集成中的并行化模型 您可能已经注意到，我们的代码中还有更多的并行化机会：我们还可以并行运行LSTMEnsemble中包含的模型。 做到这一点的方法很简单，这就是我们应该更改LSTMEnsemble的forward方法的方式： def forward(self, x : torch.Tensor) -> torch.Tensor: # Launch tasks for each model futures : List[torch.jit.Future[torch.Tensor]] = [] for model in self.models: futures.append(torch.jit.fork(model, x)) # Collect the results from the launched tasks results : List[torch.Tensor] = [] for future in futures: results.append(torch.jit.wait(future)) return torch.stack(results).sum(dim=0) 或者，如果您重视简洁性，我们可以使用列表推导： def forward(self, x : torch.Tensor) -> torch.Tensor: futures = [torch.jit.fork(model, x) for model in self.models] results = [torch.jit.wait(fut) for fut in futures] return torch.stack(results).sum(dim=0) 如介绍中所述，我们使用循环为集合中的每个模型分派任务。 然后，我们使用了另一个循环来等待所有任务完成。 这提供了更多的计算重叠。 通过此小更新，脚本将在1.4秒内运行，总速度为32%！ 两行代码相当不错。 我们还可以再次使用 Chrome 跟踪器来查看运行情况： 现在我们可以看到所有LSTM实例都在完全并行运行。 总结 在本教程中，我们学习了fork()和wait()，这是在 TorchScript 中执行动态，互操作并行的基本 API。 我们看到了一些典型的使用模式，这些模式使用这些函数并行执行 TorchScript 代码中的函数，方法或Modules的执行。 最后，我们通过一个使用该技术优化模型的示例进行了研究，并探索了 PyTorch 中可用的性能测量和可视化工具。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"49.html":{"url":"49.html","title":"C++ 前端中的 Autograd","keywords":"","body":"C++ 前端中的 Autograd 原文：https://pytorch.org/tutorials/advanced/cpp_autograd.html autograd包对于在 PyTorch 中构建高度灵活和动态的神经网络至关重要。 PyTorch Python 前端中的大多数 autograd API 也可以在 C++ 前端中使用，从而可以轻松地将 Autograd 代码从 Python 转换为 C++。 在本教程中，我们将看几个在 PyTorch C++ 前端中进行 Autograd 的示例。 请注意，本教程假定您已经对 Python 前端中的 Autograd 有基本的了解。 如果不是这种情况，请先阅读 Autograd：自动微分。 基本的 Autograd 操作 （改编自本教程） 创建一个张量并设置torch::requires_grad()以跟踪它的计算 auto x = torch::ones({2, 2}, torch::requires_grad()); std::cout 出： 1 1 1 1 [ CPUFloatType{2,2} ] 进行张量运算： auto y = x + 2; std::cout 出： 3 3 3 3 [ CPUFloatType{2,2} ] y是由于操作而创建的，因此具有grad_fn。 std::cout name() 出： AddBackward1 在y上执行更多操作 auto z = y * y * 3; auto out = z.mean(); std::cout name() name() 出： 27 27 27 27 [ CPUFloatType{2,2} ] MulBackward1 27 [ CPUFloatType{} ] MeanBackward0 .requires_grad_( ... )原地更改现有张量的requires_grad标志。 auto a = torch::randn({2, 2}); a = ((a * 3) / (a - 1)); std::cout name() 出： false true SumBackward0 现在让我们反向传播。 因为out包含单个标量，所以out.backward()等效于out.backward(torch::tensor(1.))。 out.backward(); 打印梯度d(out) / dx std::cout 出： 4.5000 4.5000 4.5000 4.5000 [ CPUFloatType{2,2} ] 您应该具有4.5的矩阵。 有关如何获得此值的说明，请参见本教程中的相应部分。 现在，让我们来看一个向量雅各布产品的示例： x = torch::randn(3, torch::requires_grad()); y = x * 2; while (y.norm().item() name() 出： -1021.4020 314.6695 -613.4944 [ CPUFloatType{3} ] MulBackward1 如果我们想要向量-Jacobian 乘积，请将向量作为参数传递给backward： auto v = torch::tensor({0.1, 1.0, 0.0001}, torch::kFloat); y.backward(v); std::cout 出： 102.4000 1024.0000 0.1024 [ CPUFloatType{3} ] 您也可以通过在代码块中放置torch::NoGradGuard来停止对需要梯度的张量的跟踪历史的自动定格 std::cout 出： true true false 或者使用.detach()获得具有相同内容但不需要梯度的新张量： std::cout () 出： true false true 有关 C++ 张量自动梯度 API 的更多信息，例如grad/requires_grad/is_leaf/backward/detach/detach_/register_hook/retain_grad，请参见相应的 C++ API 文档。 用 C++ 计算高阶梯度 高阶梯度的应用之一是计算梯度罚分。 我们来看看使用torch::autograd::grad的示例： #include auto model = torch::nn::Linear(4, 3); auto input = torch::randn({3, 4}).requires_grad_(true); auto output = model(input); // Calculate loss auto target = torch::randn({3, 3}); auto loss = torch::nn::MSELoss()(output, target); // Use norm of gradients as penalty auto grad_output = torch::ones_like(output); auto gradient = torch::autograd::grad({output}, {input}, /*grad_outputs=*/{grad_output}, /*create_graph=*/true)[0]; auto gradient_penalty = torch::pow((gradient.norm(2, /*dim=*/1) - 1), 2).mean(); // Add gradient penalty to loss auto combined_loss = loss + gradient_penalty; combined_loss.backward(); std::cout 出： -0.1042 -0.0638 0.0103 0.0723 -0.2543 -0.1222 0.0071 0.0814 -0.1683 -0.1052 0.0355 0.1024 [ CPUFloatType{3,4} ] 有关如何使用它们的更多信息，请参见torch::autograd::backward和torch::autograd::grad的文档。 在 C++ 中使用自定义 Autograd 函数 （改编自本教程） 向torch::autograd添加新的基本操作需要为每个操作实现一个新的torch::autograd::Function子类。 torch::autograd::Function用于torch::autograd计算结果和梯度，并对操作历史进行编码。 每个新函数都需要您实现两种方法：forward和backward，有关详细要求，请参见此链接。 在下面，您可以从torch::nn找到Linear函数的代码： #include using namespace torch::autograd; // Inherit from Function class LinearFunction : public Function { public: // Note that both forward and backward are static functions // bias is an optional argument static torch::Tensor forward( AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) { ctx->save_for_backward({input, weight, bias}); auto output = input.mm(weight.t()); if (bias.defined()) { output += bias.unsqueeze(0).expand_as(output); } return output; } static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) { auto saved = ctx->get_saved_variables(); auto input = saved[0]; auto weight = saved[1]; auto bias = saved[2]; auto grad_output = grad_outputs[0]; auto grad_input = grad_output.mm(weight); auto grad_weight = grad_output.t().mm(input); auto grad_bias = torch::Tensor(); if (bias.defined()) { grad_bias = grad_output.sum(0); } return {grad_input, grad_weight, grad_bias}; } }; 然后，我们可以通过以下方式使用LinearFunction： auto x = torch::randn({2, 3}).requires_grad_(); auto weight = torch::randn({4, 3}).requires_grad_(); auto y = LinearFunction::apply(x, weight); y.sum().backward(); std::cout 出： 0.5314 1.2807 1.4864 0.5314 1.2807 1.4864 [ CPUFloatType{2,3} ] 3.7608 0.9101 0.0073 3.7608 0.9101 0.0073 3.7608 0.9101 0.0073 3.7608 0.9101 0.0073 [ CPUFloatType{4,3} ] 在这里，我们给出了一个由非张量参数设置参数的函数的附加示例： #include using namespace torch::autograd; class MulConstant : public Function { public: static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) { // ctx is a context object that can be used to stash information // for backward computation ctx->saved_data[\"constant\"] = constant; return tensor * constant; } static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) { // We return as many input gradients as there were arguments. // Gradients of non-tensor arguments to forward must be `torch::Tensor()`. return {grad_outputs[0] * ctx->saved_data[\"constant\"].toDouble(), torch::Tensor()}; } }; 然后，我们可以通过以下方式使用MulConstant： auto x = torch::randn({2}).requires_grad_(); auto y = MulConstant::apply(x, 5.5); y.sum().backward(); std::cout 出： 5.5000 5.5000 [ CPUFloatType{2} ] 有关torch::autograd::Function的更多信息，请参见其文档。 将 Autograd 代码从 Python 转换为 C++ 在较高的层次上，在 C++ 中使用 Autograd 的最简单方法是先在 Python 中拥有可用的 Autograd 代码，然后使用下表将您的 Autograd 代码从 Python 转换为 C++： Python C++ torch.autograd.backward torch::autograd::backward） torch.autograd.grad torch::autograd::grad） torch.Tensor.detach torch::Tensor::detach） torch.Tensor.detach_ torch::Tensor::detach_） torch.Tensor.backward torch::Tensor::backward） torch.Tensor.register_hook torch::Tensor::register_hook） torch.Tensor.requires_grad torch::Tensor::requires_grad_） torch.Tensor.retain_grad torch::Tensor::retain_grad） torch.Tensor.grad torch::Tensor::grad） torch.Tensor.grad_fn torch::Tensor::grad_fn） torch.Tensor.set_data torch::Tensor::set_data） torch.Tensor.data torch::Tensor::data） torch.Tensor.output_nr torch::Tensor::output_nr） torch.Tensor.is_leaf torch::Tensor::is_leaf） 翻译后，您的大多数 Python Autograd 代码都应仅在 C++ 中工作。 如果不是这种情况，请在 GitHub ISSUE 中提交错误报告，我们将尽快对其进行修复。 总结 现在，您应该对 PyTorch 的 C++ autograd API 有了一个很好的了解。 您可以在此处找到本说明中显示的代码示例。 与往常一样，如果您遇到任何问题或疑问，可以使用我们的论坛或 GitHub ISSUE 进行联系。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"50.html":{"url":"50.html","title":"在 C++ 中注册调度运算符","keywords":"","body":"在 C++ 中注册调度运算符 原文：https://pytorch.org/tutorials/advanced/dispatcher.html 调度器是 PyTorch 的内部组件，负责确定调用torch::add之类的函数时应实际运行哪些代码。 这是不平凡的，因为 PyTorch 操作需要处理很多交叉关注点，这些关注点“层叠”在另一个之上。 以下是其处理的一些示例： 根据输入张量的设备，在运算符的 CPU 和 CUDA 实现之间切换。 在运算符的自动微分和后端实现之间切换，这取决于是否需要自动微分处理。 必要时应用自动广播来实现自动混合精度。 当运算符在vmap调用下运行时，应用批量规则。 如果要跟踪导出的模型，则跟踪操作的执行。 如果在自定义运算符代码中发现自己手动编写了if语句来处理这些情况，则调度器 API 可以帮助组织代码。 （相反，如果您的自定义运算符非常简单并且仅用于 CPU 推断，则可能不需要使用调度器，只需使用基本 API。） 在本教程中，我们将描述如何构造自定义运算符注册以使用调度器来组织各种组件。 我们假设您熟悉如何注册运算符以及如何编写自定义自动微分函数。 定义模式和后端实现 调度器背后的一般原理是将一个运算符的实现分为多个内核，每个内核都为特定的调度键实现功能； 例如，CPU，CUDA 或 Autograd。 调度器在您调用运算符时确定最高优先级的调度键是什么（这通过查看张量参数和某些线程本地状态来完成），并将控制权传递给内核以使用该调度键。 最终结果是，当您调用运算符时，我们首先执行 Autograd 内核，然后根据传入的张量的设备类型将其重新分配到 CPU 或 CUDA 内核。 让我们看一下实现这一目标所涉及的各个部分。 首先，我们必须为所讨论的运算符定义架构。 与简单的pybind11样式的运算符注册不同，我们目前实际上并未提供运算符的实现； 我们只提供一个模式字符串，指定所有其他内核将遵守的运算符的类型签名： TORCH_LIBRARY(myops, m) { m.def(\"myadd(Tensor self, Tensor other) -> Tensor\"); } 接下来，我们需要实际提供此运算符的一些实现。 具体来说，这是一个非常简单的 CPU 实现： Tensor myadd_cpu(const Tensor& self_, const Tensor& other_) { TORCH_CHECK(self_.sizes() == other_.sizes()); TORCH_INTERNAL_ASSERT(self_.device().type() == DeviceType::CPU); TORCH_INTERNAL_ASSERT(other_.device().type() == DeviceType::CPU); Tensor self = self_.contiguous(); Tensor other = other_.contiguous(); Tensor result = torch::empty(self.sizes(), self.options()); const float* self_ptr = self.data_ptr(); const float* other_ptr = other.data_ptr(); float* result_ptr = result.data_ptr(); for (int64_t i = 0; i 我们想将此函数注册为myops::myadd的实现。 但是，简单的注册方法（def(\"myadd\", myadd_cpu)）将注册内核以在所有情况下都可以运行，即使张量不是 CPU 张量！ （在内部，我们将它们称为“全部捕获”内核，因为它们捕获所有情况。）为确保仅针对 CPU 张量运行myadd_cpu，我们可以使用TORCH_LIBRARY_IMPL宏： TORCH_LIBRARY_IMPL(myops, CPU, m) { m.impl(\"myadd\", myadd_cpu); } 通过TORCH_LIBRARY_IMPL，我们可以在特定的调度键（在本例中为 CPU）上为运算符注册实现。 每次对impl的调用都会将 CPU 内核与相应的运算符（我们先前在TORCH_LIBRARY块中定义）相关联。 如果我们还有 CUDA 实现myadd_cuda，我们可以将其注册在单独的TORCH_LIBRARY_IMPL块中： TORCH_LIBRARY_IMPL(myops, CUDA, m) { m.impl(\"myadd\", myadd_cuda); } 这些注册可以跨文件甚至跨库边界拆分； 因此，例如，您可以将这两个TORCH_LIBRARY_IMPL块编译为单独的myops_cpu和myops_cuda动态库。 一般来说，您的注册结构如下所示： 单个TORCH_LIBRARY在集中位置列出名称空间中的每个自定义运算符。 每个调度键的TORCH_LIBRARY_IMPL，用于注册该键的实现（例如，CPU 或 CUDA）。 如果愿意，可以按每个运算符将TORCH_LIBRARY_IMPL块进一步细分为一个块。 如果每个运算符的实现都有一个单独的文件，但是又不想在标头中显示运算符，这将很方便。 您只需将注册内容放入定义您的运算符的 cpp 文件中。 注意 您知道吗，您还可以为 PyTorch 中的现有核心运算符编写TORCH_LIBRARY_IMPL块？ 这就是实现 XLA 对 PyTorch 的支持的方式：torch_xla库包含一个TORCH_LIBRARY_IMPL，该库为 XLA 调度键上的所有基本运算符提供实现。 添加 Autograd 支持 至此，我们有了一个同时具有 CPU 和 CUDA 实现的运算符。 我们如何为它添加 Autograd 支持？ 您可能会猜到，我们将注册一个 Autograd 内核（类似于自定义 Autograd 函数教程中描述的内容）！ 但是，有一个变数：与 CPU 和 CUDA 内核不同，Autograd 内核需要重新分发：它需要回调调度器才能到达最终的 CPU 和 CUDA 实现。 因此，在编写 Autograd 内核之前，让我们编写一个调度函数，该函数调用调度器以为您的运算符找到合适的内核。 该函数构成了供您的运算符使用的公共 C++ API，实际上，PyTorch C++ API 中的所有张量函数都在后台完全以相同的方式调用了调度器。 调度函数如下所示： Tensor myadd(const Tensor& self, const Tensor& other) { static auto op = torch::Dispatcher::singleton() .findSchemaOrThrow(\"myops::myadd\", \"\") .typed(); return op.call(self, other); } 让我们分解一下： 在第一行中，我们从调度器中查找与要调度到的运算符相对应的类型化运算符句柄。 findSchemaOrThrow具有两个参数：运算符的（名称空间限定）名称和运算符的重载名称（通常只是空字符串）。 typed将动态类型的句柄转换为静态类型的句柄（进行运行时测试以确保您提供了正确的 C++ 类型），以便我们可以对其进行常规的 C++ 调用。 我们将其传递给decltype(myadd)，因为调度函数的类型与注册到调度器的基础内核的类型相同。 为了提高性能，此计算是在静态变量中完成的，因此我们只需要进行一次（慢速）查找。 如果键入了要调用的运算符的名称，则第一次调用此函数时，此查找将出错。 在第二行中，我们只需将所有参数传递到调度函数中，就可以简单地call运算符句柄。 这实际上将调用调度器，最终控制权将转移到适合此调用的任何内核。 有了分发函数，我们现在可以编写 Autograd 内核： class MyAddFunction : public torch::autograd::Function { public: static Tensor forward( AutogradContext *ctx, torch::Tensor self, torch::Tensor other) { at::AutoNonVariableTypeMode g; return myadd(self, other); } static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) { auto grad_output = grad_outputs[0]; return {grad_output, grad_output}; } }; Tensor myadd_autograd(const Tensor& self, const Tensor& other) { return MyAddFunction::apply(self, other)[0]; } 使用torch::autograd::Function正常编写 Autograd 函数，除了代替直接在forward()中编写实现，我们： 使用at::AutoNonVariableTypeMode RAII 保护器关闭 Autograd 处理，然后 调用调度函数myadd以回调调度器。 如果没有（1），您的调用将无限循环（并且栈溢出），因为myadd将使您返回此函数（因为最高优先级分配键仍将是自动微分的。）对于（1），自动微分从一组正在考虑的调度键中排除，我们将转到下一个处理器，即 CPU 和 CUDA。 现在，我们可以按照注册 CPU/CUDA 函数的相同方式注册此函数： TORCH_LIBRARY_IMPL(myops, Autograd, m) { m.impl(\"myadd\", myadd_autograd); } 超越 Autograd 从某种意义上说，调度员并没有做太多事情：它所做的只是实现一种美化的if语句，其方法如下： class MyAddFunction : ... { public: static Tensor forward( AutogradContext *ctx, torch::Tensor self, torch::Tensor other) { if (self.device().type() == DeviceType::CPU) { return add_cpu(self, other); } else if (self.device().type() == DeviceType::CUDA) { return add_cuda(self, other); } else { TORCH_CHECK(0, \"Unsupported device \", self.device().type()); } } ... } 那么为什么要使用调度器呢？ 有几个原因： 它是分散的。 您可以组装运算符的所有部分（CPU，CUDA，Autograd），而不必编写引用所有元素的集中式if语句。 重要的是，第三方可以注册其他方面的额外实现，而不必修补运算符的原始定义。 它比 CPU，CUDA 和 Autograd 支持更多的调度键。 您可以在c10/core/DispatchKey.h中查看 PyTorch 中当前实现的调度键的完整列表。 这些调度键为运算符实现了多种可选功能，如果您决定希望自定义运算符支持该功能，则只需为相应的键注册内核即可。 调度器实现对盒装后备函数的支持，后者是可以一次实现并应用于系统中所有运算符的函数。 盒装后备可用于提供调度键的默认行为。 如果您使用调度器来实现您的运算符，那么您还可以选择所有这些操作的备用。 这是一些特定的调度键，您可能需要为其定义一个运算符。 Autocast Autocast 调度键实现对自动混合精度（AMP）的支持。 自动广播包装器内核通常会在运行操作之前将传入的float16或float32 CUDA 张量转换为某些首选精度。 例如，浮点 CUDA 张量上的积和卷积通常运行得更快，并且在float16中使用较少的内存，而不会影响收敛。 自动广播包装器仅在启用自动广播的上下文中有效。 这是假设的自定义Matmul的自动广播包装器及其注册信息： // Autocast-specific helper functions #include Tensor mymatmul_autocast(const Tensor& self, const Tensor& other) { c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast); return mymatmul(at::autocast::cached_cast(at::kHalf, self), at::autocast::cached_cast(at::kHalf, other)); } TORCH_LIBRARY_IMPL(myops, Autocast, m) { m.impl(\"mymatmul\", mymatmul_autocast); } 如果tensor为 CUDA 和float32，则cached_cast(kHalf, tensor)将tensor强制转换为float16，否则，tensor保持不变（参见资格策略对于本地自动播报的操作）。 这样可以确保网络是否在float16和float32 CUDA 张量的任何混合形式上调用mymatmul，mymatmul在float16中运行。 同时，使用非 CUDA，整数类型或float64输入的对mymatmul的调用不受影响。 建议使用cached_cast在您自己的自动广播包装程序中遵循本机资格策略，但不是必需的。 例如，如果要对所有输入类型强制执行float16，则可以使用return mymatmul(self.half(), other.half());而不是使用cached_cast。 请注意，就像我们的 Autograd 内核一样，我们在重新分配之前从分配中排除Autocast键。 默认情况下，如果未提供自动广播包装器，我们将直接进入常规的运算符实现（不进行自动广播）。 （在此示例中，我们没有使用myadd，因为逐点加法不需要自动广播，因此应该会失败。） 什么时候应该注册自动广播包装器？ 不幸的是，对于运算符的首选精度并没有严格的规定。 通过查看运算符列表，您可以了解某些本机运算符的首选精度。 一般指导： 进行减少操作的操作可能应该在float32中执行， 在幕后进行卷积或宝石运算的任何操作都应在float16中执行，并且 具有多个浮点张量输入的其他运算符应将它们标准化为通用精度（除非实现支持具有不同精度的输入）。 如果您的自定义操作属于第三类，则promote_type模板有助于找出输入张量中存在的最宽浮点类型，这是执行类型的最安全选择： #include Tensor my_multiple_input_op_autocast(const Tensor& t0, const Tensor& t1) { c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast); // The required at::kHalf argument is an optimistic initial guess. auto exec_type = at::autocast::promote_type(at::kHalf, t0, t1); return my_multiple_input_op(at::autocast::cached_cast(exec_type, t0), at::autocast::cached_cast(exec_type, t1)); } 如果您的自定义操作已启用 Autograd，则只需编写和注册自动广播包装器，其名称与注册自动梯度包装器的名称相同。 例如，如果您想为 Autograd 部分中显示的myadd函数使用自动广播包装，那么您所需要做的就是 Tensor myadd_autocast(const Tensor& self, const Tensor& other) { c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast); return myadd(at::autocast::cached_cast(, self), at::autocast::cached_cast(, other)); } TORCH_LIBRARY_IMPL(myops, Autocast, m) { m.impl(\"myadd\", myadd_autocast); } 没有单独的体操可使后向方法自动广播兼容。 但是，在自定义 Autograd 函数中定义的向后方法将以与正向方法的自动广播集相同的dtype运行，因此您应该选择既适合于正向方法又适合于向后方法的。 批量 批量张量允许您按示例方式编写代码，然后在vmap调用下运行时自动对其进行批量。 当前正在开发用于编写批量规则的 API，但是一旦稳定该 API，就可以通过在 Batched 调度键处注册内核来为运算符添加对vmap的支持。 追踪器 追踪器调度键实现了对在运行torch.jit.trace时将运算符调用记录到跟踪中的支持。 我们打算提供一个盒装后备，它将实现对任意操作的跟踪，请参阅 ISSUE#41478 以跟踪进度。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"51.html":{"url":"51.html","title":"模型优化","keywords":"","body":"模型优化 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"52.html":{"url":"52.html","title":"分析您的 PyTorch 模块","keywords":"","body":"分析您的 PyTorch 模块 原文：https://pytorch.org/tutorials/beginner/profiler.html 作者： Suraj Subramanian PyTorch 包含一个探查器 API，可用于识别代码中各种 PyTorch 操作的时间和内存成本。 Profiler 可以轻松集成到您的代码中，结果可以打印为表格或在 JSON 跟踪文件中显示。 注意 Profiler 支持多线程模型。 Profiler 与该操作在同一线程中运行，但它还将对可能在另一个线程中运行的子运算符进行概要分析。 同时运行的探查器的作用域将限制在其自己的线程中，以防止结果混淆。 转到此秘籍，可以更快地了解 Profiler API 的用法。 import torch import numpy as np from torch import nn import torch.autograd.profiler as profiler 使用 Profiler 的性能调试 Profiler 有助于识别模型中的性能瓶颈。 在此示例中，我们构建了一个自定义模块，该模块执行两个子任务： 输入的线性变换，以及 使用转换结果来获取遮罩张量上的索引。 我们使用profiler.record_function(\"label\")将每个子任务的代码包装在单独的带标签的上下文管理器中。 在事件探查器输出中，子任务中所有操作的综合性能指标将显示在其相应的标签下。 请注意，使用 Profiler 会产生一些开销，并且最好仅用于调查代码。 如果要对运行时进行基准测试，请记住将其删除。 class MyModule(nn.Module): def __init__(self, in_features: int, out_features: int, bias: bool = True): super(MyModule, self).__init__() self.linear = nn.Linear(in_features, out_features, bias) def forward(self, input, mask): with profiler.record_function(\"LINEAR PASS\"): out = self.linear(input) with profiler.record_function(\"MASK INDICES\"): threshold = out.sum(axis=1).mean().item() hi_idx = np.argwhere(mask.cpu().numpy() > threshold) hi_idx = torch.from_numpy(hi_idx).cuda() return out, hi_idx 分析正向传播 我们初始化随机输入和蒙版张量以及模型。 在运行探查器之前，我们需要对 CUDA 进行预热，以确保进行准确的性能基准测试。 我们将模块的正向传播包装在profiler.profile上下文管理器中。 with_stack=True参数在跟踪中附加操作的文件和行号。 警告 with_stack=True会产生额外的开销，并且更适合于研究代码。 如果要对性能进行基准测试，请记住将其删除。 model = MyModule(500, 10).cuda() input = torch.rand(128, 500).cuda() mask = torch.rand((500, 500, 500), dtype=torch.double).cuda() # warm-up model(input, mask) with profiler.profile(with_stack=True, profile_memory=True) as prof: out, idx = model(input, mask) 打印分析器结果 最后，我们打印分析器结果。 profiler.key_averages通过运算符名称，以及可选地通过输入形状和/或栈跟踪事件来聚合结果。 按输入形状分组有助于识别模型使用哪些张量形状。 在这里，我们使用group_by_stack_n=5通过操作及其回溯（截断为最近的 5 个事件）聚合运行时，并按事件注册的顺序显示事件。 还可以通过传递sort_by参数对表进行排序（有关有效的排序键，请参阅文档）。 注意 在笔记本中运行 Profiler 时，您可能会在栈跟踪中看到(13): forward之类的条目，而不是文件名。 这些对应于(line number): calling-function。 print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5)) \"\"\" (Some columns are omitted) ------------- ------------ ------------ ------------ --------------------------------- Name Self CPU % Self CPU Self CPU Mem Source Location ------------- ------------ ------------ ------------ --------------------------------- MASK INDICES 87.88% 5.212s -953.67 Mb /mnt/xarfuse/.../torch/au (10): forward /mnt/xarfuse/.../torch/nn (9): /mnt/xarfuse/.../IPython/ aten::copy_ 12.07% 715.848ms 0 b (12): forward /mnt/xarfuse/.../torch/nn (9): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ LINEAR PASS 0.01% 350.151us -20 b /mnt/xarfuse/.../torch/au (7): forward /mnt/xarfuse/.../torch/nn (9): /mnt/xarfuse/.../IPython/ aten::addmm 0.00% 293.342us 0 b /mnt/xarfuse/.../torch/nn /mnt/xarfuse/.../torch/nn /mnt/xarfuse/.../torch/nn (8): forward /mnt/xarfuse/.../torch/nn aten::mean 0.00% 235.095us 0 b (11): forward /mnt/xarfuse/.../torch/nn (9): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ ----------------------------- ------------ ---------- ---------------------------------- Self CPU time total: 5.931s \"\"\" 提高内存性能 请注意，就内存和时间而言，最昂贵的操作位于forward (10)，代表掩码索引中的操作。 让我们尝试先解决内存消耗问题。 我们可以看到第 12 行的.to()操作消耗 953.67 Mb。 该操作将mask复制到 CPU。 mask使用torch.double数据类型初始化。 我们可以通过将其转换为torch.float来减少内存占用吗？ model = MyModule(500, 10).cuda() input = torch.rand(128, 500).cuda() mask = torch.rand((500, 500, 500), dtype=torch.float).cuda() # warm-up model(input, mask) with profiler.profile(with_stack=True, profile_memory=True) as prof: out, idx = model(input, mask) print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5)) \"\"\" (Some columns are omitted) ----------------- ------------ ------------ ------------ -------------------------------- Name Self CPU % Self CPU Self CPU Mem Source Location ----------------- ------------ ------------ ------------ -------------------------------- MASK INDICES 93.61% 5.006s -476.84 Mb /mnt/xarfuse/.../torch/au (10): forward /mnt/xarfuse/ /torch/nn (9): /mnt/xarfuse/.../IPython/ aten::copy_ 6.34% 338.759ms 0 b (12): forward /mnt/xarfuse/.../torch/nn (9): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ aten::as_strided 0.01% 281.808us 0 b (11): forward /mnt/xarfuse/.../torch/nn (9): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ aten::addmm 0.01% 275.721us 0 b /mnt/xarfuse/.../torch/nn /mnt/xarfuse/.../torch/nn /mnt/xarfuse/.../torch/nn (8): forward /mnt/xarfuse/.../torch/nn aten::_local 0.01% 268.650us 0 b (11): forward _scalar_dense /mnt/xarfuse/.../torch/nn (9): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ ----------------- ------------ ------------ ------------ -------------------------------- Self CPU time total: 5.347s \"\"\" 此操作的 CPU 内存占用量减少了一半。 提高时间表现 虽然所消耗的时间也有所减少，但仍然太高。 原来，将矩阵从 CUDA 复制到 CPU 非常昂贵！ forward (12)中的aten::copy_运算符将mask复制到 CPU，以便可以使用 NumPy argwhere函数。 forward(13)处的aten::copy_将数组作为张量复制回 CUDA。 如果我们在这里使用torch函数nonzero()，则可以消除这两个方面。 class MyModule(nn.Module): def __init__(self, in_features: int, out_features: int, bias: bool = True): super(MyModule, self).__init__() self.linear = nn.Linear(in_features, out_features, bias) def forward(self, input, mask): with profiler.record_function(\"LINEAR PASS\"): out = self.linear(input) with profiler.record_function(\"MASK INDICES\"): threshold = out.sum(axis=1).mean() hi_idx = (mask > threshold).nonzero(as_tuple=True) return out, hi_idx model = MyModule(500, 10).cuda() input = torch.rand(128, 500).cuda() mask = torch.rand((500, 500, 500), dtype=torch.float).cuda() # warm-up model(input, mask) with profiler.profile(with_stack=True, profile_memory=True) as prof: out, idx = model(input, mask) print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5)) \"\"\" (Some columns are omitted) -------------- ------------ ------------ ------------ --------------------------------- Name Self CPU % Self CPU Self CPU Mem Source Location -------------- ------------ ------------ ------------ --------------------------------- aten::gt 57.17% 129.089ms 0 b (12): forward /mnt/xarfuse/.../torch/nn (25): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ aten::nonzero 37.38% 84.402ms 0 b (12): forward /mnt/xarfuse/.../torch/nn (25): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ INDEX SCORE 3.32% 7.491ms -119.21 Mb /mnt/xarfuse/.../torch/au (10): forward /mnt/xarfuse/.../torch/nn (25): /mnt/xarfuse/.../IPython/ aten::as_strided 0.20% 441.587us 0 b (12): forward /mnt/xarfuse/.../torch/nn (25): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ aten::nonzero _numpy 0.18% 395.602us 0 b (12): forward /mnt/xarfuse/.../torch/nn (25): /mnt/xarfuse/.../IPython/ /mnt/xarfuse/.../IPython/ -------------- ------------ ------------ ------------ --------------------------------- Self CPU time total: 225.801ms \"\"\" 进一步阅读 我们已经看到了 Profiler 如何用于调查 PyTorch 模型中的时间和内存瓶颈。 在此处阅读有关 Profiler 的更多信息： 事件探查器使用秘籍 分析基于 RPC 的工作负载 Profiler API 文档 脚本的总运行时间：（0 分钟 0.000 秒） 下载 Python 源码：profiler.py 下载 Jupyter 笔记本：profiler.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"53.html":{"url":"53.html","title":"使用 Ray Tune 的超参数调整","keywords":"","body":"使用 Ray Tune 的超参数调整 原文：https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html 超参数调整可以使平均模型与高精度模型有所不同。 通常，选择不同的学习率或更改网络层大小等简单的事情可能会对模型表现产生巨大影响。 幸运的是，有一些工具可以帮助您找到最佳的参数组合。 Ray Tune 是用于分布式超参数调整的行业标准工具。 Ray Tune 包含最新的超参数搜索算法，与 TensorBoard 和其他分析库集成，并通过 Ray 的分布式机器学习引擎本地支持分布式训练。 在本教程中，我们将向您展示如何将 Ray Tune 集成到 PyTorch 训练工作流程中。 我们将扩展 PyTorch 文档的本教程，以训练 CIFAR10 图像分类器。 如您所见，我们只需要添加一些细微的修改即可。 特别是，我们需要 在函数中包装数据加载和训练， 使一些网络参数可配置， 添加检查点（可选）， 并定义用于模型调整的搜索空间 要运行本教程，请确保已安装以下包： ray[tune]：分布式超参数调整库 torchvision：用于数据转换器 设置/导入 让我们从导入开始： from functools import partial import numpy as np import os import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import random_split import torchvision import torchvision.transforms as transforms from ray import tune from ray.tune import CLIReporter from ray.tune.schedulers import ASHAScheduler 建立 PyTorch 模型需要大多数导入产品。 Ray Tune 仅最后三个导入。 数据加载器 我们将数据加载器包装在它们自己的函数中，并传递一个全局数据目录。 这样，我们可以在不同的试验之间共享数据目录。 def load_data(data_dir=\"./data\"): transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) trainset = torchvision.datasets.CIFAR10( root=data_dir, train=True, download=True, transform=transform) testset = torchvision.datasets.CIFAR10( root=data_dir, train=False, download=True, transform=transform) return trainset, testset 可配置的神经网络 我们只能调整那些可配置的参数。 在此示例中，我们可以指定全连接层的层大小： class Net(nn.Module): def __init__(self, l1=120, l2=84): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, l1) self.fc2 = nn.Linear(l1, l2) self.fc3 = nn.Linear(l2, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 训练函数 现在变得有趣了，因为我们对 PyTorch 文档中的示例进行了一些更改。 我们将训练脚本包装在函数train_cifar(config, checkpoint_dir=None, data_dir=None)中。 可以猜到，config参数将接收我们要训练的超参数。 checkpoint_dir参数用于还原检查点。 data_dir指定了我们加载和存储数据的目录，因此多次运行可以共享同一数据源。 net = Net(config[\"l1\"], config[\"l2\"]) if checkpoint_dir: model_state, optimizer_state = torch.load( os.path.join(checkpoint_dir, \"checkpoint\")) net.load_state_dict(model_state) optimizer.load_state_dict(optimizer_state) 优化器的学习率也可以配置： optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9) 我们还将训练数据分为训练和验证子集。 因此，我们训练了 80% 的数据，并计算了其余 20% 的验证损失。 我们遍历训练和测试集的批量大小也是可配置的。 通过DataParallel添加（多）GPU 支持 图像分类主要受益于 GPU。 幸运的是，我们可以继续在 Ray Tune 中使用 PyTorch 的抽象。 因此，我们可以将模型包装在nn.DataParallel中，以支持在多个 GPU 上进行数据并行训练： device = \"cpu\" if torch.cuda.is_available(): device = \"cuda:0\" if torch.cuda.device_count() > 1: net = nn.DataParallel(net) net.to(device) 通过使用device变量，我们可以确保在没有 GPU 的情况下训练也能正常进行。 PyTorch 要求我们将数据显式发送到 GPU 内存，如下所示： for i, data in enumerate(trainloader, 0): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) 该代码现在支持在 CPU，单个 GPU 和多个 GPU 上进行训练。 值得注意的是，Ray 还支持分数 GPU ，因此我们可以在试验之间共享 GPU，只要模型仍适合 GPU 内存即可。 我们稍后再讲。 与 Ray Tune 交流 最有趣的部分是与 Ray Tune 的交流： with tune.checkpoint_dir(epoch) as checkpoint_dir: path = os.path.join(checkpoint_dir, \"checkpoint\") torch.save((net.state_dict(), optimizer.state_dict()), path) tune.report(loss=(val_loss / val_steps), accuracy=correct / total) 在这里，我们首先保存一个检查点，然后将一些指标报告给 Ray Tune。 具体来说，我们将验证损失和准确率发送回 Ray Tune。 然后，Ray Tune 可以使用这些指标来决定哪种超参数配置可以带来最佳结果。 这些指标还可用于尽早停止效果不佳的试验，以避免浪费资源进行试验。 保存检查点是可选的，但是，如果我们想使用高级调度器，例如基于总体的训练，则有必要。 另外，通过保存检查点，我们可以稍后加载经过训练的模型并在测试集上对其进行验证。 完整的训练函数 完整的代码示例如下所示： def train_cifar(config, checkpoint_dir=None, data_dir=None): net = Net(config[\"l1\"], config[\"l2\"]) device = \"cpu\" if torch.cuda.is_available(): device = \"cuda:0\" if torch.cuda.device_count() > 1: net = nn.DataParallel(net) net.to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9) if checkpoint_dir: model_state, optimizer_state = torch.load( os.path.join(checkpoint_dir, \"checkpoint\")) net.load_state_dict(model_state) optimizer.load_state_dict(optimizer_state) trainset, testset = load_data(data_dir) test_abs = int(len(trainset) * 0.8) train_subset, val_subset = random_split( trainset, [test_abs, len(trainset) - test_abs]) trainloader = torch.utils.data.DataLoader( train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8) valloader = torch.utils.data.DataLoader( val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8) for epoch in range(10): # loop over the dataset multiple times running_loss = 0.0 epoch_steps = 0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() epoch_steps += 1 if i % 2000 == 1999: # print every 2000 mini-batches print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps)) running_loss = 0.0 # Validation loss val_loss = 0.0 val_steps = 0 total = 0 correct = 0 for i, data in enumerate(valloader, 0): with torch.no_grad(): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) outputs = net(inputs) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() loss = criterion(outputs, labels) val_loss += loss.cpu().numpy() val_steps += 1 with tune.checkpoint_dir(epoch) as checkpoint_dir: path = os.path.join(checkpoint_dir, \"checkpoint\") torch.save((net.state_dict(), optimizer.state_dict()), path) tune.report(loss=(val_loss / val_steps), accuracy=correct / total) print(\"Finished Training\") 如您所见，大多数代码直接来自原始示例。 测试集准确率 通常，机器学习模型的表现是在保持测试集上使用尚未用于训练模型的数据进行测试的。 我们还将其包装在一个函数中： def test_accuracy(net, device=\"cpu\"): trainset, testset = load_data() testloader = torch.utils.data.DataLoader( testset, batch_size=4, shuffle=False, num_workers=2) correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data images, labels = images.to(device), labels.to(device) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() return correct / total 该函数还需要一个device参数，因此我们可以在 GPU 上进行测试集验证。 配置搜索空间 最后，我们需要定义 Ray Tune 的搜索空间。 这是一个例子： config = { \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)), \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)), \"lr\": tune.loguniform(1e-4, 1e-1), \"batch_size\": tune.choice([2, 4, 8, 16]) } tune.sample_from()函数使您可以定义自己的采样方法以获得超参数。 在此示例中，l1和l2参数应为 4 到 256 之间的 2 的幂，因此应为 4、8、16、32、64、128 或 256。lr（学习率）应在 0.0001 和 0.1 之间均匀采样。 最后，批量大小可以在 2、4、8 和 16 之间选择。 现在，在每次试用中，Ray Tune 都会从这些搜索空间中随机抽取参数组合。 然后它将并行训练许多模型，并在其中找到表现最佳的模型。 我们还使用ASHAScheduler，它将尽早终止效果不佳的测试。 我们用functools.partial包装train_cifar函数以设置常量data_dir参数。 我们还可以告诉 Ray Tune 每个审判应提供哪些资源： gpus_per_trial = 2 # ... result = tune.run( partial(train_cifar, data_dir=data_dir), resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial}, config=config, num_samples=num_samples, scheduler=scheduler, progress_reporter=reporter, checkpoint_at_end=True) 您可以指定 CPU 的数量，例如增加 PyTorch DataLoader实例的num_workers。 在每次试用中，选定数量的 GPU 对 PyTorch 都是可见的。 试用版无法访问未要求使用 GPU 的 GPU，因此您不必担心使用同一组资源进行两次试用。 在这里，我们还可以指定分数 GPU，因此gpus_per_trial=0.5之类的东西完全有效。 然后，试用版将彼此共享 GPU。 您只需要确保模型仍然适合 GPU 内存即可。 训练完模型后，我们将找到表现最好的模型，并从检查点文件中加载训练后的网络。 然后，我们获得测试仪的准确率，并通过打印报告一切。 完整的main函数如下： def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2): data_dir = os.path.abspath(\"./data\") load_data(data_dir) config = { \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)), \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)), \"lr\": tune.loguniform(1e-4, 1e-1), \"batch_size\": tune.choice([2, 4, 8, 16]) } scheduler = ASHAScheduler( metric=\"loss\", mode=\"min\", max_t=max_num_epochs, grace_period=1, reduction_factor=2) reporter = CLIReporter( # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"], metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"]) result = tune.run( partial(train_cifar, data_dir=data_dir), resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial}, config=config, num_samples=num_samples, scheduler=scheduler, progress_reporter=reporter) best_trial = result.get_best_trial(\"loss\", \"min\", \"last\") print(\"Best trial config: {}\".format(best_trial.config)) print(\"Best trial final validation loss: {}\".format( best_trial.last_result[\"loss\"])) print(\"Best trial final validation accuracy: {}\".format( best_trial.last_result[\"accuracy\"])) best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"]) device = \"cpu\" if torch.cuda.is_available(): device = \"cuda:0\" if gpus_per_trial > 1: best_trained_model = nn.DataParallel(best_trained_model) best_trained_model.to(device) best_checkpoint_dir = best_trial.checkpoint.value model_state, optimizer_state = torch.load(os.path.join( best_checkpoint_dir, \"checkpoint\")) best_trained_model.load_state_dict(model_state) test_acc = test_accuracy(best_trained_model, device) print(\"Best trial test set accuracy: {}\".format(test_acc)) if __name__ == \"__main__\": # You can change the number of GPUs per trial here: main(num_samples=10, max_num_epochs=10, gpus_per_trial=0) 出： Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz Extracting /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data Files already downloaded and verified == Status == Memory usage on this node: 4.0/240.1 GiB Using AsyncHyperBand: num_stopped=0 Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None Resources requested: 2/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 1/10 (1 RUNNING) +---------------------+----------+-------+--------------+------+------+-------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | |---------------------+----------+-------+--------------+------+------+-------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | +---------------------+----------+-------+--------------+------+------+-------------+ [2m[36m(pid=1588)[0m Files already downloaded and verified [2m[36m(pid=1568)[0m Files already downloaded and verified [2m[36m(pid=1504)[0m Files already downloaded and verified [2m[36m(pid=1575)[0m Files already downloaded and verified [2m[36m(pid=1494)[0m Files already downloaded and verified [2m[36m(pid=1572)[0m Files already downloaded and verified [2m[36m(pid=1567)[0m Files already downloaded and verified [2m[36m(pid=1585)[0m Files already downloaded and verified [2m[36m(pid=1565)[0m Files already downloaded and verified [2m[36m(pid=1505)[0m Files already downloaded and verified [2m[36m(pid=1588)[0m Files already downloaded and verified [2m[36m(pid=1568)[0m Files already downloaded and verified [2m[36m(pid=1504)[0m Files already downloaded and verified [2m[36m(pid=1575)[0m Files already downloaded and verified [2m[36m(pid=1494)[0m Files already downloaded and verified [2m[36m(pid=1572)[0m Files already downloaded and verified [2m[36m(pid=1567)[0m Files already downloaded and verified [2m[36m(pid=1565)[0m Files already downloaded and verified [2m[36m(pid=1585)[0m Files already downloaded and verified [2m[36m(pid=1505)[0m Files already downloaded and verified [2m[36m(pid=1585)[0m [1, 2000] loss: 2.307 [2m[36m(pid=1568)[0m [1, 2000] loss: 2.226 [2m[36m(pid=1565)[0m [1, 2000] loss: 2.141 [2m[36m(pid=1505)[0m [1, 2000] loss: 2.339 [2m[36m(pid=1504)[0m [1, 2000] loss: 2.042 [2m[36m(pid=1572)[0m [1, 2000] loss: 2.288 [2m[36m(pid=1567)[0m [1, 2000] loss: 2.047 [2m[36m(pid=1575)[0m [1, 2000] loss: 2.316 [2m[36m(pid=1494)[0m [1, 2000] loss: 2.322 [2m[36m(pid=1588)[0m [1, 2000] loss: 2.289 [2m[36m(pid=1585)[0m [1, 4000] loss: 1.154 [2m[36m(pid=1505)[0m [1, 4000] loss: 1.170 [2m[36m(pid=1565)[0m [1, 4000] loss: 0.939 [2m[36m(pid=1568)[0m [1, 4000] loss: 1.102 [2m[36m(pid=1504)[0m [1, 4000] loss: 0.916 [2m[36m(pid=1572)[0m [1, 4000] loss: 1.156 Result for DEFAULT_d3304_00003: accuracy: 0.226 date: 2021-01-05_20-23-37 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 1 loss: 2.083958268547058 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 27.169169902801514 time_this_iter_s: 27.169169902801514 time_total_s: 27.169169902801514 timestamp: 1609878217 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00003 == Status == Memory usage on this node: 9.2/240.1 GiB Using AsyncHyperBand: num_stopped=0 Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.083958268547058 Resources requested: 20/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (10 RUNNING) +---------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | | | | | DEFAULT_d3304_00001 | RUNNING | | 8 | 16 | 32 | 0.077467 | | | | | DEFAULT_d3304_00002 | RUNNING | | 4 | 8 | 128 | 0.00436986 | | | | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 2.08396 | 0.226 | 1 | | DEFAULT_d3304_00004 | RUNNING | | 4 | 16 | 32 | 0.016474 | | | | | DEFAULT_d3304_00005 | RUNNING | | 4 | 128 | 64 | 0.00757252 | | | | | DEFAULT_d3304_00006 | RUNNING | | 2 | 64 | 256 | 0.00177236 | | | | | DEFAULT_d3304_00007 | RUNNING | | 8 | 8 | 8 | 0.000155891 | | | | | DEFAULT_d3304_00008 | RUNNING | | 2 | 16 | 64 | 0.0310199 | | | | | DEFAULT_d3304_00009 | RUNNING | | 4 | 4 | 32 | 0.0175239 | | | | +---------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1567)[0m [1, 4000] loss: 0.943 [2m[36m(pid=1494)[0m [1, 4000] loss: 1.155 [2m[36m(pid=1575)[0m [1, 4000] loss: 1.162 [2m[36m(pid=1585)[0m [1, 6000] loss: 0.768 [2m[36m(pid=1505)[0m [1, 6000] loss: 0.780 [2m[36m(pid=1565)[0m [1, 6000] loss: 0.582 [2m[36m(pid=1504)[0m [1, 6000] loss: 0.587 [2m[36m(pid=1568)[0m [1, 6000] loss: 0.770 [2m[36m(pid=1572)[0m [1, 6000] loss: 0.771 [2m[36m(pid=1567)[0m [1, 6000] loss: 0.615 Result for DEFAULT_d3304_00007: accuracy: 0.1011 date: 2021-01-05_20-23-51 done: true experiment_id: 947614a8c2a74533be128b929f363bd1 experiment_tag: 7_batch_size=8,l1=8,l2=8,lr=0.00015589 hostname: 1a844a452371 iterations_since_restore: 1 loss: 2.3038805620193483 node_ip: 172.17.0.2 pid: 1494 should_checkpoint: true time_since_restore: 41.69914960861206 time_this_iter_s: 41.69914960861206 time_total_s: 41.69914960861206 timestamp: 1609878231 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00007 == Status == Memory usage on this node: 9.1/240.1 GiB Using AsyncHyperBand: num_stopped=1 Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.193919415283203 Resources requested: 20/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (10 RUNNING) +---------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | | | | | DEFAULT_d3304_00001 | RUNNING | | 8 | 16 | 32 | 0.077467 | | | | | DEFAULT_d3304_00002 | RUNNING | | 4 | 8 | 128 | 0.00436986 | | | | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 2.08396 | 0.226 | 1 | | DEFAULT_d3304_00004 | RUNNING | | 4 | 16 | 32 | 0.016474 | | | | | DEFAULT_d3304_00005 | RUNNING | | 4 | 128 | 64 | 0.00757252 | | | | | DEFAULT_d3304_00006 | RUNNING | | 2 | 64 | 256 | 0.00177236 | | | | | DEFAULT_d3304_00007 | RUNNING | 172.17.0.2:1494 | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | RUNNING | | 2 | 16 | 64 | 0.0310199 | | | | | DEFAULT_d3304_00009 | RUNNING | | 4 | 4 | 32 | 0.0175239 | | | | +---------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ Result for DEFAULT_d3304_00001: accuracy: 0.1017 date: 2021-01-05_20-23-51 done: true experiment_id: 26ac228b4b454584869f8490742cf253 experiment_tag: 1_batch_size=8,l1=16,l2=32,lr=0.077467 hostname: 1a844a452371 iterations_since_restore: 1 loss: 2.321864831352234 node_ip: 172.17.0.2 pid: 1575 should_checkpoint: true time_since_restore: 42.09821367263794 time_this_iter_s: 42.09821367263794 time_total_s: 42.09821367263794 timestamp: 1609878231 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00001 [2m[36m(pid=1588)[0m [2, 2000] loss: 1.916 [2m[36m(pid=1585)[0m [1, 8000] loss: 0.576 [2m[36m(pid=1505)[0m [1, 8000] loss: 0.584 [2m[36m(pid=1565)[0m [1, 8000] loss: 0.422 [2m[36m(pid=1504)[0m [1, 8000] loss: 0.433 [2m[36m(pid=1572)[0m [1, 8000] loss: 0.578 [2m[36m(pid=1568)[0m [1, 8000] loss: 0.580 Result for DEFAULT_d3304_00003: accuracy: 0.3762 date: 2021-01-05_20-24-00 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 2 loss: 1.7041921138763427 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 50.74612545967102 time_this_iter_s: 23.576955556869507 time_total_s: 50.74612545967102 timestamp: 1609878240 timesteps_since_restore: 0 training_iteration: 2 trial_id: d3304_00003 == Status == Memory usage on this node: 8.0/240.1 GiB Using AsyncHyperBand: num_stopped=2 Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.3038805620193483 Resources requested: 16/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (8 RUNNING, 2 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | | | | | DEFAULT_d3304_00002 | RUNNING | | 4 | 8 | 128 | 0.00436986 | | | | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.70419 | 0.3762 | 2 | | DEFAULT_d3304_00004 | RUNNING | | 4 | 16 | 32 | 0.016474 | | | | | DEFAULT_d3304_00005 | RUNNING | | 4 | 128 | 64 | 0.00757252 | | | | | DEFAULT_d3304_00006 | RUNNING | | 2 | 64 | 256 | 0.00177236 | | | | | DEFAULT_d3304_00008 | RUNNING | | 2 | 16 | 64 | 0.0310199 | | | | | DEFAULT_d3304_00009 | RUNNING | | 4 | 4 | 32 | 0.0175239 | | | | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1567)[0m [1, 8000] loss: 0.458 [2m[36m(pid=1585)[0m [1, 10000] loss: 0.461 [2m[36m(pid=1505)[0m [1, 10000] loss: 0.467 [2m[36m(pid=1565)[0m [1, 10000] loss: 0.329 [2m[36m(pid=1504)[0m [1, 10000] loss: 0.344 [2m[36m(pid=1572)[0m [1, 10000] loss: 0.463 [2m[36m(pid=1568)[0m [1, 10000] loss: 0.464 [2m[36m(pid=1567)[0m [1, 10000] loss: 0.360 [2m[36m(pid=1588)[0m [3, 2000] loss: 1.663 Result for DEFAULT_d3304_00002: accuracy: 0.3791 date: 2021-01-05_20-24-18 done: false experiment_id: eaf4d25c9a0e46219afb226ed323095b experiment_tag: 2_batch_size=4,l1=8,l2=128,lr=0.0043699 hostname: 1a844a452371 iterations_since_restore: 1 loss: 1.6690538251161575 node_ip: 172.17.0.2 pid: 1504 should_checkpoint: true time_since_restore: 68.1856791973114 time_this_iter_s: 68.1856791973114 time_total_s: 68.1856791973114 timestamp: 1609878258 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00002 == Status == Memory usage on this node: 8.0/240.1 GiB Using AsyncHyperBand: num_stopped=2 Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.193919415283203 Resources requested: 16/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (8 RUNNING, 2 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | | | | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.66905 | 0.3791 | 1 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.70419 | 0.3762 | 2 | | DEFAULT_d3304_00004 | RUNNING | | 4 | 16 | 32 | 0.016474 | | | | | DEFAULT_d3304_00005 | RUNNING | | 4 | 128 | 64 | 0.00757252 | | | | | DEFAULT_d3304_00006 | RUNNING | | 2 | 64 | 256 | 0.00177236 | | | | | DEFAULT_d3304_00008 | RUNNING | | 2 | 16 | 64 | 0.0310199 | | | | | DEFAULT_d3304_00009 | RUNNING | | 4 | 4 | 32 | 0.0175239 | | | | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1585)[0m [1, 12000] loss: 0.384 [2m[36m(pid=1505)[0m [1, 12000] loss: 0.390 Result for DEFAULT_d3304_00009: accuracy: 0.101 date: 2021-01-05_20-24-19 done: true experiment_id: 471eb6134c2a45509b005af46861c602 experiment_tag: 9_batch_size=4,l1=4,l2=32,lr=0.017524 hostname: 1a844a452371 iterations_since_restore: 1 loss: 2.310983589553833 node_ip: 172.17.0.2 pid: 1572 should_checkpoint: true time_since_restore: 69.29919123649597 time_this_iter_s: 69.29919123649597 time_total_s: 69.29919123649597 timestamp: 1609878259 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00009 Result for DEFAULT_d3304_00004: accuracy: 0.102 date: 2021-01-05_20-24-19 done: true experiment_id: bd1f438c1fdd4a9ba98074d1cfd573fe experiment_tag: 4_batch_size=4,l1=16,l2=32,lr=0.016474 hostname: 1a844a452371 iterations_since_restore: 1 loss: 2.313420217037201 node_ip: 172.17.0.2 pid: 1568 should_checkpoint: true time_since_restore: 69.48366618156433 time_this_iter_s: 69.48366618156433 time_total_s: 69.48366618156433 timestamp: 1609878259 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00004 [2m[36m(pid=1565)[0m [1, 12000] loss: 0.267 Result for DEFAULT_d3304_00005: accuracy: 0.3301 date: 2021-01-05_20-24-22 done: false experiment_id: 738b3d315db548a7956646b2c07f1b0c experiment_tag: 5_batch_size=4,l1=128,l2=64,lr=0.0075725 hostname: 1a844a452371 iterations_since_restore: 1 loss: 1.8058318739891053 node_ip: 172.17.0.2 pid: 1567 should_checkpoint: true time_since_restore: 72.0806794166565 time_this_iter_s: 72.0806794166565 time_total_s: 72.0806794166565 timestamp: 1609878262 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00005 Result for DEFAULT_d3304_00003: accuracy: 0.4242 date: 2021-01-05_20-24-23 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 3 loss: 1.5498835063934326 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 73.29849410057068 time_this_iter_s: 22.552368640899658 time_total_s: 73.29849410057068 timestamp: 1609878263 timesteps_since_restore: 0 training_iteration: 3 trial_id: d3304_00003 == Status == Memory usage on this node: 6.9/240.1 GiB Using AsyncHyperBand: num_stopped=4 Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.3038805620193483 Resources requested: 12/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (6 RUNNING, 4 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | | | | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.66905 | 0.3791 | 1 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.54988 | 0.4242 | 3 | | DEFAULT_d3304_00005 | RUNNING | 172.17.0.2:1567 | 4 | 128 | 64 | 0.00757252 | 1.80583 | 0.3301 | 1 | | DEFAULT_d3304_00006 | RUNNING | | 2 | 64 | 256 | 0.00177236 | | | | | DEFAULT_d3304_00008 | RUNNING | | 2 | 16 | 64 | 0.0310199 | | | | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1585)[0m [1, 14000] loss: 0.329 [2m[36m(pid=1504)[0m [2, 2000] loss: 1.708 [2m[36m(pid=1565)[0m [1, 14000] loss: 0.225 [2m[36m(pid=1505)[0m [1, 14000] loss: 0.334 [2m[36m(pid=1567)[0m [2, 2000] loss: 1.803 [2m[36m(pid=1585)[0m [1, 16000] loss: 0.288 [2m[36m(pid=1588)[0m [4, 2000] loss: 1.541 [2m[36m(pid=1504)[0m [2, 4000] loss: 0.840 [2m[36m(pid=1565)[0m [1, 16000] loss: 0.198 [2m[36m(pid=1505)[0m [1, 16000] loss: 0.292 [2m[36m(pid=1567)[0m [2, 4000] loss: 0.912 Result for DEFAULT_d3304_00003: accuracy: 0.4494 date: 2021-01-05_20-24-44 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 4 loss: 1.4720179980278014 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 94.81268787384033 time_this_iter_s: 21.514193773269653 time_total_s: 94.81268787384033 timestamp: 1609878284 timesteps_since_restore: 0 training_iteration: 4 trial_id: d3304_00003 == Status == Memory usage on this node: 6.9/240.1 GiB Using AsyncHyperBand: num_stopped=4 Bracket: Iter 8.000: None | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.3038805620193483 Resources requested: 12/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (6 RUNNING, 4 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | | | | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.66905 | 0.3791 | 1 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.47202 | 0.4494 | 4 | | DEFAULT_d3304_00005 | RUNNING | 172.17.0.2:1567 | 4 | 128 | 64 | 0.00757252 | 1.80583 | 0.3301 | 1 | | DEFAULT_d3304_00006 | RUNNING | | 2 | 64 | 256 | 0.00177236 | | | | | DEFAULT_d3304_00008 | RUNNING | | 2 | 16 | 64 | 0.0310199 | | | | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1585)[0m [1, 18000] loss: 0.256 [2m[36m(pid=1565)[0m [1, 18000] loss: 0.173 [2m[36m(pid=1504)[0m [2, 6000] loss: 0.572 [2m[36m(pid=1505)[0m [1, 18000] loss: 0.259 [2m[36m(pid=1567)[0m [2, 6000] loss: 0.611 [2m[36m(pid=1585)[0m [1, 20000] loss: 0.230 [2m[36m(pid=1565)[0m [1, 20000] loss: 0.156 [2m[36m(pid=1505)[0m [1, 20000] loss: 0.234 [2m[36m(pid=1504)[0m [2, 8000] loss: 0.417 [2m[36m(pid=1588)[0m [5, 2000] loss: 1.452 [2m[36m(pid=1567)[0m [2, 8000] loss: 0.461 Result for DEFAULT_d3304_00003: accuracy: 0.4839 date: 2021-01-05_20-25-06 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 5 loss: 1.4083827662467956 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 116.5817449092865 time_this_iter_s: 21.769057035446167 time_total_s: 116.5817449092865 timestamp: 1609878306 timesteps_since_restore: 0 training_iteration: 5 trial_id: d3304_00003 == Status == Memory usage on this node: 6.9/240.1 GiB Using AsyncHyperBand: num_stopped=4 Bracket: Iter 8.000: None | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.3038805620193483 Resources requested: 12/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (6 RUNNING, 4 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | | 2 | 4 | 16 | 0.000111924 | | | | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.66905 | 0.3791 | 1 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.40838 | 0.4839 | 5 | | DEFAULT_d3304_00005 | RUNNING | 172.17.0.2:1567 | 4 | 128 | 64 | 0.00757252 | 1.80583 | 0.3301 | 1 | | DEFAULT_d3304_00006 | RUNNING | | 2 | 64 | 256 | 0.00177236 | | | | | DEFAULT_d3304_00008 | RUNNING | | 2 | 16 | 64 | 0.0310199 | | | | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1504)[0m [2, 10000] loss: 0.339 Result for DEFAULT_d3304_00000: accuracy: 0.1104 date: 2021-01-05_20-25-10 done: false experiment_id: 454624d453954d46b33a1eb496e7ec53 experiment_tag: 0_batch_size=2,l1=4,l2=16,lr=0.00011192 hostname: 1a844a452371 iterations_since_restore: 1 loss: 2.2988875378131866 node_ip: 172.17.0.2 pid: 1585 should_checkpoint: true time_since_restore: 120.59520411491394 time_this_iter_s: 120.59520411491394 time_total_s: 120.59520411491394 timestamp: 1609878310 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00000 Result for DEFAULT_d3304_00008: accuracy: 0.0983 date: 2021-01-05_20-25-11 done: true experiment_id: 381603b190bc47a9b794321f7692695f experiment_tag: 8_batch_size=2,l1=16,l2=64,lr=0.03102 hostname: 1a844a452371 iterations_since_restore: 1 loss: 2.336980807876587 node_ip: 172.17.0.2 pid: 1505 should_checkpoint: true time_since_restore: 121.36707901954651 time_this_iter_s: 121.36707901954651 time_total_s: 121.36707901954651 timestamp: 1609878311 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00008 Result for DEFAULT_d3304_00006: accuracy: 0.4586 date: 2021-01-05_20-25-11 done: false experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 1 loss: 1.5124113649010658 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 121.536208152771 time_this_iter_s: 121.536208152771 time_total_s: 121.536208152771 timestamp: 1609878311 timesteps_since_restore: 0 training_iteration: 1 trial_id: d3304_00006 == Status == Memory usage on this node: 6.6/240.1 GiB Using AsyncHyperBand: num_stopped=5 Bracket: Iter 8.000: None | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 10/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (5 RUNNING, 5 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | 172.17.0.2:1585 | 2 | 4 | 16 | 0.000111924 | 2.29889 | 0.1104 | 1 | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.66905 | 0.3791 | 1 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.40838 | 0.4839 | 5 | | DEFAULT_d3304_00005 | RUNNING | 172.17.0.2:1567 | 4 | 128 | 64 | 0.00757252 | 1.80583 | 0.3301 | 1 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.51241 | 0.4586 | 1 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ Result for DEFAULT_d3304_00002: accuracy: 0.4078 date: 2021-01-05_20-25-16 done: false experiment_id: eaf4d25c9a0e46219afb226ed323095b experiment_tag: 2_batch_size=4,l1=8,l2=128,lr=0.0043699 hostname: 1a844a452371 iterations_since_restore: 2 loss: 1.6191314194440842 node_ip: 172.17.0.2 pid: 1504 should_checkpoint: true time_since_restore: 126.61185264587402 time_this_iter_s: 58.42617344856262 time_total_s: 126.61185264587402 timestamp: 1609878316 timesteps_since_restore: 0 training_iteration: 2 trial_id: d3304_00002 [2m[36m(pid=1567)[0m [2, 10000] loss: 0.371 [2m[36m(pid=1585)[0m [2, 2000] loss: 2.298 [2m[36m(pid=1565)[0m [2, 2000] loss: 1.466 [2m[36m(pid=1588)[0m [6, 2000] loss: 1.383 Result for DEFAULT_d3304_00005: accuracy: 0.3647 date: 2021-01-05_20-25-24 done: true experiment_id: 738b3d315db548a7956646b2c07f1b0c experiment_tag: 5_batch_size=4,l1=128,l2=64,lr=0.0075725 hostname: 1a844a452371 iterations_since_restore: 2 loss: 1.7739140236496926 node_ip: 172.17.0.2 pid: 1567 should_checkpoint: true time_since_restore: 134.1462869644165 time_this_iter_s: 62.06560754776001 time_total_s: 134.1462869644165 timestamp: 1609878324 timesteps_since_restore: 0 training_iteration: 2 trial_id: d3304_00005 == Status == Memory usage on this node: 6.3/240.1 GiB Using AsyncHyperBand: num_stopped=6 Bracket: Iter 8.000: None | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 10/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (5 RUNNING, 5 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | 172.17.0.2:1585 | 2 | 4 | 16 | 0.000111924 | 2.29889 | 0.1104 | 1 | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.61913 | 0.4078 | 2 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.40838 | 0.4839 | 5 | | DEFAULT_d3304_00005 | RUNNING | 172.17.0.2:1567 | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.51241 | 0.4586 | 1 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1504)[0m [3, 2000] loss: 1.656 Result for DEFAULT_d3304_00003: accuracy: 0.5061 date: 2021-01-05_20-25-27 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 6 loss: 1.3623717227935792 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 137.95851016044617 time_this_iter_s: 21.376765251159668 time_total_s: 137.95851016044617 timestamp: 1609878327 timesteps_since_restore: 0 training_iteration: 6 trial_id: d3304_00003 [2m[36m(pid=1585)[0m [2, 4000] loss: 1.147 [2m[36m(pid=1565)[0m [2, 4000] loss: 0.749 [2m[36m(pid=1504)[0m [3, 4000] loss: 0.838 [2m[36m(pid=1585)[0m [2, 6000] loss: 0.760 [2m[36m(pid=1565)[0m [2, 6000] loss: 0.498 [2m[36m(pid=1588)[0m [7, 2000] loss: 1.326 [2m[36m(pid=1504)[0m [3, 6000] loss: 0.560 [2m[36m(pid=1585)[0m [2, 8000] loss: 0.561 Result for DEFAULT_d3304_00003: accuracy: 0.5209 date: 2021-01-05_20-25-48 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 7 loss: 1.316757419013977 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 158.4953932762146 time_this_iter_s: 20.536883115768433 time_total_s: 158.4953932762146 timestamp: 1609878348 timesteps_since_restore: 0 training_iteration: 7 trial_id: d3304_00003 == Status == Memory usage on this node: 5.8/240.1 GiB Using AsyncHyperBand: num_stopped=6 Bracket: Iter 8.000: None | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 8/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (4 RUNNING, 6 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | 172.17.0.2:1585 | 2 | 4 | 16 | 0.000111924 | 2.29889 | 0.1104 | 1 | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.61913 | 0.4078 | 2 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.31676 | 0.5209 | 7 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.51241 | 0.4586 | 1 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1565)[0m [2, 8000] loss: 0.372 [2m[36m(pid=1504)[0m [3, 8000] loss: 0.416 [2m[36m(pid=1585)[0m [2, 10000] loss: 0.434 [2m[36m(pid=1565)[0m [2, 10000] loss: 0.292 [2m[36m(pid=1588)[0m [8, 2000] loss: 1.278 [2m[36m(pid=1504)[0m [3, 10000] loss: 0.333 [2m[36m(pid=1585)[0m [2, 12000] loss: 0.347 [2m[36m(pid=1565)[0m [2, 12000] loss: 0.245 Result for DEFAULT_d3304_00003: accuracy: 0.5406 date: 2021-01-05_20-26-08 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 8 loss: 1.267511115884781 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 179.13841199874878 time_this_iter_s: 20.64301872253418 time_total_s: 179.13841199874878 timestamp: 1609878368 timesteps_since_restore: 0 training_iteration: 8 trial_id: d3304_00003 == Status == Memory usage on this node: 5.8/240.1 GiB Using AsyncHyperBand: num_stopped=6 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 8/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (4 RUNNING, 6 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | 172.17.0.2:1585 | 2 | 4 | 16 | 0.000111924 | 2.29889 | 0.1104 | 1 | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.61913 | 0.4078 | 2 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.26751 | 0.5406 | 8 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.51241 | 0.4586 | 1 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ Result for DEFAULT_d3304_00002: accuracy: 0.3997 date: 2021-01-05_20-26-11 done: false experiment_id: eaf4d25c9a0e46219afb226ed323095b experiment_tag: 2_batch_size=4,l1=8,l2=128,lr=0.0043699 hostname: 1a844a452371 iterations_since_restore: 3 loss: 1.7084122330278158 node_ip: 172.17.0.2 pid: 1504 should_checkpoint: true time_since_restore: 182.02509140968323 time_this_iter_s: 55.413238763809204 time_total_s: 182.02509140968323 timestamp: 1609878371 timesteps_since_restore: 0 training_iteration: 3 trial_id: d3304_00002 [2m[36m(pid=1585)[0m [2, 14000] loss: 0.290 [2m[36m(pid=1565)[0m [2, 14000] loss: 0.213 [2m[36m(pid=1504)[0m [4, 2000] loss: 1.653 [2m[36m(pid=1588)[0m [9, 2000] loss: 1.245 [2m[36m(pid=1585)[0m [2, 16000] loss: 0.244 [2m[36m(pid=1565)[0m [2, 16000] loss: 0.186 Result for DEFAULT_d3304_00003: accuracy: 0.5409 date: 2021-01-05_20-26-29 done: false experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 9 loss: 1.2721123942375183 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 199.56540870666504 time_this_iter_s: 20.42699670791626 time_total_s: 199.56540870666504 timestamp: 1609878389 timesteps_since_restore: 0 training_iteration: 9 trial_id: d3304_00003 == Status == Memory usage on this node: 5.8/240.1 GiB Using AsyncHyperBand: num_stopped=6 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 8/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (4 RUNNING, 6 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | 172.17.0.2:1585 | 2 | 4 | 16 | 0.000111924 | 2.29889 | 0.1104 | 1 | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.70841 | 0.3997 | 3 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.27211 | 0.5409 | 9 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.51241 | 0.4586 | 1 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1504)[0m [4, 4000] loss: 0.842 [2m[36m(pid=1585)[0m [2, 18000] loss: 0.214 [2m[36m(pid=1565)[0m [2, 18000] loss: 0.159 [2m[36m(pid=1504)[0m [4, 6000] loss: 0.561 [2m[36m(pid=1585)[0m [2, 20000] loss: 0.191 [2m[36m(pid=1588)[0m [10, 2000] loss: 1.210 [2m[36m(pid=1565)[0m [2, 20000] loss: 0.143 Result for DEFAULT_d3304_00003: accuracy: 0.5619 date: 2021-01-05_20-26-50 done: true experiment_id: d4b00469893d498ea65a729df202882a experiment_tag: 3_batch_size=16,l1=32,l2=4,lr=0.0012023 hostname: 1a844a452371 iterations_since_restore: 10 loss: 1.2222298237800597 node_ip: 172.17.0.2 pid: 1588 should_checkpoint: true time_since_restore: 220.31984639167786 time_this_iter_s: 20.754437685012817 time_total_s: 220.31984639167786 timestamp: 1609878410 timesteps_since_restore: 0 training_iteration: 10 trial_id: d3304_00003 == Status == Memory usage on this node: 5.8/240.1 GiB Using AsyncHyperBand: num_stopped=7 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 8/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (4 RUNNING, 6 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | 172.17.0.2:1585 | 2 | 4 | 16 | 0.000111924 | 2.29889 | 0.1104 | 1 | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.70841 | 0.3997 | 3 | | DEFAULT_d3304_00003 | RUNNING | 172.17.0.2:1588 | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.51241 | 0.4586 | 1 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1504)[0m [4, 8000] loss: 0.422 Result for DEFAULT_d3304_00000: accuracy: 0.2724 date: 2021-01-05_20-26-55 done: true experiment_id: 454624d453954d46b33a1eb496e7ec53 experiment_tag: 0_batch_size=2,l1=4,l2=16,lr=0.00011192 hostname: 1a844a452371 iterations_since_restore: 2 loss: 1.8605026947617531 node_ip: 172.17.0.2 pid: 1585 should_checkpoint: true time_since_restore: 225.84529209136963 time_this_iter_s: 105.25008797645569 time_total_s: 225.84529209136963 timestamp: 1609878415 timesteps_since_restore: 0 training_iteration: 2 trial_id: d3304_00000 == Status == Memory usage on this node: 5.3/240.1 GiB Using AsyncHyperBand: num_stopped=8 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4720179980278014 | Iter 2.000: -1.7390530687630177 | Iter 1.000: -2.301384049916267 Resources requested: 6/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (3 RUNNING, 7 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | RUNNING | 172.17.0.2:1585 | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.70841 | 0.3997 | 3 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.51241 | 0.4586 | 1 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ Result for DEFAULT_d3304_00006: accuracy: 0.5007 date: 2021-01-05_20-26-57 done: false experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 2 loss: 1.3979384284215048 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 227.80454421043396 time_this_iter_s: 106.26833605766296 time_total_s: 227.80454421043396 timestamp: 1609878417 timesteps_since_restore: 0 training_iteration: 2 trial_id: d3304_00006 [2m[36m(pid=1504)[0m [4, 10000] loss: 0.335 Result for DEFAULT_d3304_00002: accuracy: 0.3849 date: 2021-01-05_20-27-06 done: true experiment_id: eaf4d25c9a0e46219afb226ed323095b experiment_tag: 2_batch_size=4,l1=8,l2=128,lr=0.0043699 hostname: 1a844a452371 iterations_since_restore: 4 loss: 1.720731588792801 node_ip: 172.17.0.2 pid: 1504 should_checkpoint: true time_since_restore: 236.71593952178955 time_this_iter_s: 54.69084811210632 time_total_s: 236.71593952178955 timestamp: 1609878426 timesteps_since_restore: 0 training_iteration: 4 trial_id: d3304_00002 == Status == Memory usage on this node: 4.7/240.1 GiB Using AsyncHyperBand: num_stopped=9 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.5963747934103012 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 4/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (2 RUNNING, 8 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00002 | RUNNING | 172.17.0.2:1504 | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.39794 | 0.5007 | 2 | | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1565)[0m [3, 2000] loss: 1.373 [2m[36m(pid=1565)[0m [3, 4000] loss: 0.696 [2m[36m(pid=1565)[0m [3, 6000] loss: 0.466 [2m[36m(pid=1565)[0m [3, 8000] loss: 0.357 [2m[36m(pid=1565)[0m [3, 10000] loss: 0.283 [2m[36m(pid=1565)[0m [3, 12000] loss: 0.241 [2m[36m(pid=1565)[0m [3, 14000] loss: 0.203 [2m[36m(pid=1565)[0m [3, 16000] loss: 0.178 [2m[36m(pid=1565)[0m [3, 18000] loss: 0.160 [2m[36m(pid=1565)[0m [3, 20000] loss: 0.142 Result for DEFAULT_d3304_00006: accuracy: 0.5095 date: 2021-01-05_20-28-36 done: false experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 3 loss: 1.4272501501079649 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 326.1525847911835 time_this_iter_s: 98.34804058074951 time_total_s: 326.1525847911835 timestamp: 1609878516 timesteps_since_restore: 0 training_iteration: 3 trial_id: d3304_00006 == Status == Memory usage on this node: 4.2/240.1 GiB Using AsyncHyperBand: num_stopped=9 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.5963747934103012 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 2/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (1 RUNNING, 9 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.42725 | 0.5095 | 3 | | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00002 | TERMINATED | | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1565)[0m [4, 2000] loss: 1.320 [2m[36m(pid=1565)[0m [4, 4000] loss: 0.701 [2m[36m(pid=1565)[0m [4, 6000] loss: 0.454 [2m[36m(pid=1565)[0m [4, 8000] loss: 0.345 [2m[36m(pid=1565)[0m [4, 10000] loss: 0.276 [2m[36m(pid=1565)[0m [4, 12000] loss: 0.234 [2m[36m(pid=1565)[0m [4, 14000] loss: 0.199 [2m[36m(pid=1565)[0m [4, 16000] loss: 0.170 [2m[36m(pid=1565)[0m [4, 18000] loss: 0.151 [2m[36m(pid=1565)[0m [4, 20000] loss: 0.144 Result for DEFAULT_d3304_00006: accuracy: 0.4749 date: 2021-01-05_20-30-15 done: false experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 4 loss: 1.4950430885698218 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 425.3827154636383 time_this_iter_s: 99.23013067245483 time_total_s: 425.3827154636383 timestamp: 1609878615 timesteps_since_restore: 0 training_iteration: 4 trial_id: d3304_00006 == Status == Memory usage on this node: 4.1/240.1 GiB Using AsyncHyperBand: num_stopped=9 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4950430885698218 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 2/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (1 RUNNING, 9 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.49504 | 0.4749 | 4 | | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00002 | TERMINATED | | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1565)[0m [5, 2000] loss: 1.314 [2m[36m(pid=1565)[0m [5, 4000] loss: 0.663 [2m[36m(pid=1565)[0m [5, 6000] loss: 0.453 [2m[36m(pid=1565)[0m [5, 8000] loss: 0.341 [2m[36m(pid=1565)[0m [5, 10000] loss: 0.278 [2m[36m(pid=1565)[0m [5, 12000] loss: 0.235 [2m[36m(pid=1565)[0m [5, 14000] loss: 0.197 [2m[36m(pid=1565)[0m [5, 16000] loss: 0.173 [2m[36m(pid=1565)[0m [5, 18000] loss: 0.155 [2m[36m(pid=1565)[0m [5, 20000] loss: 0.137 Result for DEFAULT_d3304_00006: accuracy: 0.531 date: 2021-01-05_20-31-56 done: false experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 5 loss: 1.373500657767952 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 526.6667892932892 time_this_iter_s: 101.28407382965088 time_total_s: 526.6667892932892 timestamp: 1609878716 timesteps_since_restore: 0 training_iteration: 5 trial_id: d3304_00006 == Status == Memory usage on this node: 4.1/240.1 GiB Using AsyncHyperBand: num_stopped=9 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4950430885698218 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 2/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (1 RUNNING, 9 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.3735 | 0.531 | 5 | | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00002 | TERMINATED | | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1565)[0m [6, 2000] loss: 1.325 [2m[36m(pid=1565)[0m [6, 4000] loss: 0.668 [2m[36m(pid=1565)[0m [6, 6000] loss: 0.457 [2m[36m(pid=1565)[0m [6, 8000] loss: 0.338 [2m[36m(pid=1565)[0m [6, 10000] loss: 0.283 [2m[36m(pid=1565)[0m [6, 12000] loss: 0.232 [2m[36m(pid=1565)[0m [6, 14000] loss: 0.198 [2m[36m(pid=1565)[0m [6, 16000] loss: 0.175 [2m[36m(pid=1565)[0m [6, 18000] loss: 0.149 [2m[36m(pid=1565)[0m [6, 20000] loss: 0.140 Result for DEFAULT_d3304_00006: accuracy: 0.4852 date: 2021-01-05_20-33-55 done: false experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 6 loss: 1.5015573524537555 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 645.3050956726074 time_this_iter_s: 118.63830637931824 time_total_s: 645.3050956726074 timestamp: 1609878835 timesteps_since_restore: 0 training_iteration: 6 trial_id: d3304_00006 == Status == Memory usage on this node: 4.1/240.1 GiB Using AsyncHyperBand: num_stopped=9 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4950430885698218 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 2/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (1 RUNNING, 9 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.50156 | 0.4852 | 6 | | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00002 | TERMINATED | | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1565)[0m [7, 2000] loss: 1.295 [2m[36m(pid=1565)[0m [7, 4000] loss: 0.662 [2m[36m(pid=1565)[0m [7, 6000] loss: 0.452 [2m[36m(pid=1565)[0m [7, 8000] loss: 0.339 [2m[36m(pid=1565)[0m [7, 10000] loss: 0.270 [2m[36m(pid=1565)[0m [7, 12000] loss: 0.235 [2m[36m(pid=1565)[0m [7, 14000] loss: 0.193 [2m[36m(pid=1565)[0m [7, 16000] loss: 0.169 [2m[36m(pid=1565)[0m [7, 18000] loss: 0.154 [2m[36m(pid=1565)[0m [7, 20000] loss: 0.137 Result for DEFAULT_d3304_00006: accuracy: 0.4696 date: 2021-01-05_20-35-52 done: false experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 7 loss: 1.5851255111492393 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 762.1866834163666 time_this_iter_s: 116.88158774375916 time_total_s: 762.1866834163666 timestamp: 1609878952 timesteps_since_restore: 0 training_iteration: 7 trial_id: d3304_00006 == Status == Memory usage on this node: 4.1/240.1 GiB Using AsyncHyperBand: num_stopped=9 Bracket: Iter 8.000: -1.267511115884781 | Iter 4.000: -1.4950430885698218 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 2/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (1 RUNNING, 9 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.58513 | 0.4696 | 7 | | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00002 | TERMINATED | | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ [2m[36m(pid=1565)[0m [8, 2000] loss: 1.341 [2m[36m(pid=1565)[0m [8, 4000] loss: 0.667 [2m[36m(pid=1565)[0m [8, 6000] loss: 0.445 [2m[36m(pid=1565)[0m [8, 8000] loss: 0.336 [2m[36m(pid=1565)[0m [8, 10000] loss: 0.271 [2m[36m(pid=1565)[0m [8, 12000] loss: 0.228 [2m[36m(pid=1565)[0m [8, 14000] loss: 0.196 [2m[36m(pid=1565)[0m [8, 16000] loss: 0.175 [2m[36m(pid=1565)[0m [8, 18000] loss: 0.155 [2m[36m(pid=1565)[0m [8, 20000] loss: 0.135 Result for DEFAULT_d3304_00006: accuracy: 0.467 date: 2021-01-05_20-37-32 done: true experiment_id: d8bae0fc87134e6398fd0341279c1a1a experiment_tag: 6_batch_size=2,l1=64,l2=256,lr=0.0017724 hostname: 1a844a452371 iterations_since_restore: 8 loss: 1.6539037554110967 node_ip: 172.17.0.2 pid: 1565 should_checkpoint: true time_since_restore: 862.3724186420441 time_this_iter_s: 100.18573522567749 time_total_s: 862.3724186420441 timestamp: 1609879052 timesteps_since_restore: 0 training_iteration: 8 trial_id: d3304_00006 == Status == Memory usage on this node: 4.1/240.1 GiB Using AsyncHyperBand: num_stopped=10 Bracket: Iter 8.000: -1.4607074356479388 | Iter 4.000: -1.4950430885698218 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 2/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (1 RUNNING, 9 TERMINATED) +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00006 | RUNNING | 172.17.0.2:1565 | 2 | 64 | 256 | 0.00177236 | 1.6539 | 0.467 | 8 | | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00002 | TERMINATED | | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+ == Status == Memory usage on this node: 4.0/240.1 GiB Using AsyncHyperBand: num_stopped=10 Bracket: Iter 8.000: -1.4607074356479388 | Iter 4.000: -1.4950430885698218 | Iter 2.000: -1.7041921138763427 | Iter 1.000: -2.301384049916267 Resources requested: 0/32 CPUs, 0/2 GPUs, 0.0/157.71 GiB heap, 0.0/49.37 GiB objects Result logdir: /var/lib/jenkins/ray_results/DEFAULT_2021-01-05_20-23-08 Number of trials: 10/10 (10 TERMINATED) +---------------------+------------+-------+--------------+------+------+-------------+---------+------------+----------------------+ | Trial name | status | loc | batch_size | l1 | l2 | lr | loss | accuracy | training_iteration | |---------------------+------------+-------+--------------+------+------+-------------+---------+------------+----------------------| | DEFAULT_d3304_00000 | TERMINATED | | 2 | 4 | 16 | 0.000111924 | 1.8605 | 0.2724 | 2 | | DEFAULT_d3304_00001 | TERMINATED | | 8 | 16 | 32 | 0.077467 | 2.32186 | 0.1017 | 1 | | DEFAULT_d3304_00002 | TERMINATED | | 4 | 8 | 128 | 0.00436986 | 1.72073 | 0.3849 | 4 | | DEFAULT_d3304_00003 | TERMINATED | | 16 | 32 | 4 | 0.00120234 | 1.22223 | 0.5619 | 10 | | DEFAULT_d3304_00004 | TERMINATED | | 4 | 16 | 32 | 0.016474 | 2.31342 | 0.102 | 1 | | DEFAULT_d3304_00005 | TERMINATED | | 4 | 128 | 64 | 0.00757252 | 1.77391 | 0.3647 | 2 | | DEFAULT_d3304_00006 | TERMINATED | | 2 | 64 | 256 | 0.00177236 | 1.6539 | 0.467 | 8 | | DEFAULT_d3304_00007 | TERMINATED | | 8 | 8 | 8 | 0.000155891 | 2.30388 | 0.1011 | 1 | | DEFAULT_d3304_00008 | TERMINATED | | 2 | 16 | 64 | 0.0310199 | 2.33698 | 0.0983 | 1 | | DEFAULT_d3304_00009 | TERMINATED | | 4 | 4 | 32 | 0.0175239 | 2.31098 | 0.101 | 1 | +---------------------+------------+-------+--------------+------+------+-------------+---------+------------+----------------------+ Best trial config: {'l1': 32, 'l2': 4, 'lr': 0.0012023396319256663, 'batch_size': 16} Best trial final validation loss: 1.2222298237800597 Best trial final validation accuracy: 0.5619 Files already downloaded and verified Files already downloaded and verified Best trial test set accuracy: 0.5537 如果运行代码，则示例输出如下所示： 为了避免浪费资源，大多数审判​​已提早停止。 效果最好的试验的验证准确率约为 58%，可以在测试仪上进行确认。 就是这样了！ 您现在可以调整 PyTorch 模型的参数。 脚本的总运行时间：（14 分钟 43.400 秒） 下载 Python 源码：hyperparameter_tuning_tutorial.py 下载 Jupyter 笔记本：hyperparameter_tuning_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"54.html":{"url":"54.html","title":"模型剪裁教程","keywords":"","body":"模型剪裁教程 原文：https://pytorch.org/tutorials/intermediate/pruning_tutorial.html 作者： Michela Paganini 最新的深度学习技术依赖于难以部署的过度参数化模型。 相反，已知生物神经网络使用有效的稀疏连通性。 为了减少内存，电池和硬件消耗，同时又不牺牲精度，在设备上部署轻量级模型并通过私有设备上计算来确保私密性，确定通过减少模型中的参数数量来压缩模型的最佳技术很重要。 在研究方面，剪裁用于研究参数过度配置和参数不足网络在学习动态方面的差异，以研究幸运的稀疏子网络的作用（“彩票”），以及初始化，作为破坏性的神经结构搜索技术等等。 在本教程中，您将学习如何使用torch.nn.utils.prune稀疏神经网络，以及如何扩展它以实现自己的自定义剪裁技术。 要求 \"torch>=1.4.0a0+8e8a5e0\" import torch from torch import nn import torch.nn.utils.prune as prune import torch.nn.functional as F 创建模型 在本教程中，我们使用 LeCun 等人，1998 年的 LeNet 架构。 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") class LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() # 1 input image channel, 6 output channels, 3x3 square conv kernel self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) self.fc1 = nn.Linear(16 * 5 * 5, 120) # 5x5 image dimension self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, int(x.nelement() / x.shape[0])) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x model = LeNet().to(device=device) 检查模块 让我们检查一下 LeNet 模型中的（未剪裁）conv1层。 现在它将包含两个参数weight和bias，并且没有缓冲区。 module = model.conv1 print(list(module.named_parameters())) 出： [('weight', Parameter containing: tensor([[[[ 0.1552, 0.0102, -0.1944], [ 0.0263, 0.1374, -0.3139], [ 0.2838, 0.1943, 0.0948]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.2295], [-0.0050, 0.2485, -0.3230], [-0.1317, -0.0054, 0.2659]]], [[[-0.0932, 0.1316, 0.0670], [ 0.0572, -0.1845, 0.0870], [ 0.1372, 0.1080, 0.0324]]], [[[ 0.0908, -0.3280, 0.0365], [-0.3108, 0.2317, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0407, 0.0512, 0.0954], [-0.0437, 0.0302, -0.1317], [ 0.2573, 0.0626, 0.0883]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing: tensor([-0.1803, 0.1331, -0.3267, 0.3173, -0.0349, 0.1828], device='cuda:0', requires_grad=True))] print(list(module.named_buffers())) 出： [] 剪裁模块 要剪裁模块（在此示例中，为 LeNet 架构的conv1层），请首先从torch.nn.utils.prune中可用的那些技术中选择一种剪裁技术（或通过子类化BasePruningMethod实现您自己的东西）。 然后，指定模块和该模块中要剪裁的参数的名称。 最后，使用所选剪裁技术所需的适当关键字参数，指定剪裁参数。 在此示例中，我们将在conv1层中名为weight的参数中随机剪裁 30% 的连接。 模块作为第一个参数传递给函数； name使用其字符串标识符在该模块中标识参数； amount表示与剪裁的连接百分比（如果是介于 0 和 1 之间的浮点数），或表示与剪裁的连接的绝对数量（如果它是非负整数）。 prune.random_unstructured(module, name=\"weight\", amount=0.3) 剪裁是通过从参数中删除weight并将其替换为名为weight_orig的新参数（即，将\"_orig\"附加到初始参数name）来进行的。 weight_orig存储未剪裁的张量版本。 bias未剪裁，因此它将保持完整。 print(list(module.named_parameters())) 出： [('bias', Parameter containing: tensor([-0.1803, 0.1331, -0.3267, 0.3173, -0.0349, 0.1828], device='cuda:0', requires_grad=True)), ('weight_orig', Parameter containing: tensor([[[[ 0.1552, 0.0102, -0.1944], [ 0.0263, 0.1374, -0.3139], [ 0.2838, 0.1943, 0.0948]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.2295], [-0.0050, 0.2485, -0.3230], [-0.1317, -0.0054, 0.2659]]], [[[-0.0932, 0.1316, 0.0670], [ 0.0572, -0.1845, 0.0870], [ 0.1372, 0.1080, 0.0324]]], [[[ 0.0908, -0.3280, 0.0365], [-0.3108, 0.2317, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0407, 0.0512, 0.0954], [-0.0437, 0.0302, -0.1317], [ 0.2573, 0.0626, 0.0883]]]], device='cuda:0', requires_grad=True))] 通过以上选择的剪裁技术生成的剪裁掩码将保存为名为weight_mask的模块缓冲区（即，将\"_mask\"附加到初始参数name）。 print(list(module.named_buffers())) 出： [('weight_mask', tensor([[[[1., 1., 0.], [0., 0., 1.], [1., 0., 1.]]], [[[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]], [[[1., 1., 0.], [1., 0., 0.], [1., 0., 1.]]], [[[1., 1., 1.], [1., 0., 1.], [1., 1., 1.]]], [[[1., 1., 1.], [0., 0., 1.], [1., 1., 1.]]], [[[1., 0., 0.], [1., 0., 1.], [1., 0., 0.]]]], device='cuda:0'))] 为了使正向传播不更改即可工作，需要存在weight属性。 在torch.nn.utils.prune中实现的剪裁技术计算权重的剪裁版本（通过将掩码与原始参数组合）并将它们存储在属性weight中。 注意，这不再是module的参数，现在只是一个属性。 print(module.weight) 出： tensor([[[[ 0.1552, 0.0102, -0.0000], [ 0.0000, 0.0000, -0.3139], [ 0.2838, 0.0000, 0.0948]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.0000], [-0.0050, 0.0000, -0.0000], [-0.1317, -0.0000, 0.2659]]], [[[-0.0932, 0.1316, 0.0670], [ 0.0572, -0.0000, 0.0870], [ 0.1372, 0.1080, 0.0324]]], [[[ 0.0908, -0.3280, 0.0365], [-0.0000, 0.0000, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0407, 0.0000, 0.0000], [-0.0437, 0.0000, -0.1317], [ 0.2573, 0.0000, 0.0000]]]], device='cuda:0', grad_fn=) 最后，使用 PyTorch 的forward_pre_hooks在每次向前传递之前应用剪裁。 具体来说，当剪裁module时（如我们在此处所做的那样），它将为与之关联的每个参数获取forward_pre_hook进行剪裁。 在这种情况下，由于到目前为止我们只剪裁了名称为weight的原始参数，因此只会出现一个钩子。 print(module._forward_pre_hooks) 出： OrderedDict([(0, )]) 为了完整起见，我们现在也可以剪裁bias，以查看module的参数，缓冲区，挂钩和属性如何变化。 仅出于尝试另一种剪裁技术的目的，在此我们按 L1 范数剪裁偏差中的 3 个最小条目，如l1_unstructured剪裁函数中所实现的。 prune.l1_unstructured(module, name=\"bias\", amount=3) 现在，我们希望命名参数同时包含weight_orig（从前）和bias_orig。 缓冲区将包括weight_mask和bias_mask。 两个张量的剪裁后的版本将作为模块属性存在，并且该模块现在将具有两个forward_pre_hooks。 print(list(module.named_parameters())) 出： [('weight_orig', Parameter containing: tensor([[[[ 0.1552, 0.0102, -0.1944], [ 0.0263, 0.1374, -0.3139], [ 0.2838, 0.1943, 0.0948]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.2295], [-0.0050, 0.2485, -0.3230], [-0.1317, -0.0054, 0.2659]]], [[[-0.0932, 0.1316, 0.0670], [ 0.0572, -0.1845, 0.0870], [ 0.1372, 0.1080, 0.0324]]], [[[ 0.0908, -0.3280, 0.0365], [-0.3108, 0.2317, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0407, 0.0512, 0.0954], [-0.0437, 0.0302, -0.1317], [ 0.2573, 0.0626, 0.0883]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing: tensor([-0.1803, 0.1331, -0.3267, 0.3173, -0.0349, 0.1828], device='cuda:0', requires_grad=True))] print(list(module.named_buffers())) 出： [('weight_mask', tensor([[[[1., 1., 0.], [0., 0., 1.], [1., 0., 1.]]], [[[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]], [[[1., 1., 0.], [1., 0., 0.], [1., 0., 1.]]], [[[1., 1., 1.], [1., 0., 1.], [1., 1., 1.]]], [[[1., 1., 1.], [0., 0., 1.], [1., 1., 1.]]], [[[1., 0., 0.], [1., 0., 1.], [1., 0., 0.]]]], device='cuda:0')), ('bias_mask', tensor([0., 0., 1., 1., 0., 1.], device='cuda:0'))] print(module.bias) 出： tensor([-0.0000, 0.0000, -0.3267, 0.3173, -0.0000, 0.1828], device='cuda:0', grad_fn=) print(module._forward_pre_hooks) 出： OrderedDict([(0, ), (1, )]) 迭代式剪裁 一个模块中的同一参数可以被多次剪裁，各种剪裁调用的效果等于连接应用的各种蒙版的组合。 PruningContainer的compute_mask方法可处理新遮罩与旧遮罩的组合。 例如，假设我们现在想进一步剪裁module.weight，这一次是使用沿着张量的第 0 轴的结构化剪裁（第 0 轴对应于卷积层的输出通道，并且对于conv1具有 6 维） ，基于渠道的 L2 规范。 这可以通过ln_structured和n=2和dim=0函数来实现。 prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0) # As we can verify, this will zero out all the connections corresponding to # 50% (3 out of 6) of the channels, while preserving the action of the # previous mask. print(module.weight) 出： tensor([[[[ 0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, 0.0000]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.0000], [-0.0050, 0.0000, -0.0000], [-0.1317, -0.0000, 0.2659]]], [[[-0.0000, 0.0000, 0.0000], [ 0.0000, -0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]]], [[[ 0.0908, -0.3280, 0.0365], [-0.0000, 0.0000, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0000, 0.0000, 0.0000], [-0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, 0.0000]]]], device='cuda:0', grad_fn=) 现在，对应的钩子将为torch.nn.utils.prune.PruningContainer类型，并将存储应用于weight参数的剪裁历史。 for hook in module._forward_pre_hooks.values(): if hook._tensor_name == \"weight\": # select out the correct hook break print(list(hook)) # pruning history in the container 出： [, ] 序列化剪裁的模型 所有相关的张量，包括掩码缓冲区和用于计算剪裁的张量的原始参数，都存储在模型的state_dict中，因此可以根据需要轻松地序列化和保存。 print(model.state_dict().keys()) 出： odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']) 删除剪裁重新参数化 要使剪裁永久化，请删除weight_orig和weight_mask的重新参数化，然后删除forward_pre_hook，我们可以使用torch.nn.utils.prune的remove函数。 请注意，这不会撤消剪裁，好像从未发生过。 而是通过将参数weight重新分配给模型参数（剪裁后的版本）来使其永久不变。 删除重新参数化之前： print(list(module.named_parameters())) 出： [('weight_orig', Parameter containing: tensor([[[[ 0.1552, 0.0102, -0.1944], [ 0.0263, 0.1374, -0.3139], [ 0.2838, 0.1943, 0.0948]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.2295], [-0.0050, 0.2485, -0.3230], [-0.1317, -0.0054, 0.2659]]], [[[-0.0932, 0.1316, 0.0670], [ 0.0572, -0.1845, 0.0870], [ 0.1372, 0.1080, 0.0324]]], [[[ 0.0908, -0.3280, 0.0365], [-0.3108, 0.2317, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0407, 0.0512, 0.0954], [-0.0437, 0.0302, -0.1317], [ 0.2573, 0.0626, 0.0883]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing: tensor([-0.1803, 0.1331, -0.3267, 0.3173, -0.0349, 0.1828], device='cuda:0', requires_grad=True))] print(list(module.named_buffers())) 出： [('weight_mask', tensor([[[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]], [[[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]], [[[1., 1., 0.], [1., 0., 0.], [1., 0., 1.]]], [[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]], [[[1., 1., 1.], [0., 0., 1.], [1., 1., 1.]]], [[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]]], device='cuda:0')), ('bias_mask', tensor([0., 0., 1., 1., 0., 1.], device='cuda:0'))] print(module.weight) 出： tensor([[[[ 0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, 0.0000]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.0000], [-0.0050, 0.0000, -0.0000], [-0.1317, -0.0000, 0.2659]]], [[[-0.0000, 0.0000, 0.0000], [ 0.0000, -0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]]], [[[ 0.0908, -0.3280, 0.0365], [-0.0000, 0.0000, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0000, 0.0000, 0.0000], [-0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, 0.0000]]]], device='cuda:0', grad_fn=) 删除重新参数化后： prune.remove(module, 'weight') print(list(module.named_parameters())) 出： [('bias_orig', Parameter containing: tensor([-0.1803, 0.1331, -0.3267, 0.3173, -0.0349, 0.1828], device='cuda:0', requires_grad=True)), ('weight', Parameter containing: tensor([[[[ 0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, 0.0000]]], [[[-0.0296, -0.2514, 0.1300], [ 0.0756, -0.3155, -0.2900], [-0.1840, 0.1143, -0.0120]]], [[[-0.2383, -0.3022, 0.0000], [-0.0050, 0.0000, -0.0000], [-0.1317, -0.0000, 0.2659]]], [[[-0.0000, 0.0000, 0.0000], [ 0.0000, -0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]]], [[[ 0.0908, -0.3280, 0.0365], [-0.0000, 0.0000, -0.2271], [ 0.1171, 0.2113, -0.2259]]], [[[ 0.0000, 0.0000, 0.0000], [-0.0000, 0.0000, -0.0000], [ 0.0000, 0.0000, 0.0000]]]], device='cuda:0', requires_grad=True))] print(list(module.named_buffers())) 出： [('bias_mask', tensor([0., 0., 1., 1., 0., 1.], device='cuda:0'))] 剪裁模型中的多个参数 通过指定所需的剪裁技术和参数，我们可以轻松地剪裁网络中的多个张量，也许根据它们的类型，如在本示例中将看到的那样。 new_model = LeNet() for name, module in new_model.named_modules(): # prune 20% of connections in all 2D-conv layers if isinstance(module, torch.nn.Conv2d): prune.l1_unstructured(module, name='weight', amount=0.2) # prune 40% of connections in all linear layers elif isinstance(module, torch.nn.Linear): prune.l1_unstructured(module, name='weight', amount=0.4) print(dict(new_model.named_buffers()).keys()) # to verify that all masks exist 出： dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask']) 全局剪裁 到目前为止，我们仅查看了通常称为“局部”剪裁的情况，即通过比较每个条目的统计信息（权重，激活度，梯度等）来逐个剪裁模型中的张量的做法。 到该张量中的其他条目。 但是，一种通用且可能更强大的技术是通过删除（例如）删除整个模型中最低的 20% 的连接，而不是删除每一层中最低的 20% 的连接来一次剪裁模型。 这很可能导致每个层的剪裁百分比不同。 让我们看看如何使用torch.nn.utils.prune中的global_unstructured进行操作。 model = LeNet() parameters_to_prune = ( (model.conv1, 'weight'), (model.conv2, 'weight'), (model.fc1, 'weight'), (model.fc2, 'weight'), (model.fc3, 'weight'), ) prune.global_unstructured( parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2, ) 现在，我们可以检查每个剪裁参数中引起的稀疏性，该稀疏性将不等于每层中的 20%。 但是，全局稀疏度将（大约）为 20%。 print( \"Sparsity in conv1.weight: {:.2f}%\".format( 100\\. * float(torch.sum(model.conv1.weight == 0)) / float(model.conv1.weight.nelement()) ) ) print( \"Sparsity in conv2.weight: {:.2f}%\".format( 100\\. * float(torch.sum(model.conv2.weight == 0)) / float(model.conv2.weight.nelement()) ) ) print( \"Sparsity in fc1.weight: {:.2f}%\".format( 100\\. * float(torch.sum(model.fc1.weight == 0)) / float(model.fc1.weight.nelement()) ) ) print( \"Sparsity in fc2.weight: {:.2f}%\".format( 100\\. * float(torch.sum(model.fc2.weight == 0)) / float(model.fc2.weight.nelement()) ) ) print( \"Sparsity in fc3.weight: {:.2f}%\".format( 100\\. * float(torch.sum(model.fc3.weight == 0)) / float(model.fc3.weight.nelement()) ) ) print( \"Global sparsity: {:.2f}%\".format( 100\\. * float( torch.sum(model.conv1.weight == 0) + torch.sum(model.conv2.weight == 0) + torch.sum(model.fc1.weight == 0) + torch.sum(model.fc2.weight == 0) + torch.sum(model.fc3.weight == 0) ) / float( model.conv1.weight.nelement() + model.conv2.weight.nelement() + model.fc1.weight.nelement() + model.fc2.weight.nelement() + model.fc3.weight.nelement() ) ) ) 出： Sparsity in conv1.weight: 3.70% Sparsity in conv2.weight: 8.10% Sparsity in fc1.weight: 22.05% Sparsity in fc2.weight: 12.29% Sparsity in fc3.weight: 8.45% Global sparsity: 20.00% 使用自定义剪裁函数扩展torch.nn.utils.prune 要实现自己的剪裁函数，可以通过继承BasePruningMethod基类的子类来扩展nn.utils.prune模块，这与所有其他剪裁方法一样。 基类为您实现以下方法：__call__，apply_mask，apply，prune和remove。 除了一些特殊情况外，您无需为新的剪裁技术重新实现这些方法。 但是，您将必须实现__init__（构造器）和compute_mask（有关如何根据剪裁技术的逻辑为给定张量计算掩码的说明）。 另外，您将必须指定此技术实现的剪裁类型（支持的选项为global，structured和unstructured）。 需要确定在迭代应用剪裁的情况下如何组合蒙版。 换句话说，当剪裁预剪裁的参数时，当前的剪裁技术应作用于参数的未剪裁部分。 指定PRUNING_TYPE将使PruningContainer（处理剪裁掩码的迭代应用）正确识别要剪裁的参数。 例如，假设您要实现一种剪裁技术，以剪裁张量中的所有其他条目（或者-如果先前已剪裁过张量，则剪裁张量的其余未剪裁部分）。 这将是PRUNING_TYPE='unstructured'，因为它作用于层中的单个连接，而不作用于整个单元/通道（'structured'），或作用于不同的参数（'global'）。 class FooBarPruningMethod(prune.BasePruningMethod): \"\"\"Prune every other entry in a tensor \"\"\" PRUNING_TYPE = 'unstructured' def compute_mask(self, t, default_mask): mask = default_mask.clone() mask.view(-1)[::2] = 0 return mask 现在，要将其应用于nn.Module中的参数，还应该提供一个简单的函数来实例化该方法并将其应用。 def foobar_unstructured(module, name): \"\"\"Prunes tensor corresponding to parameter called `name` in `module` by removing every other entry in the tensors. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called `name+'_mask'` corresponding to the binary mask applied to the parameter `name` by the pruning method. The parameter `name` is replaced by its pruned version, while the original (unpruned) parameter is stored in a new parameter named `name+'_orig'`. Args: module (nn.Module): module containing the tensor to prune name (string): parameter name within `module` on which pruning will act. Returns: module (nn.Module): modified (i.e. pruned) version of the input module Examples: >>> m = nn.Linear(3, 4) >>> foobar_unstructured(m, name='bias') \"\"\" FooBarPruningMethod.apply(module, name) return module 试试吧！ model = LeNet() foobar_unstructured(model.fc3, name='bias') print(model.fc3.bias_mask) 出： tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.]) 脚本的总运行时间：（0 分钟 0.135 秒） 下载 Python 源码：pruning_tutorial.py 下载 Jupyter 笔记本：pruning_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"55.html":{"url":"55.html","title":"LSTM 单词语言模型上的动态量化（beta）","keywords":"","body":"LSTM 单词语言模型上的动态量化（beta） 原文：https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html 作者： James Reed 编辑：Seth Weidman 简介 量化涉及将模型的权重和激活从float转换为int，这可以导致模型尺寸更小，推断速度更快，而对准确率的影响很小。 在本教程中，我们将最简单的量化形式-动态量化应用于基于 LSTM 的下一个单词预测模型，紧紧遵循 PyTorch 示例中的单词语言模型 。 # imports import os from io import open import time import torch import torch.nn as nn import torch.nn.functional as F 1.定义模型 在这里，我们根据词语言模型示例中的模型定义 LSTM 模型架构。 class LSTMModel(nn.Module): \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\" def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5): super(LSTMModel, self).__init__() self.drop = nn.Dropout(dropout) self.encoder = nn.Embedding(ntoken, ninp) self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout) self.decoder = nn.Linear(nhid, ntoken) self.init_weights() self.nhid = nhid self.nlayers = nlayers def init_weights(self): initrange = 0.1 self.encoder.weight.data.uniform_(-initrange, initrange) self.decoder.bias.data.zero_() self.decoder.weight.data.uniform_(-initrange, initrange) def forward(self, input, hidden): emb = self.drop(self.encoder(input)) output, hidden = self.rnn(emb, hidden) output = self.drop(output) decoded = self.decoder(output) return decoded, hidden def init_hidden(self, bsz): weight = next(self.parameters()) return (weight.new_zeros(self.nlayers, bsz, self.nhid), weight.new_zeros(self.nlayers, bsz, self.nhid)) 2.加载文本数据 接下来，我们再次根据单词模型示例对预处理，将 Wikitext-2 数据集加载到语料库中。 class Dictionary(object): def __init__(self): self.word2idx = {} self.idx2word = [] def add_word(self, word): if word not in self.word2idx: self.idx2word.append(word) self.word2idx[word] = len(self.idx2word) - 1 return self.word2idx[word] def __len__(self): return len(self.idx2word) class Corpus(object): def __init__(self, path): self.dictionary = Dictionary() self.train = self.tokenize(os.path.join(path, 'train.txt')) self.valid = self.tokenize(os.path.join(path, 'valid.txt')) self.test = self.tokenize(os.path.join(path, 'test.txt')) def tokenize(self, path): \"\"\"Tokenizes a text file.\"\"\" assert os.path.exists(path) # Add words to the dictionary with open(path, 'r', encoding=\"utf8\") as f: for line in f: words = line.split() + [''] for word in words: self.dictionary.add_word(word) # Tokenize file content with open(path, 'r', encoding=\"utf8\") as f: idss = [] for line in f: words = line.split() + [''] ids = [] for word in words: ids.append(self.dictionary.word2idx[word]) idss.append(torch.tensor(ids).type(torch.int64)) ids = torch.cat(idss) return ids model_data_filepath = 'data/' corpus = Corpus(model_data_filepath + 'wikitext-2') 3.加载预先训练的模型 这是一本有关动态量化的教程，这是在训练模型后应用的一种量化技术。 因此，我们将简单地将一些预训练的权重加载到此模型架构中； 这些权重是通过使用单词语言模型示例中的默认设置训练五个周期而获得的。 ntokens = len(corpus.dictionary) model = LSTMModel( ntoken = ntokens, ninp = 512, nhid = 256, nlayers = 5, ) model.load_state_dict( torch.load( model_data_filepath + 'word_language_model_quantize.pth', map_location=torch.device('cpu') ) ) model.eval() print(model) 出： LSTMModel( (drop): Dropout(p=0.5, inplace=False) (encoder): Embedding(33278, 512) (rnn): LSTM(512, 256, num_layers=5, dropout=0.5) (decoder): Linear(in_features=256, out_features=33278, bias=True) ) 现在，我们生成一些文本以确保预先训练的模型能够正常工作-与以前类似，我们在此处遵循 input_ = torch.randint(ntokens, (1, 1), dtype=torch.long) hidden = model.init_hidden(1) temperature = 1.0 num_words = 1000 with open(model_data_filepath + 'out.txt', 'w') as outf: with torch.no_grad(): # no tracking history for i in range(num_words): output, hidden = model(input_, hidden) word_weights = output.squeeze().div(temperature).exp().cpu() word_idx = torch.multinomial(word_weights, 1)[0] input_.fill_(word_idx) word = corpus.dictionary.idx2word[word_idx] outf.write(str(word.encode('utf-8')) + ('\\n' if i % 20 == 19 else ' ')) if i % 100 == 0: print('| Generated {}/{} words'.format(i, 1000)) with open(model_data_filepath + 'out.txt', 'r') as outf: all_output = outf.read() print(all_output) 出： | Generated 0/1000 words | Generated 100/1000 words | Generated 200/1000 words | Generated 300/1000 words | Generated 400/1000 words | Generated 500/1000 words | Generated 600/1000 words | Generated 700/1000 words | Generated 800/1000 words | Generated 900/1000 words b'broadcaster' b'good' b',' b'which' b'provided' b'for' b'a' b'vignettes' b'socially' b'and' b'the' b'FIA' b\"'s\" b'ad' b'.' b'The' b'state' b'into' b'this' b'position' b'is' b'in' b'account' b'of' b'a' b'wide' b'Domonia' b'' b',' b'fallen' b'to' b'for' b'the' b'types' b'of' b'' b'developers' b'being' b'entertaining' b'.' b'' b'The' b'Claus' b'II' b'(' b'The' b'Book' b'of' b'Karnataka' b',' b'2' b'/' b'10' b')' b'was' b'released' b'by' b'British' b'@-@' b'Irish' b'ruler' b'arriving' b'on' b'the' b'winter' b'of' b'its' b'championship' b'orbit' b'.' b'In' b'early' b'spring' b'roles' b'dismay' b'when' b'he' b'replaced' b'by' b'a' b'religious' b'park' b',' b'when' b'it' b'features' b'flowers' b'they' b'do' b'populist' b'.' b'temperatures' b'attempted' b'to' b'have' b'trouble' b'met' b',' b'' b',' b'and' b'karaoke' b'leads' b'to' b'some' b'return' b'up' b'as' b'or' b'seated' b'.' b'The' b'remainder' b'of' b'w' b'voltage' b'contains' b'Allah' b'in' b'the' b'series' b'to' b'infiltrate' b'disappeared' b'.' b'Though' b'it' b'comes' b'into' b'his' b'Shinnok' b\"'s\" b'history' b',' b'they' b'may' b'sometimes' b'7' b'@-@' b'April' b',' b'roughly' b'7' b'%' b'of' b'50' b'mph' b'(' b'4' b'@.@' b'8' b'in' b')' b'while' b'males' b'have' b'put' b'except' b'far' b'as' b'alkaline' b'@-@' b'up' b'.' b'' b'Electrical' b'medical' b'rings' b'were' b'always' b'published' b'.' b'' b'Based' b'on' b'2' b'November' b',' b'Idaho' b'can' b'be' b'estimated' b'cooking' b'and' b'' b',' b'while' b'no' b',' b'thin' b'drugs' b'was' b'poor' b'to' b'each' b'area' b'.' b'It' b'has' b'not' b'campaigned' b'those' b'of' b'the' b'most' b'potent' b'population' b'of' b'leaves' b'in' b'all' b'condition' b',' b'because' b'they' b'were' b'forced' b'to' b'die' b'in' b'bhandara' b'' b'that' b'culture' b'.' b'Almost' b'a' b'prose' b'plan' b',' b'there' b'have' b'been' b'only' b'clear' b',' b'it' b'occurs' b'.' b'' b'The' b'kakapo' b'was' b'interpreted' b'on' b'1998' b'from' b'1955' b'and' b'played' b'in' b'' b',' b'Western' b'Asia' b'on' b'0' b'August' b'1966' b',' b'with' b'an' b'additional' b'population' b'that' b'Samuel' b'solemnly' b',' b'Chapman' b'sponsored' b'after' b'a' b'few' b'years' b'.' b'In' b'1990' b',' b'prominent' b'areas' b'believe' b'that' b'as' b'being' b'an' b'rural' b'planet' b',' b'they' b'is' b'neglected' b'as' b'to' b'be' b'changed' b'.' b'Congress' b'This' b'well' b'\"' b'was' b'run' b'by' b'' b',' b'Waldemar' b'Greenwood' b'.' b'170' b'have' b'just' b'in' b'place' b',' b'he' b'overruled' b'.' b'The' b'1966' b'race' b'is' b'a' b'embodies' b'state' b'of' b'Viking' b'or' b'most' b'generation' b',' b'not' b'in' b'the' b'codes' b'of' b'all' b'other' b'alignment' b'musical' b'politicians' b'.' b'No' b'system' b'have' b'participated' b'on' b'3' b'to' b'9' b'%' b'of' b'any' b'urine' b',' b'with' b'both' b'drawings' b'and' b'significantly' b'towards' b'his' b'deteriorating' b'and' b'poverty' b'.' b'As' b'a' b'rust' b',' b'contains' b'other' b'compositions' b'that' b'must' b'be' b'beneficial' b'by' b'overnight' b'or' b'fluid' b',' b'u' b'organizations' b'can' b'seek' b'mild' b'late' b'down' b'on' b'a' b'broadside' b'and' b'leads' b'to' b'its' b'cycle' b'.' b'For' b'example' b',' b'1137' b',' b'snowmelt' b'and' b'' b'\\xe2\\x80\\x94' b'a' b'variety' b'of' b'dealt' b';' b'Species' b'(' b'with' b'a' b'reduction' b'of' b'prohibitions' b')' b',' b'' b'exploration' b',' b'' b'an' b'fuel' b'eye' b'of' b'purple' b'trees' b',' b'was' b'shown' b'west' b'.' b'chased' b'Jack' b'of' b'claws' b',' b'his' b'vertex' b'states' b'that' b'they' b',' b'in' b'1922' b',' b'was' b'killed' b'.' b'' b'There' b'have' b'been' b'official' b'concerns' b'of' b'Boat' b'Kerry' b'including' b'L\\xc3\\xaa' b'\\xe3\\x80\\x89' b'and' b'' b'A' b'Forest' b',' b'\"' b'' b',' b'because' b'' b',' b'and' b'sometimes' b'encounters' b'like' b'I' b\"'ve\" b'been' b'' b'.' b'\"' b'' b'Hunter' b'pathway' b'writes' b'it' b'entering' b'the' b'second' b'.' b'The' b'kakapo' b'is' b'gems' b'used' b'after' b'died' b'from' b'two' b'games' b'in' b'six' b'' b',' b'her' b'feature' b'and' b'called' b'\"' b'mercenaries' b'\"' b',' b'which' b'supported' b'by' b'the' b'Selective' b'Race' b'.' b'\"' b'' b'Bono' b'Dutch' b'struggles' b'to' b'the' b'species' b'' b',' b'especially' b'crusaders' b'I' b'lives' b'process' b',' b'but' b'Constantin' b'approximate' b'and' b'character' b'or' b'so' b'.' b'There' b'have' b'numerous' b'pale' b'dioceses' b'as' b'a' b'resistant' b';' b'the' b'Inn' b'Comic' b'@-@' b'white' b'individuals' b',' b'its' b'flat' b',' b'' b'and' b'correct' b',' b'in' b'which' b'they' b'felt' b'.' b'In' b'the' b'arms' b',' b'the' b'original' b'occasion' b'about' b'Spanish' b'sites' b'all' b'(' b'millionaire' b'lay' b';' b'or' b'160' b'@-@' b'mosquitoes' b')' b'v' b'' b'(' b'c' b')' b'.' b'The' b'bird' b'is' b'extremely' b'paved' b',' b'and' b'they' b'are' b'claimed' b'to' b'wedding' b'the' b'' b'of' b'Excellence' b',' b'and' b'an' b'extinct' b'composite' b',' b'cute' b'outside' b'' b'.' b'This' b'may' b'be' b'seen' b'by' b'the' b'Seer' b'that' b'Tempest' b'\"' b'comes' b'\"' b'over' b'a' b'bright' b'judicial' b'guitar' b',' b'which' b'describes' b',' b'and' b'tend' b'to' b'be' b'seen' b'.' b'' b'' b'=' b'=' b'Conservation' b'for' b'contraception' b'=' b'=' b'' b'' b'Grieco' b'Island' b'is' b'a' b'eventually' b'scale' b'word' b'to' b'a' b'tropical' b'storm' b',' b'based' b'in' b'a' b'pre' b'\\xe2\\x80\\x93' b'9' b'lead' b',' b'a' b'forces' b'after' b'a' b'additional' b',' b'grey' b'substance' b',' b'Metro' b',' b'background' b',' b'and' b'cooperate' b'with' b'its' b'overly' b'overview' b',' b'so' b'the' b'heaviest' b'route' b',' b'and' b'\\xc2\\xb3' b'.' b'portion' b'may' b'occur' b'this' b'other' b'up' b'an' b'' b'break' b',' b'then' b'or' b'deep' b'distinct' b'or' b'female' b'offspring' b',' b'but' b'even' b'understand' b'.' b'Following' b'God' b'(' b'no' b'nervous' b'image' b'from' b'complaints' b')' b',' b'the' b'player' b'represents' b'three' b'or' b'over' b'9' b'\\xc2\\xb0' b'large' b'(' b'five' b'weeks' b'of' b'many' b'cats' b')' b',' b'as' b'it' b'targets' b'for' b'the' b'second' b'female' b'together' b'.' b'159' b',' b'it' b'also' b'spend' b'bold' b'markets' b'and' b'its' b'players' b'powers' b',' b'dubbed' b'those' b'of' b'lengths' b'.' b'Most' b'are' b'arrow' b'could' b'be' b'noticed' b'involving' b'they' b'fall' b'.' b'On' b'FAU' b\"'s\" b'only' b'lifetime' b'she' b'treated' b'or' b'their' b'apparent' b'soaring' b'proposition' b'has' b'5th' b'of' b'those' b'eye' b',' b'but' b'knows' b'in' b'a' b'' b'Network' b';' b'which' b'of' b'that' b'reality' b'or' b'artificial' b'when' b'struggling' b'Bungie' b'is' b'successful' b'.' b'The' b'' b'sound' b'of' b'frontier' b'ahead' b'for' b'damage' b'came' b'on' b',' b'so' b'the' b'first' b'series' b'funded' b'by' b'its' b'bowls' b',' b'a' b'chant' b'.' b'They' b'may' b'be' b'used' b'Pongsak' b'or' b'occasionally' b'protected' b'them' b'.' b'Fingal' b'cylindrical' b'conspired' b'on' b'a' b'variety' b'of' b'prey' b',' b'' b',' b'Zach' b',' b'and' b'young' b'possessing' b'Westland' b'valleys' b'.' b'Otherwise' b',' b'I' b'do' b'at' b'them' b'in' b'first' b'@-@' b'season' b'woodland' b',' b'where' b'they' b'weighed' b'them' b'to' b'correct' b'a' b'list' b'of' b'other' b'birds' b'.' b'Another' b'theme' b'where' b'or' b',' b'it' b'is' b'a' b'appropriate' b'source' b',' b'this' b'competed' b'in' b'integral' b'Waiouru' b'alone' b',' b'the' b'pathways' b'under' b'Aravind' b',' b'and' b'others' b',' b'instead' b'of' b'westward' b',' b'as' b'they' b'are' b'quarters' b'and' b'caused' b'in' b'males' b'.' b'Once' b'selective' b'centered' b',' b'they' b'threats' b'were' b'Zuniceratops' b'.' b'Although' b'the' b'most' b'spots' b'replication' b'became' b'a' b'fragile' b'pointer' b'(' b'a' b'pair' b'of' b'' b')' b',' b'strongly' b'\"' b'mammals' b'\"' b',' b'which' b'give' b'Powderfinger' b'to' b'persecution' b'.' b'Other' b'conifers' b'but' b'even' b'only' b'swallow' b'so' b'every' b'symbols' b'of' b'Manders' b',' b'in' b'massive' 它不是 GPT-2，但看起来该模型已开始学习语言结构！ 我们几乎准备好演示动态量化。 我们只需要定义一些辅助函数： bptt = 25 criterion = nn.CrossEntropyLoss() eval_batch_size = 1 # create test data set def batchify(data, bsz): # Work out how cleanly we can divide the dataset into bsz parts. nbatch = data.size(0) // bsz # Trim off any extra elements that wouldn't cleanly fit (remainders). data = data.narrow(0, 0, nbatch * bsz) # Evenly divide the data across the bsz batches. return data.view(bsz, -1).t().contiguous() test_data = batchify(corpus.test, eval_batch_size) # Evaluation functions def get_batch(source, i): seq_len = min(bptt, len(source) - 1 - i) data = source[i:i+seq_len] target = source[i+1:i+1+seq_len].reshape(-1) return data, target def repackage_hidden(h): \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\" if isinstance(h, torch.Tensor): return h.detach() else: return tuple(repackage_hidden(v) for v in h) def evaluate(model_, data_source): # Turn on evaluation mode which disables dropout. model_.eval() total_loss = 0. hidden = model_.init_hidden(eval_batch_size) with torch.no_grad(): for i in range(0, data_source.size(0) - 1, bptt): data, targets = get_batch(data_source, i) output, hidden = model_(data, hidden) hidden = repackage_hidden(hidden) output_flat = output.view(-1, ntokens) total_loss += len(data) * criterion(output_flat, targets).item() return total_loss / (len(data_source) - 1) 4.测试动态量化 最后，我们可以在模型上调用torch.quantization.quantize_dynamic！ 特别， 我们指定我们希望对模型中的nn.LSTM和nn.Linear模块进行量化 我们指定希望将权重转换为int8值 import torch.quantization quantized_model = torch.quantization.quantize_dynamic( model, {nn.LSTM, nn.Linear}, dtype=torch.qint8 ) print(quantized_model) 出： LSTMModel( (drop): Dropout(p=0.5, inplace=False) (encoder): Embedding(33278, 512) (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5) (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine) ) 该模型看起来相同； 这对我们有什么好处？ 首先，我们看到模型尺寸显着减小： def print_size_of_model(model): torch.save(model.state_dict(), \"temp.p\") print('Size (MB):', os.path.getsize(\"temp.p\")/1e6) os.remove('temp.p') print_size_of_model(model) print_size_of_model(quantized_model) 出： Size (MB): 113.945726 Size (MB): 79.739984 其次，我们看到了更快的推理时间，而评估损失没有差异： 注意：由于量化模型运行单线程，因此用于单线程比较的线程数为 1。 torch.set_num_threads(1) def time_model_evaluation(model, test_data): s = time.time() loss = evaluate(model, test_data) elapsed = time.time() - s print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed)) time_model_evaluation(model, test_data) time_model_evaluation(quantized_model, test_data) 出： loss: 5.167 elapsed time (seconds): 251.3 loss: 5.168 elapsed time (seconds): 166.3 在没有量化的情况下在 MacBook Pro 上本地运行此操作，推理大约需要 200 秒，而量化则只需大约 100 秒。 总结 动态量化可能是减小模型大小的简单方法，而对精度的影响有限。 谢谢阅读！ 与往常一样，我们欢迎您提供反馈，因此，如果有任何问题，请在这里创建一个 ISSUE。 脚本的总运行时间：（7 分钟 3.126 秒） 下载 Python 源码：dynamic_quantization_tutorial.py 下载 Jupyter 笔记本：dynamic_quantization_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"56.html":{"url":"56.html","title":"BERT 上的动态量化（Beta）","keywords":"","body":"BERT 上的动态量化（Beta） 原文：https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html 小费 为了充分利用本教程，我们建议使用此 Colab 版本。 这将使您可以尝试以下信息。 作者：Jianyu Huang 审核： Raghuraman Krishnamoorthi 编辑：Jessica Lin 简介 在本教程中，我们将动态量化应用在 BERT 模型上，紧跟 HuggingFace 转换器示例中的 BERT 模型。 通过这一循序渐进的旅程，我们将演示如何将著名的 BERT 等最新模型转换为动态量化模型。 BERT，或者说转换器的双向嵌入表示，是一种预训练语言表示的新方法，它可以在许多常见的自然语言处理（NLP）任务（例如问题解答，文本分类， 和别的。 可以在此处找到。 PyTorch 中的动态量化支持将浮点模型转换为具有静态int8或float16数据类型的权重和动态量化激活的量化模型。 当权重量化为int8时，激活（每批）动态量化为int8。 在 PyTorch 中，我们有torch.quantization.quantize_dynamic API，该 API 用仅动态权重的量化版本替换了指定的模块，并输出了量化模型。 我们在通用语言理解评估基准（GLUE）中演示了 Microsoft Research Paraphrase 语料库（MRPC）任务的准确率和推理表现结果。 MRPC（Dolan 和 Brockett，2005 年）是从在线新闻源中自动提取的句子对的语料库，带有人工标注，说明句子中的句子在语义上是否等效。 由于类别不平衡（正向为 68%，负向为 32%），我们遵循常规做法并报告 F1 得分。 MRPC 是用于语言对分类的常见 NLP 任务，如下所示。 1.设置 1.1 安装 PyTorch 和 HuggingFace 转换器 要开始本教程，首先请遵循 PyTorch 和 HuggingFace Github 仓库中的安装说明。 此外，我们还将安装 scikit-learn 包，因为我们将重复使用其内置的 F1 分数计算助手函数。 pip install sklearn pip install transformers 由于我们将使用 PyTorch 的 Beta 版部分，因此建议安装最新版本的 Torch 和tochvision。 您可以在此处找到有关本地安装的最新说明。 例如，要在 Mac 上安装： yes y | pip uninstall torch tochvision yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html 1.2 导入必要的模块 在这一步中，我们将导入本教程所需的 Python 模块。 from __future__ import absolute_import, division, print_function import logging import numpy as np import os import random import sys import time import torch from argparse import Namespace from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset) from tqdm import tqdm from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,) from transformers import glue_compute_metrics as compute_metrics from transformers import glue_output_modes as output_modes from transformers import glue_processors as processors from transformers import glue_convert_examples_to_features as convert_examples_to_features # Setup logging logger = logging.getLogger(__name__) logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt = '%m/%d/%Y %H:%M:%S', level = logging.WARN) logging.getLogger(\"transformers.modeling_utils\").setLevel( logging.WARN) # Reduce logging print(torch.__version__) 我们设置线程数以比较 FP32 和 INT8 性能之间的单线程性能。 在本教程的最后，用户可以通过使用右侧并行后端构建 PyTorch 来设置其他线程数量。 torch.set_num_threads(1) print(torch.__config__.parallel_info()) 1.3 了解辅助函数 助手函数内置在转换器库中。 我们主要使用以下辅助函数：一个用于将文本示例转换为特征向量的函数； 另一个用于测量预测结果的 F1 分数。 gum_convert_examples_to_features函数将文本转换为输入特征： 标记输入序列； 在开头插入[CLS]； 在第一句和第二句之间并在最后插入[SEP]； 生成标记类型 ID，以指示标记是属于第一序列还是第二序列。 gum_compute_metrics函数的计算指标为 F1 得分，可以将其解释为精度和召回率的加权平均值，其中 F1 得分最佳值为 1，最差值为 0。精度和召回率对 F1 得分的相对贡献相等。 F1 分数的公式为： 1.4 下载数据集 在运行 MRPC 任务之前，我们通过运行此脚本并下载 GLUE 数据并将其解压缩到目录glue_data中。 python download_glue_data.py --data_dir='glue_data' --tasks='MRPC' 2.微调 BERT 模型 BERT 的精神是预训练语言表示形式，然后以最小的任务相关参数微调各种任务上的深层双向表示形式，并获得最新的结果。 在本教程中，我们将专注于对预训练的 BERT 模型进行微调，以对 MRPC 任务上的语义等效句子对进行分类。 要为 MRPC 任务微调预训练的 BERT 模型（HuggingFace 转换器中的bert-base-uncased模型），可以按照示例中的命令进行操作： export GLUE_DIR=./glue_data export TASK_NAME=MRPC export OUT_DIR=./$TASK_NAME/ python ./run_glue.py \\ --model_type bert \\ --model_name_or_path bert-base-uncased \\ --task_name $TASK_NAME \\ --do_train \\ --do_eval \\ --do_lower_case \\ --data_dir $GLUE_DIR/$TASK_NAME \\ --max_seq_length 128 \\ --per_gpu_eval_batch_size=8 \\ --per_gpu_train_batch_size=8 \\ --learning_rate 2e-5 \\ --num_train_epochs 3.0 \\ --save_steps 100000 \\ --output_dir $OUT_DIR 我们在此处为 MRPC 任务提供了经过微调的 BERT 模型。 为了节省时间，您可以将模型文件（约 400 MB）直接下载到本地文件夹$OUT_DIR中。 2.1 设置全局配置 在这里，我们设置了用于在动态量化之前和之后评估微调 BERT 模型的全局配置。 configs = Namespace() # The output directory for the fine-tuned model, $OUT_DIR. configs.output_dir = \"./MRPC/\" # The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME. configs.data_dir = \"./glue_data/MRPC\" # The model name or path for the pre-trained model. configs.model_name_or_path = \"bert-base-uncased\" # The maximum length of an input sequence configs.max_seq_length = 128 # Prepare GLUE task. configs.task_name = \"MRPC\".lower() configs.processor = processors[configs.task_name]() configs.output_mode = output_modes[configs.task_name] configs.label_list = configs.processor.get_labels() configs.model_type = \"bert\".lower() configs.do_lower_case = True # Set the device, batch size, topology, and caching flags. configs.device = \"cpu\" configs.per_gpu_eval_batch_size = 8 configs.n_gpu = 0 configs.local_rank = -1 configs.overwrite_cache = False # Set random seed for reproducibility. def set_seed(seed): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) set_seed(42) 2.2 加载经过微调的 BERT 模型 我们从configs.output_dir加载标记器和经过微调的 BERT 序列分类器模型（FP32）。 tokenizer = BertTokenizer.from_pretrained( configs.output_dir, do_lower_case=configs.do_lower_case) model = BertForSequenceClassification.from_pretrained(configs.output_dir) model.to(configs.device) 2.3 定义分词和评估函数 我们重用了 Huggingface 中的分词和评估函数。 # coding=utf-8 # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team. # Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. def evaluate(args, model, tokenizer, prefix=\"\"): # Loop to handle MNLI double evaluation (matched, mis-matched) eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,) eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,) results = {} for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs): eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True) if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]: os.makedirs(eval_output_dir) args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu) # Note that DistributedSampler samples randomly eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset) eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size) # multi-gpu eval if args.n_gpu > 1: model = torch.nn.DataParallel(model) # Eval! logger.info(\"***** Running evaluation {} *****\".format(prefix)) logger.info(\" Num examples = %d\", len(eval_dataset)) logger.info(\" Batch size = %d\", args.eval_batch_size) eval_loss = 0.0 nb_eval_steps = 0 preds = None out_label_ids = None for batch in tqdm(eval_dataloader, desc=\"Evaluating\"): model.eval() batch = tuple(t.to(args.device) for t in batch) with torch.no_grad(): inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]} if args.model_type != 'distilbert': inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None # XLM, DistilBERT and RoBERTa don't use segment_ids outputs = model(**inputs) tmp_eval_loss, logits = outputs[:2] eval_loss += tmp_eval_loss.mean().item() nb_eval_steps += 1 if preds is None: preds = logits.detach().cpu().numpy() out_label_ids = inputs['labels'].detach().cpu().numpy() else: preds = np.append(preds, logits.detach().cpu().numpy(), axis=0) out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0) eval_loss = eval_loss / nb_eval_steps if args.output_mode == \"classification\": preds = np.argmax(preds, axis=1) elif args.output_mode == \"regression\": preds = np.squeeze(preds) result = compute_metrics(eval_task, preds, out_label_ids) results.update(result) output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\") with open(output_eval_file, \"w\") as writer: logger.info(\"***** Eval results {} *****\".format(prefix)) for key in sorted(result.keys()): logger.info(\" %s = %s\", key, str(result[key])) writer.write(\"%s = %s\\n\" % (key, str(result[key]))) return results def load_and_cache_examples(args, task, tokenizer, evaluate=False): if args.local_rank not in [-1, 0] and not evaluate: torch.distributed.barrier() # Make sure only the first process in distributed training process the dataset, and the others will use the cache processor = processors[task]() output_mode = output_modes[task] # Load data features from cache or dataset file cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format( 'dev' if evaluate else 'train', list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), str(task))) if os.path.exists(cached_features_file) and not args.overwrite_cache: logger.info(\"Loading features from cached file %s\", cached_features_file) features = torch.load(cached_features_file) else: logger.info(\"Creating features from dataset file at %s\", args.data_dir) label_list = processor.get_labels() if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']: # HACK(label indices are swapped in RoBERTa pretrained model) label_list[1], label_list[2] = label_list[2], label_list[1] examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir) features = convert_examples_to_features(examples, tokenizer, label_list=label_list, max_length=args.max_seq_length, output_mode=output_mode, pad_on_left=bool(args.model_type in ['xlnet']), # pad on the left for xlnet pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0], pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0, ) if args.local_rank in [-1, 0]: logger.info(\"Saving features into cached file %s\", cached_features_file) torch.save(features, cached_features_file) if args.local_rank == 0 and not evaluate: torch.distributed.barrier() # Make sure only the first process in distributed training process the dataset, and the others will use the cache # Convert to Tensors and build dataset all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long) all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long) if output_mode == \"classification\": all_labels = torch.tensor([f.label for f in features], dtype=torch.long) elif output_mode == \"regression\": all_labels = torch.tensor([f.label for f in features], dtype=torch.float) dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels) return dataset 3.应用动态量化 我们在模型上调用torch.quantization.quantize_dynamic，将动态量化应用于 HuggingFace BERT 模型。 特别， 我们指定要对模型中的torch.nn.Linear模块进行量化； 我们指定希望将权重转换为量化的int8值。 quantized_model = torch.quantization.quantize_dynamic( model, {torch.nn.Linear}, dtype=torch.qint8 ) print(quantized_model) 3.1 检查模型大小 首先，检查模型尺寸。 我们可以观察到模型大小的显着减小（FP32 总大小：438 MB； INT8 总大小：181 MB）： def print_size_of_model(model): torch.save(model.state_dict(), \"temp.p\") print('Size (MB):', os.path.getsize(\"temp.p\")/1e6) os.remove('temp.p') print_size_of_model(model) print_size_of_model(quantized_model) 本教程中使用的 BERT 模型（bert-base-uncased）的词汇量V为 30522。在嵌入量为 768 的情况下，单词嵌入表的总大小为~4 (Bytes/FP32) * 30522 * 768 = 90 MB 。 因此，借助量化，非嵌入表部分的模型大小从 350 MB（FP32 模型）减少到 90 MB（INT8 模型）。 3.2 评估推理准确率和时间 接下来，我们比较一下动态量化后原始 FP32 模型和 INT8 模型之间的推断时间以及评估精度。 def time_model_evaluation(model, configs, tokenizer): eval_start_time = time.time() result = evaluate(configs, model, tokenizer, prefix=\"\") eval_end_time = time.time() eval_duration_time = eval_end_time - eval_start_time print(result) print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time)) # Evaluate the original FP32 BERT model time_model_evaluation(model, configs, tokenizer) # Evaluate the INT8 BERT model after the dynamic quantization time_model_evaluation(quantized_model, configs, tokenizer) 在 MacBook Pro 上本地运行此程序，无需进行量化，推理（对于 MRPC 数据集中的所有 408 个示例）大约需要 160 秒，而进行量化则只需大约 90 秒。 我们总结了在 Macbook Pro 上运行量化 BERT 模型推断的结果，如下所示： | Prec | F1 score | Model Size | 1 thread | 4 threads | | FP32 | 0.9019 | 438 MB | 160 sec | 85 sec | | INT8 | 0.902 | 181 MB | 90 sec | 46 sec | 在 MRPC 任务的微调 BERT 模型上应用训练后动态量化后，我们的 F1 分数准确率为 0.6%。 作为比较，在最新论文（表 1）中，通过应用训练后动态量化，可以达到 0.8788；通过应用量化感知训练，可以达到 0.8956。 主要区别在于我们在 PyTorch 中支持非对称量化，而该论文仅支持对称量化。 请注意，在本教程中，为了进行单线程比较，我们将线程数设置为 1。 我们还为这些量化的 INT8 运算符支持运算内并行化。 用户现在可以通过torch.set_num_threads(N)设置多线程（N是内部运算并行线程的数量）。 启用帧内并行支持的一项初步要求是使用正确的后端（例如 OpenMP，Native 或 TBB）构建 PyTorch。 您可以使用torch.__config__.parallel_info()检查并行化设置。 在使用 PyTorch 和本机后端进行并行化的同一台 MacBook Pro 上，我们可以花大约 46 秒的时间来处理 MRPC 数据集的评估。 3.3 序列化量化模型 跟踪模型后，我们可以使用torch.jit.save序列化并保存量化模型，以备将来使用。 input_ids = ids_tensor([8, 128], 2) token_type_ids = ids_tensor([8, 128], 2) attention_mask = ids_tensor([8, 128], vocab_size=2) dummy_input = (input_ids, attention_mask, token_type_ids) traced_model = torch.jit.trace(quantized_model, dummy_input) torch.jit.save(traced_model, \"bert_traced_eager_quant.pt\") 要加载量化模型，我们可以使用torch.jit.load loaded_quantized_model = torch.jit.load(\"bert_traced_eager_quant.pt\") 总结 在本教程中，我们演示了如何演示如何将 BERT 等著名的最新 NLP 模型转换为动态量化模型。 动态量化可以减小模型的大小，而对准确率的影响有限。 谢谢阅读！ 与往常一样，我们欢迎您提供反馈，因此，如果有任何问题，请在这里创建一个 ISSUE。 参考文献 [1] J.Devlin, M. Chang, K. Lee and K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018). [2] HuggingFace Transformers. [3] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat (2019). Q8BERT: Quantized 8bit BERT. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"57.html":{"url":"57.html","title":"PyTorch 中使用 Eager 模式的静态量化（beta）","keywords":"","body":"PyTorch 中使用 Eager 模式的静态量化（beta） 原文：https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html# 作者： Raghuraman Krishnamoorthi 编辑：Seth Weidman 本教程说明了如何进行训练后的静态量化，并说明了两种更先进的技术-每通道量化和量化感知训练-可以进一步提高模型的准确率。 请注意，目前仅支持 CPU 量化，因此在本教程中我们将不使用 GPU/CUDA。 在本教程结束时，您将看到 PyTorch 中的量化如何导致模型大小显着减小同时提高速度。 此外，您将在此处看到如何轻松应用显示的一些高级量化技术，以使您的量化模型受到的准确率影响要小得多。 警告：我们使用了许多其他 PyTorch 仓库中的样板代码，例如，定义了MobileNetV2模型架构，定义了数据加载器等等。 我们当然鼓励您阅读它； 但是如果要使用量化功能，请随时跳到“ 4。 训练后静态量化”部分。 我们将从进行必要的导入开始： import numpy as np import torch import torch.nn as nn import torchvision from torch.utils.data import DataLoader from torchvision import datasets import torchvision.transforms as transforms import os import time import sys import torch.quantization # # Setup warnings import warnings warnings.filterwarnings( action='ignore', category=DeprecationWarning, module=r'.*' ) warnings.filterwarnings( action='default', module=r'torch.quantization' ) # Specify random seed for repeatable results torch.manual_seed(191009) 1.模型架构 我们首先定义 MobileNetV2 模型架构，并进行了一些值得注意的修改以实现量化： 用nn.quantized.FloatFunctional代替添加 在网络的开头和结尾处插入QuantStub和DeQuantStub。 用 ReLU 替换 ReLU6 注意：此代码取自此处。 from torch.quantization import QuantStub, DeQuantStub def _make_divisible(v, divisor, min_value=None): \"\"\" This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by 8 It can be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py :param v: :param divisor: :param min_value: :return: \"\"\" if min_value is None: min_value = divisor new_v = max(min_value, int(v + divisor / 2) // divisor * divisor) # Make sure that round down does not go down by more than 10%. if new_v 2.辅助函数 接下来，我们定义一些助手函数以帮助模型评估。 这些主要来自这里。 class AverageMeter(object): \"\"\"Computes and stores the average and current value\"\"\" def __init__(self, name, fmt=':f'): self.name = name self.fmt = fmt self.reset() def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count def __str__(self): fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})' return fmtstr.format(**self.__dict__) def accuracy(output, target, topk=(1,)): \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\" with torch.no_grad(): maxk = max(topk) batch_size = target.size(0) _, pred = output.topk(maxk, 1, True, True) pred = pred.t() correct = pred.eq(target.view(1, -1).expand_as(pred)) res = [] for k in topk: correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True) res.append(correct_k.mul_(100.0 / batch_size)) return res def evaluate(model, criterion, data_loader, neval_batches): model.eval() top1 = AverageMeter('Acc@1', ':6.2f') top5 = AverageMeter('Acc@5', ':6.2f') cnt = 0 with torch.no_grad(): for image, target in data_loader: output = model(image) loss = criterion(output, target) cnt += 1 acc1, acc5 = accuracy(output, target, topk=(1, 5)) print('.', end = '') top1.update(acc1[0], image.size(0)) top5.update(acc5[0], image.size(0)) if cnt >= neval_batches: return top1, top5 return top1, top5 def load_model(model_file): model = MobileNetV2() state_dict = torch.load(model_file) model.load_state_dict(state_dict) model.to('cpu') return model def print_size_of_model(model): torch.save(model.state_dict(), \"temp.p\") print('Size (MB):', os.path.getsize(\"temp.p\")/1e6) os.remove('temp.p') 3.定义数据集和数据加载器 作为最后的主要设置步骤，我们为训练和测试集定义了数据加载器。 ImageNet 数据 我们为本教程创建的特定数据集仅包含来自 ImageNet 数据的 1000 张图像，每个类别都有一张（此数据集的大小刚好超过 250 MB，可以相对轻松地下载）。 此自定义数据集的 URL 为： https://s3.amazonaws.com/pytorch-tutorial-assets/imagenet_1k.zip 要使用 Python 在本地下载此数据，可以使用： import requests url = 'https://s3.amazonaws.com/pytorch-tutorial-assets/imagenet_1k.zip` filename = '~/Downloads/imagenet_1k_data.zip' r = requests.get(url) with open(filename, 'wb') as f: f.write(r.content) 为了运行本教程，我们下载了这些数据，并使用 Makefile 中的这些行将其移到正确的位置。 另一方面，要使用整个 ImageNet 数据集运行本教程中的代码，可以在此之后使用torchvision下载数据。 例如，要下载训练集并对其进行一些标准转换，可以使用： import torchvision import torchvision.transforms as transforms imagenet_dataset = torchvision.datasets.ImageNet( '~/.data/imagenet', split='train', download=True, transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) 下载完数据后，我们在下面显示了一些函数，这些函数定义了将用于读取此数据的数据加载器。 这些函数主要来自此处。 def prepare_data_loaders(data_path): traindir = os.path.join(data_path, 'train') valdir = os.path.join(data_path, 'val') normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) dataset = torchvision.datasets.ImageFolder( traindir, transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ])) dataset_test = torchvision.datasets.ImageFolder( valdir, transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ])) train_sampler = torch.utils.data.RandomSampler(dataset) test_sampler = torch.utils.data.SequentialSampler(dataset_test) data_loader = torch.utils.data.DataLoader( dataset, batch_size=train_batch_size, sampler=train_sampler) data_loader_test = torch.utils.data.DataLoader( dataset_test, batch_size=eval_batch_size, sampler=test_sampler) return data_loader, data_loader_test 接下来，我们将加载经过预先​​训练的 MobileNetV2 模型。 我们在这里提供用于从torchvision中下载数据的 URL。 data_path = 'data/imagenet_1k' saved_model_dir = 'data/' float_model_file = 'mobilenet_pretrained_float.pth' scripted_float_model_file = 'mobilenet_quantization_scripted.pth' scripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth' train_batch_size = 30 eval_batch_size = 30 data_loader, data_loader_test = prepare_data_loaders(data_path) criterion = nn.CrossEntropyLoss() float_model = load_model(saved_model_dir + float_model_file).to('cpu') 接下来，我们将“融合模块”； 通过节省内存访问量，这可以使模型更快，同时还可以提高数值精度。 尽管这可以用于任何模型，但在量化模型中尤为常见。 print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv) float_model.eval() # Fuses modules float_model.fuse_model() # Note fusion of Conv+BN+Relu and Conv+Relu print('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv) 出： Inverted Residual Block: Before fusion Sequential( (0): ConvBNReLU( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) Inverted Residual Block: After fusion Sequential( (0): ConvBNReLU( (0): ConvReLU2d( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32) (1): ReLU() ) (1): Identity() (2): Identity() ) (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1)) (2): Identity() ) 最后，为了获得“基准”精度，让我们看看带有融合模块的未量化模型的精度 num_eval_batches = 10 print(\"Size of baseline model\") print_size_of_model(float_model) top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches) print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg)) torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file) 出： Size of baseline model Size (MB): 13.999657 ..........Evaluation accuracy on 300 images, 77.67 我们看到 300 张图像的准确率达到 78%，这是 ImageNet 的坚实基础，特别是考虑到我们的模型只有 14.0 MB。 这将是我们比较的基准。 接下来，让我们尝试不同的量化方法 4.训练后的静态量化 训练后的静态量化不仅涉及像动态量化中那样将权重从float转换为int，而且还执行额外的步骤，即首先通过网络馈送一批数据并计算不同激活的结果分布（具体来说，这通过在记录此数据的不同点插入观察者模块来完成）。 然后使用这些分布来确定在推理时如何具体量化不同的激活（一种简单的技术是将整个激活范围简单地划分为 256 个级别，但我们也支持更复杂的方法）。 重要的是，此附加步骤使我们能够在操作之间传递量化值，而不是在每次操作之间将这些值转换为浮点数，然后再转换为整数，从而显着提高了速度。 num_calibration_batches = 10 myModel = load_model(saved_model_dir + float_model_file).to('cpu') myModel.eval() # Fuse Conv, bn and relu myModel.fuse_model() # Specify quantization configuration # Start with simple min/max range estimation and per-tensor quantization of weights myModel.qconfig = torch.quantization.default_qconfig print(myModel.qconfig) torch.quantization.prepare(myModel, inplace=True) # Calibrate first print('Post Training Quantization Prepare: Inserting Observers') print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv) # Calibrate with the training set evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches) print('Post Training Quantization: Calibration done') # Convert to quantized model torch.quantization.convert(myModel, inplace=True) print('Post Training Quantization: Convert done') print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv) print(\"Size of model after quantization\") print_size_of_model(myModel) top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches) print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg)) 出： QConfig(activation=functools.partial(, reduce_range=True), weight=functools.partial(, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)) Post Training Quantization Prepare: Inserting Observers Inverted Residual Block:After observer insertion Sequential( (0): ConvBNReLU( (0): ConvReLU2d( (0): Conv2d( 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32 (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf) ) (1): ReLU( (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf) ) ) (1): Identity() (2): Identity() ) (1): Conv2d( 32, 16, kernel_size=(1, 1), stride=(1, 1) (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf) ) (2): Identity() ) ..........Post Training Quantization: Calibration done Post Training Quantization: Convert done Inverted Residual Block: After fusion and quantization, note fused modules: Sequential( (0): ConvBNReLU( (0): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.1516050398349762, zero_point=0, padding=(1, 1), groups=32) (1): Identity() (2): Identity() ) (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.17719413340091705, zero_point=63) (2): Identity() ) Size of model after quantization Size (MB): 3.631847 ..........Evaluation accuracy on 300 images, 66.67 对于这个量化模型，我们发现在这 300 张相同的图像上，准确率仅低至约 62%。 不过，我们确实将模型的大小减小到了 3.6 MB 以下，几乎减少了 4 倍。 此外，我们可以通过使用不同的量化配置来显着提高准确率。 我们使用推荐的配置对 x86 架构进行量化，重复相同的练习。 此配置执行以下操作： 量化每个通道的权重 使用直方图观察器，该直方图观察器收集激活的直方图，然后以最佳方式选择量化参数。 per_channel_quantized_model = load_model(saved_model_dir + float_model_file) per_channel_quantized_model.eval() per_channel_quantized_model.fuse_model() per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') print(per_channel_quantized_model.qconfig) torch.quantization.prepare(per_channel_quantized_model, inplace=True) evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches) torch.quantization.convert(per_channel_quantized_model, inplace=True) top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches) print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg)) torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file) 出： QConfig(activation=functools.partial(, reduce_range=True), weight=functools.partial(, dtype=torch.qint8, qscheme=torch.per_channel_symmetric)) ....................Evaluation accuracy on 300 images, 74.67 仅更改这种量化配置方法，就可以将准确率提高到 76% 以上！ 尽管如此，这仍比上述 78% 的基准差 1-2%。 因此，让我们尝试量化意识的训练。 5.量化感知的训练 量化感知的训练（QAT）是通常导致最高准确率的量化方法。 使用 QAT，在训练的正向和反向过程中，所有权重和激活都被“伪量化”：即，浮点值四舍五入以模拟int8值，但所有计算仍使用浮点数完成。 因此，在训练过程中进行所有权重调整，同时“意识到”该模型将最终被量化的事实。 因此，在量化之后，此方法通常会比动态量化或训练后静态量化产生更高的精度。 实际执行 QAT 的总体工作流程与之前非常相似： 我们可以使用与以前相同的模型：量化感知的训练不需要额外的准备。 我们需要使用qconfig来指定要在权重和激活之后插入哪种伪量化，而不是指定观察者 我们首先定义一个训练函数： def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches): model.train() top1 = AverageMeter('Acc@1', ':6.2f') top5 = AverageMeter('Acc@5', ':6.2f') avgloss = AverageMeter('Loss', '1.5f') cnt = 0 for image, target in data_loader: start_time = time.time() print('.', end = '') cnt += 1 image, target = image.to(device), target.to(device) output = model(image) loss = criterion(output, target) optimizer.zero_grad() loss.backward() optimizer.step() acc1, acc5 = accuracy(output, target, topk=(1, 5)) top1.update(acc1[0], image.size(0)) top5.update(acc5[0], image.size(0)) avgloss.update(loss, image.size(0)) if cnt >= ntrain_batches: print('Loss', avgloss.avg) print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}' .format(top1=top1, top5=top5)) return print('Full imagenet train set: * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}' .format(top1=top1, top5=top5)) return 我们像以前一样融合模块 qat_model = load_model(saved_model_dir + float_model_file) qat_model.fuse_model() optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001) qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') 最后，prepare_qat执行“伪量化”，为量化感知训练准备模型 torch.quantization.prepare_qat(qat_model, inplace=True) print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv) 出： Inverted Residual Block: After preparation for QAT, note fake-quantization modules Sequential( (0): ConvBNReLU( (0): ConvBnReLU2d( 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (weight_fake_quant): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) (activation_post_process): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf) ) ) (1): Identity() (2): Identity() ) (1): ConvBn2d( 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (weight_fake_quant): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) (activation_post_process): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf) ) ) (2): Identity() ) 高精度训练量化模型需要在推断时对数字进行精确建模。 因此，对于量化感知的训练，我们通过以下方式修改训练循环： 在训练快要结束时切换批量规范以使用运行均值和方差，以更好地匹配推理数字。 我们还冻结了量化器参数（比例和零点），并对权重进行了微调。 num_train_batches = 20 # Train and check accuracy after each epoch for nepoch in range(8): train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches) if nepoch > 3: # Freeze quantizer parameters qat_model.apply(torch.quantization.disable_observer) if nepoch > 2: # Freeze batch norm mean and variance estimates qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats) # Check the accuracy after each epoch quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False) quantized_model.eval() top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches) print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg)) 出： ....................Loss tensor(2.0747, grad_fn=) Training: * Acc@1 56.167 Acc@5 77.333 ..........Epoch 0 :Evaluation accuracy on 300 images, 77.67 ....................Loss tensor(2.0358, grad_fn=) Training: * Acc@1 54.833 Acc@5 78.500 ..........Epoch 1 :Evaluation accuracy on 300 images, 77.00 ....................Loss tensor(2.0417, grad_fn=) Training: * Acc@1 54.667 Acc@5 77.333 ..........Epoch 2 :Evaluation accuracy on 300 images, 74.67 ....................Loss tensor(1.9055, grad_fn=) Training: * Acc@1 56.833 Acc@5 78.667 ..........Epoch 3 :Evaluation accuracy on 300 images, 76.33 ....................Loss tensor(1.9055, grad_fn=) Training: * Acc@1 58.167 Acc@5 80.000 ..........Epoch 4 :Evaluation accuracy on 300 images, 77.00 ....................Loss tensor(1.7821, grad_fn=) Training: * Acc@1 60.500 Acc@5 82.833 ..........Epoch 5 :Evaluation accuracy on 300 images, 76.33 ....................Loss tensor(1.8145, grad_fn=) Training: * Acc@1 58.833 Acc@5 82.333 ..........Epoch 6 :Evaluation accuracy on 300 images, 74.33 ....................Loss tensor(1.6930, grad_fn=) Training: * Acc@1 63.000 Acc@5 81.333 ..........Epoch 7 :Evaluation accuracy on 300 images, 75.67 在这里，我们只对少数几个周期执行量化感知训练。 尽管如此，量化感知的训练在整个 imagenet 数据集上的准确率仍超过 71%，接近浮点精度 71.9%。 有关量化感知的训练的更多信息： QAT 是训练后量化技术的超集，可以进行更多调试。 例如，我们可以分析模型的准确率是否受到权重或激活量化的限制。 由于我们使用伪量化来对实际量化算术的数值建模，因此我们还可以在浮点中模拟量化模型的准确率。 我们也可以轻松地模拟训练后量化。 来自量化的加速 最后，让我们确认一下我们上面提到的内容：量化模型实际上执行推理的速度更快吗？ 让我们测试一下： def run_benchmark(model_file, img_loader): elapsed = 0 model = torch.jit.load(model_file) model.eval() num_batches = 5 # Run the scripted model on a few batches of images for i, (images, target) in enumerate(img_loader): if i 出： Elapsed time: 7 ms Elapsed time: 4 ms 在 MacBook Pro 上本地运行此程序，常规模型的运行时间为 61 毫秒，而量化模型的运行时间仅为 20 毫秒，这说明了量化模型与浮点模型相比，典型的 2-4 倍加速。 总结 在本教程中，我们展示了两种量化方法-训练后静态量化和量化感知训练-描述它们在“幕后”进行的操作以及如何在 PyTorch 中使用它们。 谢谢阅读！ 与往常一样，我们欢迎您提供反馈，因此，如果有任何问题，请在这里创建一个 ISSUE。 脚本的总运行时间：（5 分钟 40.226 秒） 下载 Python 源码：static_quantization_tutorial.py 下载 Jupyter 笔记本：static_quantization_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"58.html":{"url":"58.html","title":"计算机视觉的量化迁移学习教程（beta）","keywords":"","body":"计算机视觉的量化迁移学习教程（beta） 原文：https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html 小费 为了充分利用本教程，我们建议使用此 Colab 版本。 这将使您可以尝试以下信息。 作者： Zafar Takhirov 由审核： Raghuraman Krishnamoorthi 编辑：Jessica Lin 本教程以 Sasank Chilamkurthy 编写的原始 PyTorch 迁移学习教程为基础。 迁移学习是指利用预训练的模型应用于不同数据集的技术。 使用迁移学习的主要方法有两种： 作为固定特征提取器的 ConvNet：在这里，您“冻结”网络中所有参数的权重，除了最后几层（又称“头部”，通常是全连接层）。 将这些最后一层替换为使用随机权重初始化的新层，并且仅训练这些层。 ConvNet 的微调：使用随机训练的网络初始化模型，而不是随机初始化，然后像往常一样使用不同的数据集进行训练。 通常，如果输出数量不同，则在网络中也会更换头部（或头部的一部分）。 这种方法通常将学习率设置为较小的值。 这样做是因为已经对网络进行了训练，并且只需进行较小的更改即可将其“微调”到新的数据集。 您还可以结合以上两种方法：首先，可以冻结特征提取器，并训练头部。 之后，您可以解冻特征提取器（或其一部分），将学习率设置为较小的值，然后继续进行训练。 在本部分中，您将使用第一种方法-使用量化模型提取特征。 第 0 部分，先决条件 在深入学习迁移学习之前，让我们回顾一下“先决条件”，例如安装和数据加载/可视化。 # Imports import copy import matplotlib.pyplot as plt import numpy as np import os import time plt.ion() 安装每夜构建 因为您将使用 PyTorch 的 Beta 部分，所以建议安装最新版本的torch和torchvision。 您可以在这里找到有关本地安装的最新说明。 例如，要在没有 GPU 支持的情况下进行安装： pip install numpy pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html # For CUDA support use https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html 加载数据 注意 本部分与原始的迁移学习教程相同。 我们将使用torchvision和torch.utils.data包加载数据。 您今天要解决的问题是从图像中对蚂蚁和蜜蜂进行分类。 该数据集包含约 120 张针对蚂蚁和蜜蜂的训练图像。 每个类别有 75 个验证图像。 可以认为这是一个很小的数据集。 但是，由于我们正在使用迁移学习，因此我们应该能够很好地进行概括。 此数据集是 imagenet 的很小子集。 注意 从此处下载数据，并将其提取到data目录。 import torch from torchvision import transforms, datasets # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train': transforms.Compose([ transforms.Resize(224), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = 'data/hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=8) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") 可视化一些图像 让我们可视化一些训练图像，以了解数据扩充。 import torchvision def imshow(inp, title=None, ax=None, figsize=(5, 5)): \"\"\"Imshow for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) if ax is None: fig, ax = plt.subplots(1, figsize=figsize) ax.imshow(inp) ax.set_xticks([]) ax.set_yticks([]) if title is not None: ax.set_title(title) # Get a batch of training data inputs, classes = next(iter(dataloaders['train'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs, nrow=4) fig, ax = plt.subplots(1, figsize=(10, 10)) imshow(out, title=[class_names[x] for x in classes], ax=ax) 模型训练的支持函数 以下是模型训练的通用函数。 此函数也： 安排学习率 保存最佳模型 def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'): \"\"\" Support function for model training. Args: model: Model to be trained criterion: Optimization criterion (loss) optimizer: Optimizer to use for training scheduler: Instance of ``torch.optim.lr_scheduler`` num_epochs: Number of epochs device: Device to run the training on. Must be 'cpu' or 'cuda' \"\"\" since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == 'train': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model 可视化模型预测的支持函数 通用函数，显示一些图像的预测 def visualize_model(model, rows=3, cols=3): was_training = model.training model.eval() current_row = current_col = 0 fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2)) with torch.no_grad(): for idx, (imgs, lbls) in enumerate(dataloaders['val']): imgs = imgs.cpu() lbls = lbls.cpu() outputs = model(imgs) _, preds = torch.max(outputs, 1) for jdx in range(imgs.size()[0]): imshow(imgs.data[jdx], ax=ax[current_row, current_col]) ax[current_row, current_col].axis('off') ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]])) current_col += 1 if current_col >= cols: current_row += 1 current_col = 0 if current_row >= rows: model.train(mode=was_training) return model.train(mode=was_training) 第 1 部分，基于量化特征提取器训练自定义分类器 在本节中，您将使用“冻结”量化特征提取器，并在其顶部训练自定义分类器头。 与浮点模型不同，您无需为量化模型设置require_grad = False，因为它没有可训练的参数。 请参阅文档了解更多详细信息。 加载预训练的模型：在本练习中，您将使用 ResNet-18 。 import torchvision.models.quantization as models # You will need the number of filters in the `fc` for future use. # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_fe = models.resnet18(pretrained=True, progress=True, quantize=True) num_ftrs = model_fe.fc.in_features 此时，您需要修改预训练模型。 该模型在开始和结束时都有量化/去量化块。 但是，由于只使用特征提取器，因此反量化层必须在线性层（头部）之前移动。 最简单的方法是将模型包装在nn.Sequential模块中。 第一步是在 ResNet 模型中隔离特征提取器。 尽管在本示例中，您被责成使用fc以外的所有层作为特征提取器，但实际上，您可以根据需要选择任意数量的零件。 如果您也想替换一些卷积层，这将很有用。 注意 将特征提取器与量化模型的其余部分分开时，必须手动将量化器/去量化器放置在要保持量化的部分的开头和结尾。 下面的函数创建一个带有自定义头部的模型。 from torch import nn def create_combined_model(model_fe): # Step 1\\. Isolate the feature extractor. model_fe_features = nn.Sequential( model_fe.quant, # Quantize the input model_fe.conv1, model_fe.bn1, model_fe.relu, model_fe.maxpool, model_fe.layer1, model_fe.layer2, model_fe.layer3, model_fe.layer4, model_fe.avgpool, model_fe.dequant, # Dequantize the output ) # Step 2\\. Create a new \"head\" new_head = nn.Sequential( nn.Dropout(p=0.5), nn.Linear(num_ftrs, 2), ) # Step 3\\. Combine, and don't forget the quant stubs. new_model = nn.Sequential( model_fe_features, nn.Flatten(1), new_head, ) return new_model 警告 当前，量化模型只能在 CPU 上运行。 但是，可以将模型的未量化部分发送到 GPU。 import torch.optim as optim new_model = create_combined_model(model_fe) new_model = new_model.to('cpu') criterion = nn.CrossEntropyLoss() # Note that we are only training the head. optimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) 训练和评估 此步骤在 CPU 上大约需要 15-25 分钟。 由于量化模型只能在 CPU 上运行，因此您不能在 GPU 上运行训练。 new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25, device='cpu') visualize_model(new_model) plt.tight_layout() 第 2 部分，微调量化模型 在这一部分中，我们将微调用于迁移学习的特征提取器，并对特征提取器进行量化。 请注意，在第 1 部分和第 2 部分中，特征提取器都是量化的。 不同之处在于，在第 1 部分中，我们使用了预训练的量化模型。 在这一部分中，我们将在对感兴趣的数据集进行微调之后创建一个量化的特征提取器，因此这是一种在具有量化优势的同时通过迁移学习获得更好的准确率的方法。 请注意，在我们的特定示例中，训练集非常小（120 张图像），因此微调整个模型的好处并不明显。 但是，此处显示的过程将提高使用较大数据集进行传递学习的准确率。 预训练特征提取器必须是可量化的。 为确保其可量化，请执行以下步骤： 使用 torch.quantization.fuse_modules 熔断 (Conv, BN, ReLU) ， (Conv, BN) 和 (Conv, ReLU) 。 将特征提取器与自定义头部连接。 这需要对特征提取器的输出进行反量化。 在特征提取器中的适当位置插入伪量化模块，以模拟训练期间的量化。 对于步骤（1），我们使用torchvision/models/quantization中的模型，这些模型具有成员方法fuse_model。 此函数将所有conv，bn和relu模块融合在一起。 对于自定义模型，这需要使用模块列表调用torch.quantization.fuse_modules API 进行手动融合。 步骤（2）由上一节中使用的create_combined_model函数执行。 步骤（3）通过使用torch.quantization.prepare_qat来实现，它会插入伪量化模块。 在步骤（4）中，您可以开始“微调”模型，然后将其转换为完全量化的版本（步骤 5）。 要将微调模型转换为量化模型，可以调用torch.quantization.convert函数（在我们的情况下，仅对特征提取器进行量化）。 注意 由于随机初始化，您的结果可能与本教程中显示的结果不同。 # notice quantize=False model = models.resnet18(pretrained=True, progress=True, quantize=False) num_ftrs = model.fc.in_features # Step 1 model.train() model.fuse_model() # Step 2 model_ft = create_combined_model(model) model_ft[0].qconfig = torch.quantization.default_qat_qconfig # Use default QAT configuration # Step 3 model_ft = torch.quantization.prepare_qat(model_ft, inplace=True) 微调模型 在当前教程中，整个模型都经过了微调。 通常，这将导致更高的精度。 但是，由于此处使用的训练集很小，最终导致我们过度适应了训练集。 步骤 4.微调模型 for param in model_ft.parameters(): param.requires_grad = True model_ft.to(device) # We can fine-tune on GPU if available criterion = nn.CrossEntropyLoss() # Note that we are training everything, so the learning rate is lower # Notice the smaller learning rate optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1) # Decay LR by a factor of 0.3 every several epochs exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3) model_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25, device=device) 步骤 5.转换为量化模型 from torch.quantization import convert model_ft_tuned.cpu() model_quantized_and_trained = convert(model_ft_tuned, inplace=False) 让我们看看量化模型在几张图像上的表现 visualize_model(model_quantized_and_trained) plt.ioff() plt.tight_layout() plt.show() 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"59.html":{"url":"59.html","title":"并行和分布式训练","keywords":"","body":"并行和分布式训练 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"60.html":{"url":"60.html","title":"PyTorch 分布式概述","keywords":"","body":"PyTorch 分布式概述 原文：https://pytorch.org/tutorials/beginner/dist_overview.html 作者：Shen Li 这是torch.distributed包的概述页面。 由于在不同位置添加了越来越多的文档，示例和教程，因此不清楚要针对特定​​问题咨询哪个文档或教程，或者阅读这些内容的最佳顺序是什么。 该页面的目的是通过将文档分类为不同的主题并简要描述每个主题来解决此问题。 如果这是您第一次使用 PyTorch 构建分布式训练应用，建议使用本文档导航至最适合您的用例的技术。 简介 从 PyTorch v1.6.0 开始，torch.distributed中的功能可以分为三个主要组件： 分布式数据并行训练（DDP）是一种广泛采用的单程序多数据训练范例。 使用 DDP，可以在每个流程上复制模型，并且每个模型副本都将获得一组不同的输入数据样本。 DDP 负责梯度通信，以保持模型副本同步，并使其与梯度计算重叠，以加快训练速度。 基于 RPC 的分布式训练（RPC）开发来支持无法适应数据并行训练的常规训练结构，例如分布式管道并行性，参数服务器范式以及 DDP 与其他训练范式的组合。 它有助于管理远程对象的生命周期，并将自动微分引擎扩展到机器范围之外。 集体通信（c10d）库支持跨组内的进程发送张量。 它提供了集体通信 API（例如all_reduce和all_gather）和 P2P 通信 API（例如send和 isend）。 从 v1.6.0 开始，DDP 和 RPC（ProcessGroup 后端）建立在 c10d 上，其中前者使用集体通信，而后者使用 P2P 通信。 通常，开发人员无需直接使用此原始通信 API，因为上述 DDP 和 RPC 功能可以满足许多分布式训练方案的需求。 但是，在某些情况下，此 API 仍然很有帮助。 一个示例是分布式参数平均，其中应用希望在反向传播之后计算所有模型参数的平均值，而不是使用 DDP 来传递梯度。 这可以使通信与计算脱钩，并允许对通信内容进行更细粒度的控制，但另一方面，它也放弃了 DDP 提供的性能优化。 用 PyTorch 编写分布式应用显示了使用 c10d 通信 API 的示例。 现有的大多数文档都是为 DDP 或 RPC 编写的，本页面的其余部分将详细介绍这两个组件的材料。 数据并行训练 PyTorch 为数据并行训练提供了几种选择。 对于从简单到复杂以及从原型到生产逐渐增长的应用，共同的发展轨迹将是： 如果数据和模型可以放在一个 GPU 中，并且不关心训练速度，请使用单设备训练。 如果服务器上有多个 GPU，请使用单机多 GPU DataParallel，并且您希望以最少的代码更改来加快训练速度。 如果您想进一步加快训练速度并愿意编写更多代码来设置它，请使用单机多 GPU DistributedDataParallel。 如果应用需要跨计算机边界扩展，请使用多计算机DistributedDataParallel和启动脚本。 如果预计会出现错误（例如，OOM），或者在训练过程中资源可以动态加入和离开，请使用扭弹性启动分布式训练。 注意 数据并行训练还可以与自动混合精度（AMP）一起使用。 torch.nn.DataParallel DataParallel包以最低的编码障碍实现了单机多 GPU 并行处理。 它只需要一行更改应用代码。 教程可选：数据并行显示了一个示例。 需要注意的是，尽管DataParallel非常易于使用，但通常无法提供最佳性能。 这是因为DataParallel的实现会在每个正向传播中复制该模型，并且其单进程多线程并行性自然会遭受 GIL 争用。 为了获得更好的性能，请考虑使用DistributedDataParallel。 torch.nn.parallel.DistributedDataParallel 与DataParallel相比，DistributedDataParallel还需要设置一个步骤，即调用init_process_group。 DDP 使用多进程并行性，因此在模型副本之间没有 GIL 争用。 此外，该模型是在 DDP 构建时而不是在每个正向传播时广播的，这也有助于加快训练速度。 DDP 附带了几种性能优化技术。 有关更深入的说明，请参阅此 DDP 论文（VLDB'20）。 DDP 材料如下： DDP 注解提供了一个入门示例，并简要介绍了其设计和实现。 如果这是您第一次使用 DDP，请从本文档开始。 分布式数据并行入门解释了 DDP 训练的一些常见问题，包括不平衡的工作量，检查点和多设备模型。 请注意，DDP 可以轻松与单机模型并行最佳实践教程中描述的单机多设备模型并行性结合。 启动和配置分布式数据并行应用文档显示了如何使用 DDP 启动脚本。 使用 Amazon AWS 的 PyTorch 分布式训练器演示了如何在 AWS 上使用 DDP。 TorchElastic 随着应用复杂性和规模的增长，故障恢复成为当务之急。 有时，使用 DDP 时不可避免地会遇到 OOM 之类的错误，但是 DDP 本身无法从这些错误中恢复，基本的try-except块也无法工作。 这是因为 DDP 要求所有进程以紧密同步的方式运行，并且在不同进程中启动的所有AllReduce通信都必须匹配。 如果组中的某个进程抛出 OOM 异常，则很可能导致不同步（AllReduce操作不匹配），从而导致崩溃或挂起。 如果您期望在训练过程中发生故障，或者资源可能会动态离开并加入，请使用 Torrlastic 启动分布式数据并行训练。 通用分布式训练 许多训练范式不适合数据并行性，例如参数服务器范式，分布式管道并行性，具有多个观察者或智能体的强化学习应用等。 torch.distributed.rpc旨在支持一般的分布式训练方案 。 torch.distributed.rpc包具有四个主要支柱： RPC 支持在远程工作器上运行给定函数 RRef 帮助管理远程对象的生存期。 引用计数协议在 RRef 注解中提供。 分布式自动微分将自动微分引擎扩展到机器范围之外。 有关更多详细信息，请参考分布式 Autograd 设计。 分布式优化器，它使用分布式 Autograd 引擎计算的梯度自动与所有参与的工作器联系以更新参数。 RPC 教程如下： 分布式 RPC 框架入门教程首先使用一个简单的强化学习（RL）示例来演示 RPC 和 RRef。 然后，它对 RNN 示例应用了基本的分布式模型并行性，以展示如何使用分布式 Autograd 和分布式优化器。 使用分布式 RPC 框架实现参数服务器教程借鉴了 HogWild 的训练精神，并将其应用于异步参数服务器（PS）训练应用。 使用 RPC 的分布式管道并行化教程将单机管道并行示例（在单机模型并行最佳实践中介绍）扩展到了分布式环境，并展示了如何使用 RPC 来实现它 。 使用异步执行实现批量 RPC 教程演示了如何使用@rpc.functions.async_execution装饰器实现 RPC 批量。这可以帮助加速推理和训练。 它使用了以上教程 1 和 2 中采用的类似 RL 和 PS 示例。 将分布式DataParallel与分布式 RPC 框架结合教程演示了如何将 DDP 与 RPC 结合使用分布式数据并行性和分布式模型并行性来训练模型。 PyTorch 分布式开发人员 如果您想为 PyTorch 分布式做出贡献，请参阅我们的开发人员指南。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"61.html":{"url":"61.html","title":"单机模型并行最佳实践","keywords":"","body":"单机模型并行最佳实践 原文：https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html 作者：Shen Li 模型并行在分布式训练技术中被广泛使用。 先前的帖子已经解释了如何使用DataParallel在多个 GPU 上训练神经网络； 此功能将相同的模型复制到所有 GPU，其中每个 GPU 消耗输入数据的不同分区。 尽管它可以极大地加快训练过程，但不适用于模型太大而无法容纳单个 GPU 的某些用例。 这篇文章展示了如何通过使用模型并行解决该问题，与DataParallel相比，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型（具体来说， 假设模型m包含 10 层：使用DataParallel时，每个 GPU 都具有这 10 层中的每一个的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层。 模型并行化的高级思想是将模型的不同子网放置在不同的设备上，并相应地实现forward方法以在设备之间移动中间输出。 由于模型的一部分仅在任何单个设备上运行，因此一组设备可以共同为更大的模型服务。 在本文中，我们将不会尝试构建庞大的模型并将其压缩到有限数量的 GPU 中。 取而代之的是，本文着重展示并行模型的思想。 读者可以将这些想法应用到实际应用中。 注意 对于模型跨越多个服务器的分布式模型并行训练，请参考分布式 RPC 框架入门，以获取示例和详细信息。 基本用法 让我们从包含两个线性层的玩具模型开始。 要在两个 GPU 上运行该模型，只需将每个线性层放置在不同的 GPU 上，然后移动输入和中间输出以匹配层设备。 import torch import torch.nn as nn import torch.optim as optim class ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = torch.nn.Linear(10, 10).to('cuda:0') self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10, 5).to('cuda:1') def forward(self, x): x = self.relu(self.net1(x.to('cuda:0'))) return self.net2(x.to('cuda:1')) 请注意，除了五个to(device)调用将线性层和张量放置在适当的设备上之外，上述ToyModel看起来非常类似于在单个 GPU 上实现它的方式。 那是模型中唯一需要更改的地方。 backward()和torch.optim将自动处理梯度，就像模型在一个 GPU 上一样。 调用损失函数时，只需确保标签与输出位于同一设备上。 model = ToyModel() loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.001) optimizer.zero_grad() outputs = model(torch.randn(20, 10)) labels = torch.randn(20, 5).to('cuda:1') loss_fn(outputs, labels).backward() optimizer.step() 将模型并行应用于现有模块 只需进行几行更改，就可以在多个 GPU 上运行现有的单 GPU 模块。 以下代码显示了如何将torchvision.models.resnet50()分解为两个 GPU。 这个想法是继承现有的ResNet模块，并在构建过程中将层拆分为两个 GPU。 然后，通过相应地移动中间输出，覆盖forward方法来缝合两个子网。 from torchvision.models.resnet import ResNet, Bottleneck num_classes = 1000 class ModelParallelResNet50(ResNet): def __init__(self, *args, **kwargs): super(ModelParallelResNet50, self).__init__( Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs) self.seq1 = nn.Sequential( self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2 ).to('cuda:0') self.seq2 = nn.Sequential( self.layer3, self.layer4, self.avgpool, ).to('cuda:1') self.fc.to('cuda:1') def forward(self, x): x = self.seq2(self.seq1(x).to('cuda:1')) return self.fc(x.view(x.size(0), -1)) 对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，您可能已经注意到，如果模型合适，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在layer2和layer3之间从cuda:0复制到cuda:1，因此性能进一步恶化。 让我们进行实验以更定量地了解执行时间。 在此实验中，我们通过运行随机输入和标签来训练ModelParallelResNet50和现有的torchvision.models.resnet50()。 训练后，模型将不会产生任何有用的预测，但是我们可以对执行时间有一个合理的了解。 import torchvision.models as models num_batches = 3 batch_size = 120 image_w = 128 image_h = 128 def train(model): model.train(True) loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.001) one_hot_indices = torch.LongTensor(batch_size) \\ .random_(0, num_classes) \\ .view(batch_size, 1) for _ in range(num_batches): # generate random inputs and labels inputs = torch.randn(batch_size, 3, image_w, image_h) labels = torch.zeros(batch_size, num_classes) \\ .scatter_(1, one_hot_indices, 1) # run forward pass optimizer.zero_grad() outputs = model(inputs.to('cuda:0')) # run backward pass labels = labels.to(outputs.device) loss_fn(outputs, labels).backward() optimizer.step() 上面的train(model)方法使用nn.MSELoss作为损失函数，并使用optim.SGD作为优化器。 它模拟了对128 X 128图像的训练，这些图像分为 3 批，每批包含 120 张图像。 然后，我们使用timeit来运行train(model)方法 10 次，并绘制带有标准差的执行时间。 import matplotlib.pyplot as plt plt.switch_backend('Agg') import numpy as np import timeit num_repeat = 10 stmt = \"train(model)\" setup = \"model = ModelParallelResNet50()\" # globals arg is only available in Python 3\\. In Python 2, use the following # import __builtin__ # __builtin__.__dict__.update(locals()) mp_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) mp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times) setup = \"import torchvision.models as models;\" + \\ \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\" rn_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) rn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times) def plot(means, stds, labels, fig_name): fig, ax = plt.subplots() ax.bar(np.arange(len(means)), means, yerr=stds, align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6) ax.set_ylabel('ResNet50 Execution Time (Second)') ax.set_xticks(np.arange(len(means))) ax.set_xticklabels(labels) ax.yaxis.grid(True) plt.tight_layout() plt.savefig(fig_name) plt.close(fig) plot([mp_mean, rn_mean], [mp_std, rn_std], ['Model Parallel', 'Single GPU'], 'mp_vs_rn.png') 结果表明，模型并行实现的执行时间比现有的单 GPU 实现长4.02/3.75-1=7%。 因此，我们可以得出结论，在 GPU 之间来回复制张量大约有 7% 的开销。 有待改进的地方，因为我们知道两个 GPU 之一在整个执行过程中处于空闲状态。 一种选择是将每个批量进一步划分为拆分流水线，以便当一个拆分到达第二子网时，可以将下一个拆分馈入第一子网。 这样，两个连续的拆分可以在两个 GPU 上同时运行。 通过流水线输入加快速度 在以下实验中，我们将每个 120 图像批量进一步分为 20 图像分割。 当 PyTorch 异步启动 CUDA 操作时，该实现无需生成多个线程即可实现并发。 class PipelineParallelResNet50(ModelParallelResNet50): def __init__(self, split_size=20, *args, **kwargs): super(PipelineParallelResNet50, self).__init__(*args, **kwargs) self.split_size = split_size def forward(self, x): splits = iter(x.split(self.split_size, dim=0)) s_next = next(splits) s_prev = self.seq1(s_next).to('cuda:1') ret = [] for s_next in splits: # A. s_prev runs on cuda:1 s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) # B. s_next runs on cuda:0, which can run concurrently with A s_prev = self.seq1(s_next).to('cuda:1') s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) return torch.cat(ret) setup = \"model = PipelineParallelResNet50()\" pp_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) pp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times) plot([mp_mean, rn_mean, pp_mean], [mp_std, rn_std, pp_std], ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'], 'mp_vs_rn_vs_pp.png') 请注意，设备到设备的张量复制操作在源设备和目标设备上的当前流上同步。 如果创建多个流，则必须确保复制操作正确同步。 在完成复制操作之前写入源张量或读取/写入目标张量可能导致不确定的行为。 上面的实现仅在源设备和目标设备上都使用默认流，因此不必强制执行其他同步。 实验结果表明，对并行 ResNet50 进行建模的流水线输入可大致加快3.75/2.51-1=49%的速度，加快训练过程。 距离理想的 100% 加速仍然相去甚远。 由于我们在管道并行实现中引入了新参数split_sizes，因此尚不清楚新参数如何影响整体训练时间。 直观地讲，使用较小的split_size会导致许多小的 CUDA 内核启动，而使用较大的split_size会导致在第一次和最后一次拆分期间出现较长的空闲时间。 两者都不是最优的。 对于此特定实验，可能会有最佳的split_size配置。 让我们尝试通过使用几个不同的split_size值进行实验来找到它。 means = [] stds = [] split_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60] for split_size in split_sizes: setup = \"model = PipelineParallelResNet50(split_size=%d)\" % split_size pp_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) means.append(np.mean(pp_run_times)) stds.append(np.std(pp_run_times)) fig, ax = plt.subplots() ax.plot(split_sizes, means) ax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro') ax.set_ylabel('ResNet50 Execution Time (Second)') ax.set_xlabel('Pipeline Split Size') ax.set_xticks(split_sizes) ax.yaxis.grid(True) plt.tight_layout() plt.savefig(\"split_size_tradeoff.png\") plt.close(fig) 结果表明，将split_size设置为 12 可获得最快的训练速度，从而导致3.75/2.43-1=54%加速。 仍有机会进一步加快训练过程。 例如，对cuda:0的所有操作都放在其默认流上。 这意味着下一个拆分的计算不能与上一个拆分的复制操作重叠。 但是，由于上一个和下一个分割是不同的张量，因此将一个计算与另一个副本重叠是没有问题的。 实现需要在两个 GPU 上使用多个流，并且不同的子网结构需要不同的流管理策略。 由于没有通用的多流解决方案适用于所有模型并行用例，因此在本教程中将不再讨论。 注意： 这篇文章显示了几个性能指标。 当您在自己的计算机上运行相同的代码时，您可能会看到不同的数字，因为结果取决于底层的硬件和软件。 为了使您的环境获得最佳性能，一种正确的方法是首先生成曲线以找出最佳分割尺寸，然后将该分割尺寸用于管道输入。 脚本的总运行时间：（6 分钟 20.515 秒） 下载 Python 源码：model_parallel_tutorial.py 下载 Jupyter 笔记本：model_parallel_tutorial.ipynb 由 Sphinx 画廊生成的画廊 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"62.html":{"url":"62.html","title":"分布式数据并行入门","keywords":"","body":"分布式数据并行入门 原文：https://pytorch.org/tutorials/intermediate/ddp_tutorial.html 作者：Shen Li 编辑：Joe Zhu 先决条件： PyTorch 分布式概述 DistributedDataParallel API 文档 DistributedDataParallel注意事项 DistributedDataParallel（DDP）在模块级别实现可在多台计算机上运行的数据并行性。 使用 DDP 的应用应产生多个进程，并为每个进程创建一个 DDP 实例。 DDP 在torch.distributed包中使用集体通信来同步梯度和缓冲区。 更具体地说，DDP 为model.parameters()给定的每个参数注册一个 Autograd 挂钩，当在后向传递中计算相应的梯度时，挂钩将触发。 然后，DDP 使用该信号触发跨进程的梯度同步。 有关更多详细信息，请参考 DDP 设计说明。 推荐的使用 DDP 的方法是为每个模型副本生成一个进程，其中一个模型副本可以跨越多个设备。 DDP 进程可以放在同一台计算机上，也可以在多台计算机上，但是 GPU 设备不能在多个进程之间共享。 本教程从一个基本的 DDP 用例开始，然后演示了更高级的用例，包括检查点模型以及将 DDP 与模型并行结合。 注意 本教程中的代码在 8-GPU 服务器上运行，但可以轻松地推广到其他环境。 DataParallel和DistributedDataParallel之间的比较 在深入探讨之前，让我们澄清一下为什么尽管增加了复杂性，但还是考虑使用DistributedDataParallel而不是DataParallel： 首先，DataParallel是单进程，多线程，并且只能在单台机器上运行，而DistributedDataParallel是多进程，并且适用于单机和多机训练。 即使在单台机器上，DataParallel通常也比DistributedDataParallel慢，这是因为跨线程的 GIL 争用，每次迭代复制的模型以及分散输入和收集输出所带来的额外开销。 回顾先前的教程，如果模型太大而无法容纳在单个 GPU 上，则必须使用模型并行将其拆分到多个 GPU 中。 DistributedDataParallel与模型并行一起使用； DataParallel目前没有。 当 DDP 与模型并行组合时，每个 DDP 进程将并行使用模型，而所有进程共同将并行使用数据。 如果您的模型需要跨越多台机器，或者您的用例不适合数据并行性范式，请参阅 RPC API ，以获得更多通用的分布式训练支持。 基本用例 要创建 DDP 模块，请首先正确设置过程组。 更多细节可以在用 PyTorch 编写分布式应用中找到。 import os import sys import tempfile import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP def setup(rank, world_size): if sys.platform == 'win32': # Distributed package only covers collective communications with Gloo # backend and FileStore on Windows platform. Set init_method parameter # in init_process_group to a local file. # Example init_method=\"file:///f:/libtmp/some_file\" init_method=\"file:///{your local file path}\" # initialize the process group dist.init_process_group( \"gloo\", init_method=init_method, rank=rank, world_size=world_size ) else: os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '12355' # initialize the process group dist.init_process_group(\"gloo\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() 现在，让我们创建一个玩具模块，将其与 DDP 封装在一起，并提供一些虚拟输入数据。 请注意，由于 DDP 会将模型状态从等级 0 进程广播到 DDP 构造器中的所有其他进程，因此您不必担心不同的 DDP 进程从不同的模型参数初始值开始。 class ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.net2 = nn.Linear(10, 5) def forward(self, x): return self.net2(self.relu(self.net1(x))) def demo_basic(rank, world_size): print(f\"Running basic DDP example on rank {rank}.\") setup(rank, world_size) # create model and move it to GPU with id rank model = ToyModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) optimizer.zero_grad() outputs = ddp_model(torch.randn(20, 10)) labels = torch.randn(20, 5).to(rank) loss_fn(outputs, labels).backward() optimizer.step() cleanup() def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) 如您所见，DDP 包装了较低级别的分布式通信详细信息，并提供了干净的 API，就好像它是本地模型一样。 梯度同步通信发生在反向传递过程中，并且与反向计算重叠。 当backward()返回时，param.grad已经包含同步梯度张量。 对于基本用例，DDP 仅需要几个 LoC 即可设置流程组。 在将 DDP 应用到更高级的用例时，需要注意一些警告。 带偏差的处理速度 在 DDP 中，构造器，正向传播和反向传递都是分布式同步点。 预期不同的进程将启动相同数量的同步，并以相同的顺序到达这些同步点，并在大致相同的时间进入每个同步点。 否则，快速流程可能会提早到达，并在等待流浪者时超时。 因此，用户负责平衡流程之间的工作负载分配。 有时，由于例如网络延迟，资源争夺，不可预测的工作量峰值，不可避免地会出现处理速度偏差。 为了避免在这种情况下超时，请在调用init_process_group时传递足够大的timeout值。 保存和加载检查点 在训练过程中通常使用torch.save和torch.load来检查点模块并从检查点中恢复。 有关更多详细信息，请参见保存和加载模型。 使用 DDP 时，一种优化方法是仅在一个进程中保存模型，然后将其加载到所有进程中，从而减少写开销。 这是正确的，因为所有过程都从相同的参数开始，并且梯度在反向传播中同步，因此优化程序应将参数设置为相同的值。 如果使用此优化，请确保在保存完成之前不要启动所有进程。 此外，在加载模块时，您需要提供适当的map_location参数，以防止进程进入其他设备。 如果缺少map_location，则torch.load将首先将模块加载到 CPU，然后将每个参数复制到保存位置，这将导致同一台机器上的所有进程使用相同的设备集。 有关更高级的故障恢复和弹性支持，请参考这里。 def demo_checkpoint(rank, world_size): print(f\"Running DDP checkpoint example on rank {rank}.\") setup(rank, world_size) model = ToyModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\" if rank == 0: # All processes should see same parameters as they all start from same # random parameters and gradients are synchronized in backward passes. # Therefore, saving it in one process is sufficient. torch.save(ddp_model.state_dict(), CHECKPOINT_PATH) # Use a barrier() to make sure that process 1 loads the model after process # 0 saves it. dist.barrier() # configure map_location properly map_location = {'cuda:%d' % 0: 'cuda:%d' % rank} ddp_model.load_state_dict( torch.load(CHECKPOINT_PATH, map_location=map_location)) optimizer.zero_grad() outputs = ddp_model(torch.randn(20, 10)) labels = torch.randn(20, 5).to(rank) loss_fn = nn.MSELoss() loss_fn(outputs, labels).backward() optimizer.step() # Not necessary to use a dist.barrier() to guard the file deletion below # as the AllReduce ops in the backward pass of DDP already served as # a synchronization. if rank == 0: os.remove(CHECKPOINT_PATH) cleanup() 将 DDP 与模型并行性结合起来 DDP 还可以与多 GPU 模型一起使用。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 class ToyMpModel(nn.Module): def __init__(self, dev0, dev1): super(ToyMpModel, self).__init__() self.dev0 = dev0 self.dev1 = dev1 self.net1 = torch.nn.Linear(10, 10).to(dev0) self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10, 5).to(dev1) def forward(self, x): x = x.to(self.dev0) x = self.relu(self.net1(x)) x = x.to(self.dev1) return self.net2(x) 将多 GPU 模型传递给 DDP 时，不得设置device_ids和output_device。 输入和输出数据将通过应用或模型forward()方法放置在适当的设备中。 def demo_model_parallel(rank, world_size): print(f\"Running DDP with model parallel example on rank {rank}.\") setup(rank, world_size) # setup mp_model and devices for this process dev0 = rank * 2 dev1 = rank * 2 + 1 mp_model = ToyMpModel(dev0, dev1) ddp_mp_model = DDP(mp_model) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001) optimizer.zero_grad() # outputs will be on dev1 outputs = ddp_mp_model(torch.randn(20, 10)) labels = torch.randn(20, 5).to(dev1) loss_fn(outputs, labels).backward() optimizer.step() cleanup() if __name__ == \"__main__\": n_gpus = torch.cuda.device_count() if n_gpus 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"63.html":{"url":"63.html","title":"用 PyTorch 编写分布式应用","keywords":"","body":"用 PyTorch 编写分布式应用 原文：https://pytorch.org/tutorials/intermediate/dist_tuto.html 作者：SébArnold 先决条件： PyTorch 分布式概述 在这个简短的教程中，我们将介绍 PyTorch 的分布式包。 我们将了解如何设置分布式设置，如何使用不同的交流策略以及如何查看包的一些内部内容。 设置 PyTorch 中包含的分布式包（即torch.distributed）使研究人员和从业人员可以轻松地并行化他们在跨进程和机器集群的计算。 为此，它利用了传递消息的语义，从而允许每个进程将数据传递给任何其他进程。 与多处理包相反，进程可以使用不同的通信后端，而不仅限于在同一台计算机上执行。 为了开始，我们需要同时运行多个进程的能力。 如果您有权访问计算群集，则应咨询本地系统管理员或使用您喜欢的协调工具。 （例如 pdsh，clustershell 或其他）。出于本教程的目的，我们将使用以下模板使用一台计算机并分叉多个进程。 \"\"\"run.py:\"\"\" #!/usr/bin/env python import os import torch import torch.distributed as dist from torch.multiprocessing import Process def run(rank, size): \"\"\" Distributed function to be implemented later. \"\"\" pass def init_process(rank, size, fn, backend='gloo'): \"\"\" Initialize the distributed environment. \"\"\" os.environ['MASTER_ADDR'] = '127.0.0.1' os.environ['MASTER_PORT'] = '29500' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) if __name__ == \"__main__\": size = 2 processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join() 上面的脚本产生了两个进程，每个进程将设置分布式环境，初始化进程组（dist.init_process_group），最后执行给定的run函数。 让我们看一下init_process函数。 它确保每个进程都可以使用相同的 IP 地址和端口通过主机进行协调。 请注意，我们使用了gloo后端，但其他后端也可用。 （请参阅第 5.1 节），我们将在本教程的结尾部分介绍dist.init_process_group中发生的魔术，但实际上，它允许进程通过共享位置相互进行通信。 点对点通信 发送和接收 数据从一个进程到另一个进程的传输称为点对点通信。 这些是通过send和recv函数或其直接对应部分isend和irecv实现的。 \"\"\"Blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print('Rank ', rank, ' has data ', tensor[0]) 在上面的示例中，两个进程都从零张量开始，然后进程 0 递增张量并将其发送到进程 1，以便它们都以 1.0 结尾。 请注意，进程 1 需要分配内存以存储它将接收的数据。 另请注意，send/recv被阻塞：两个过程都停止，直到通信完成。 另一方面，即时消息是非阻塞； 脚本继续执行，方法返回Work对象，我们可以选择wait()作为对象。 \"\"\"Non-blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) req = None if rank == 0: tensor += 1 # Send the tensor to process 1 req = dist.isend(tensor=tensor, dst=1) print('Rank 0 started sending') else: # Receive tensor from process 0 req = dist.irecv(tensor=tensor, src=0) print('Rank 1 started receiving') req.wait() print('Rank ', rank, ' has data ', tensor[0]) 使用立即数时，我们必须谨慎使用已发送和已接收张量。 由于我们不知道何时将数据传递给其他进程，因此在req.wait()完成之前，我们既不应该修改发送的张量也不应该访问接收的张量。 换一种说法， 在dist.isend()之后写入tensor将导致不确定的行为。 在dist.irecv()之后从tensor读取将导致不确定的行为。 但是，在执行了req.wait()之后，我们可以保证已进行通信，并且tensor[0]中存储的值为 1.0。 当我们希望对流程的通信进行精细控制时，点对点通信非常有用。 它们可用于实现精美的算法，例如百度的 DeepSpeech 或 Facebook 的大规模实验中使用的算法。（请参阅 4.1 节） 集合通信 分散 收集 归约 全部归约 广播 全部收集 与点对点通信相反，集合允许跨组中所有进程的通信模式。 小组是我们所有过程的子集。 要创建组，我们可以将等级列表传递给dist.new_group(group)。 默认情况下，集合在所有进程（也称为世界）上执行。 例如，为了获得所有过程中所有张量的总和，我们可以使用dist.all_reduce(tensor, op, group)集合。 \"\"\" All-Reduce example.\"\"\" def run(rank, size): \"\"\" Simple point-to-point communication. \"\"\" group = dist.new_group([0, 1]) tensor = torch.ones(1) dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) 由于我们需要组中所有张量的总和，因此我们将dist.reduce_op.SUM用作化简运算符。 一般而言，任何可交换的数学运算都可以用作运算符。 PyTorch 开箱即用，带有 4 个这样的运算符，它们都在元素级运行： dist.reduce_op.SUM， dist.reduce_op.PRODUCT， dist.reduce_op.MAX， dist.reduce_op.MIN。 除了dist.all_reduce(tensor, op, group)之外，PyTorch 中目前共有 6 个集合体。 dist.broadcast(tensor, src, group)：将tensor从src复制到所有其他进程。 dist.reduce(tensor, dst, op, group)：将op应用于所有tensor，并将结果存储在dst中。 dist.all_reduce(tensor, op, group)：与reduce相同，但是结果存储在所有进程中。 dist.scatter(tensor, src, scatter_list, group)：将第i个张量scatter_list[i]复制到第i个过程。 dist.gather(tensor, dst, gather_list, group)：从dst中的所有进程复制tensor。 dist.all_gather(tensor_list, tensor, group)：将所有进程中的tensor从所有进程复制到tensor_list。 dist.barrier(group)：阻止组中的所有进程，直到每个进程都进入此函数。 分布式训练 注意：您可以在此 GitHub 存储库中找到本节的示例脚本。 现在我们了解了分布式模块的工作原理，让我们用它编写一些有用的东西。 我们的目标是复制DistributedDataParallel的功能。 当然，这将是一个教学示例，在现实世界中，您应该使用上面链接的经过官方测试，优化的最佳版本。 很简单，我们想要实现随机梯度下降的分布式版本。 我们的脚本将允许所有进程在其数据批量上计算其模型的梯度，然后平均其梯度。 为了在更改进程数时确保相似的收敛结果，我们首先必须对数据集进行分区。 （您也可以使用tnt.dataset.SplitDataset代替下面的代码段。） \"\"\" Dataset partitioning helper \"\"\" class Partition(object): def __init__(self, data, index): self.data = data self.index = index def __len__(self): return len(self.index) def __getitem__(self, index): data_idx = self.index[index] return self.data[data_idx] class DataPartitioner(object): def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234): self.data = data self.partitions = [] rng = Random() rng.seed(seed) data_len = len(data) indexes = [x for x in range(0, data_len)] rng.shuffle(indexes) for frac in sizes: part_len = int(frac * data_len) self.partitions.append(indexes[0:part_len]) indexes = indexes[part_len:] def use(self, partition): return Partition(self.data, self.partitions[partition]) 使用上面的代码片段，我们现在可以使用以下几行简单地对任何数据集进行分区： \"\"\" Partitioning MNIST \"\"\" def partition_dataset(): dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])) size = dist.get_world_size() bsz = 128 / float(size) partition_sizes = [1.0 / size for _ in range(size)] partition = DataPartitioner(dataset, partition_sizes) partition = partition.use(dist.get_rank()) train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True) return train_set, bsz 假设我们有 2 个副本，则每个进程的train_set为60000/2 = 30000个样本。 我们还将批量大小除以副本数，以使整个批量大小保持为 128。 现在，我们可以编写我们通常的前向后优化训练代码，并添加一个函数调用来平均模型的梯度。 （以下内容主要是受 PyTorch MNIST 官方示例的启发）。 \"\"\" Distributed Synchronous SGD Example \"\"\" def run(rank, size): torch.manual_seed(1234) train_set, bsz = partition_dataset() model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) num_batches = ceil(len(train_set.dataset) / float(bsz)) for epoch in range(10): epoch_loss = 0.0 for data, target in train_set: optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) epoch_loss += loss.item() loss.backward() average_gradients(model) optimizer.step() print('Rank ', dist.get_rank(), ', epoch ', epoch, ': ', epoch_loss / num_batches) 仍然需要执行average_gradients(model)函数，该函数只需要一个模型并在整个世界上平均其梯度即可。 \"\"\" Gradient averaging. \"\"\" def average_gradients(model): size = float(dist.get_world_size()) for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM) param.grad.data /= size 等等！ 我们成功实现了分布式同步 SGD，并且可以在大型计算机集群上训练任何模型。 注意：虽然从技术上来说最后一句话是是正确的，但要实现同步 SGD 的生产级实现，还需要更多技巧。 同样，请使用经过测试和优化的东西。 我们自己的 Ring-Allreduce 另一个挑战是，假设我们想实现 DeepSpeech 的高效环网减少。 使用点对点集合很容易实现。 \"\"\" Implementation of a ring-reduce with addition. \"\"\" def allreduce(send, recv): rank = dist.get_rank() size = dist.get_world_size() send_buff = send.clone() recv_buff = send.clone() accum = send.clone() left = ((rank - 1) + size) % size right = (rank + 1) % size for i in range(size - 1): if i % 2 == 0: # Send send_buff send_req = dist.isend(send_buff, right) dist.recv(recv_buff, left) accum[:] += recv_buff[:] else: # Send recv_buff send_req = dist.isend(recv_buff, right) dist.recv(send_buff, left) accum[:] += send_buff[:] send_req.wait() recv[:] = accum[:] 在上面的脚本中，allreduce(send, recv)函数的签名与 PyTorch 中的签名略有不同。 它需要一个recv张量，并将所有send张量的总和存储在其中。 作为练习留给读者，我们的版本与 DeepSpeech 中的版本之间仍然有一个区别：它们的实现将梯度张量划分为块，以便最佳地利用通信带宽。 （提示：torch.chunk） 高级主题 现在，我们准备发现torch.distributed的一些更高级的功能。 由于涉及的内容很多，本节分为两个小节： 通讯后端：我们在这里学习如何使用 MPI 和 Gloo 进行 GPU-GPU 通讯。 初始化方法：我们了解如何在dist.init_process_group()中最佳设置初始协调阶段。 通信后端 torch.distributed最优雅的方面之一是它具有抽象能力，并且可以在不同的后端之上构建。 如前所述，目前在 PyTorch 中实现了三个后端：Glo，NCCL 和 MPI。 它们各自具有不同的规格和权衡，具体取决于所需的用例。 可以在此处找到支持功能的比较表。 Gloo 后端 到目前为止，我们已经广泛使用 Gloo 后端。 它作为开发平台非常方便，因为它已包含在预编译的 PyTorch 二进制文件中，并且可在 Linux（自 0.2 开始）和 macOS（自 1.3 开始）上运行。 它支持 CPU 上的所有点对点和集合操作，以及 GPU 上的所有集合操作。 CUDA 张量的集体运算的实现未像 NCCL 后端提供的那样优化。 如您所知，如果在 GPU 上放置model，我们的分布式 SGD 示例将无法正常工作。 为了使用多个 GPU，让我们还进行以下修改： 使用device = torch.device(\"cuda:{}\".format(rank)) model = Net() => model = Net().to(device) 使用data, target = data.to(device), target.to(device) 经过上述修改，我们的模型现在可以在两个 GPU 上训练，您可以使用watch nvidia-smi监视其使用情况。 MPI 后端 消息传递接口（MPI）是来自高性能计算领域的标准化工具。 它允许进行点对点和集体通信，并且是torch.distributed API 的主要灵感。 存在几种针对不同目的而优化的 MPI 实现（例如 Open-MPI，MVAPICH2，Intel MPI）。 使用 MPI 后端的优势在于 MPI 在大型计算机群集上的广泛可用性-和高水平的优化。 一些最近的实现也能够利用 CUDA IPC 和 GPU Direct 技术，以避免通过 CPU 进行内存复制。 不幸的是，PyTorch 的二进制文件不能包含 MPI 实现，我们将不得不手动对其进行重新编译。 幸运的是，鉴于编译后，PyTorch 会单独查看以查找可用的 MPI 实现，因此此过程相当简单。 以下步骤通过从源安装 PyTorch 来安装 MPI 后端。 创建并激活 Anaconda 环境，按照指南的要求安装所有先决条件，但是尚未运行。 选择并安装您喜欢的 MPI 实现。 请注意，启用支持 CUDA 的 MPI 可能需要一些其他步骤。 在我们的情况下，我们将坚持不支持 GPU 的 Open-MPI：conda install -c conda-forge openmpi 现在，转到克隆的 PyTorch 存储库并执行python setup.py install。 为了测试我们新安装的后端，需要进行一些修改。 将if __name__ == '__main__':下的内容替换为init_process(0, 0, run, backend='mpi')。 运行mpirun -n 4 python myscript.py。 这些更改的原因是，MPI 需要在生成进程之前创建自己的环境。 MPI 还将生成自己的进程，并执行初始化方法中描述的握手，使init_process_group的rank和size参数多余。 实际上，这非常强大，因为您可以将其他参数传递给mpirun，以便为每个进程定制计算资源。 （诸如每个进程的内核数，将计算机手动分配给特定级别之类的东西，以及其它。）这样做，您应该获得与其他通信后端相同的熟悉输出。 NCCL 后端 NCCL 后端提供了针对 CUDA 张量的集体运算的优化实现。 如果仅对集体操作使用 CUDA 张量，请考虑使用此后端以获得最佳性能。 NCCL 后端包含在具有 CUDA 支持的预构建二进制文件中。 初始化方法 为了完成本教程，我们来谈谈我们调用的第一个函数：dist.init_process_group(backend, init_method)。 特别是，我们将介绍负责每个过程之间初始协调步骤的不同初始化方法。 这些方法使您可以定义协调方式。 根据您的硬件设置，这些方法之一自然应该比其他方法更合适。 除了以下各节之外，您还应该查看官方文档。 环境变量 在本教程中，我们一直在使用环境变量初始化方法。 通过在所有机器上设置以下四个环境变量，所有进程将能够正确连接到主服务器，获取有关其他进程的信息，最后与它们握手。 MASTER_PORT：计算机上的空闲端口，它将托管等级为 0 的进程。 MASTER_ADDR：将以等级 0 托管进程的计算机的 IP 地址。 WORLD_SIZE：进程总数，以便主机知道要等待多少个工作器。 RANK：每个进程的等级，因此他们将知道它是否是工作器的主人。 共享文件系统 共享文件系统要求所有进程都有权访问共享文件系统，并将通过共享文件进行协调。 这意味着每个进程都将打开文件，写入文件信息，然后等到每个人都打开文件。 之后，所有必需的信息将可用于所有过程。 为了避免争用情况，文件系统必须通过fcntl支持锁定。 dist.init_process_group( init_method='file:///mnt/nfs/sharedfile', rank=args.rank, world_size=4) TCP 通过提供等级 0 和可访问的端口号的进程的 IP 地址，可以实现通过 TCP 进行初始化。 在这里，所有工作器都可以连接到等级为 0 的流程，并交换有关如何相互联系的信息。 dist.init_process_group( init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) **致谢** 我要感谢 PyTorch 开发人员在实现，文档和测试方面做得如此出色。 当代码不清楚时，我总是可以依靠文档或测试来找到答案。 我尤其要感谢 Soumith Chintala，Adam Paszke 和 Natalia Gimelshein 提供的有见地的评论并回答了有关初稿的问题。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"64.html":{"url":"64.html","title":"分布式 RPC 框架入门","keywords":"","body":"分布式 RPC 框架入门 原文：https://pytorch.org/tutorials/intermediate/rpc_tutorial.html 作者：Shen Li 先决条件： PyTorch 分布式概述 RPC API 文档 本教程使用两个简单的示例来演示如何使用torch.distributed.rpc包构建分布式训练，该包首先在 PyTorch v1.4 中作为原型功能引入。 这两个示例的源代码可以在 PyTorch 示例中找到。 先前的教程分布式数据并行入门和使用 PyTorch 编写分布式应用，描述了DistributedDataParallel，该模型支持特定的训练范例，该模型可在多个进程之间复制，每个进程都处理输入数据的拆分。 有时，您可能会遇到需要不同训练范例的场景。 例如： 在强化学习中，从环境中获取训练数据可能相对昂贵，而模型本身可能很小。 在这种情况下，产生多个并行运行的观察者并共享一个智能体可能会很有用。 在这种情况下，智能体将在本地负责训练，但是应用仍将需要库在观察者和训练者之间发送和接收数据。 您的模型可能太大，无法容纳在一台计算机上的 GPU 中，因此需要一个库来帮助将模型拆分到多台计算机上。 或者，您可能正在实现参数服务器训练框架，其中模型参数和训练器位于不同的机器上。 torch.distributed.rpc包可以帮助解决上述情况。 在情况 1 中， RPC 和 RRef 允许将数据从一个工作程序发送到另一个工作程序，同时轻松引用远程数据对象。 在情况 2 中，分布式 Autograd 和分布式优化器使执行反向传递和优化器步骤就像本地训练一样。 在接下来的两节中，我们将使用强化学习示例和语言模型示例来演示torch.distributed.rpc的 API。 请注意，本教程并非旨在构建最准确或最有效的模型来解决给定的问题，相反，此处的主要目标是演示如何使用torch.distributed.rpc包来构建分布式训练应用。 使用 RPC 和 RRef 的分布式强化学习 本节介绍了使用 RPC 建立玩具分布式强化学习模型以解决 OpenAI Gym 中的 CartPole-v1 的步骤。 策略代码主要是从现有的单线程示例中借用的，如下所示。 我们将跳过Policy设计的详细信息，并将重点介绍 RPC 的用法。 import torch.nn as nn import torch.nn.functional as F class Policy(nn.Module): def __init__(self): super(Policy, self).__init__() self.affine1 = nn.Linear(4, 128) self.dropout = nn.Dropout(p=0.6) self.affine2 = nn.Linear(128, 2) self.saved_log_probs = [] self.rewards = [] def forward(self, x): x = self.affine1(x) x = self.dropout(x) x = F.relu(x) action_scores = self.affine2(x) return F.softmax(action_scores, dim=1) 首先，让我们准备一个助手，以在RRef的所有者工作程序上远程运行函数。 您将在本教程的示例中的多个地方发现该函数。 理想情况下，torch.distributed.rpc包应立即提供这些助手函数。 例如，如果应用可以直接调用RRef.some_func(*arg)，然后将其转换为RRef所有者的 RPC，将会更容易。 在pytorch/pytorch#31743中跟踪了此 API 的进度。 from torch.distributed.rpc import rpc_sync def _call_method(method, rref, *args, **kwargs): return method(rref.local_value(), *args, **kwargs) def _remote_method(method, rref, *args, **kwargs): args = [method, rref] + list(args) return rpc_sync(rref.owner(), _call_method, args=args, kwargs=kwargs) # to call a function on an rref, we could do the following # _remote_method(some_func, rref, *args) 我们准备介绍观察员。 在此示例中，每个观察者创建自己的环境，并等待智能体的命令来运行剧集。 在每个剧集中，一个观察者最多循环n_steps个迭代，并且在每个迭代中，它使用 RPC 将其环境状态传递给智能体并取回操作。 然后，它将该操作应用于其环境，并从环境中获取奖励和下一个状态。 之后，观察者使用另一个 RPC 向智能体报告奖励。 同样，请注意，这显然不是最有效的观察者实现。 例如，一个简单的优化可能是将当前状态和最后的报酬打包到一个 RPC 中，以减少通信开销。 但是，目标是演示 RPC API，而不是为 CartPole 构建最佳的求解器。 因此，在此示例中，让逻辑保持简单，并明确两个步骤。 import argparse import gym import torch.distributed.rpc as rpc parser = argparse.ArgumentParser( description=\"RPC Reinforcement Learning Example\", formatter_class=argparse.ArgumentDefaultsHelpFormatter, ) parser.add_argument('--world_size', default=2, help='Number of workers') parser.add_argument('--log_interval', default=1, help='Log every log_interval episodes') parser.add_argument('--gamma', default=0.1, help='how much to value future rewards') parser.add_argument('--seed', default=1, help='random seed for reproducibility') args = parser.parse_args() class Observer: def __init__(self): self.id = rpc.get_worker_info().id self.env = gym.make('CartPole-v1') self.env.seed(args.seed) def run_episode(self, agent_rref, n_steps): state, ep_reward = self.env.reset(), 0 for step in range(n_steps): # send the state to the agent to get an action action = _remote_method(Agent.select_action, agent_rref, self.id, state) # apply the action to the environment, and get the reward state, reward, done, _ = self.env.step(action) # report the reward to the agent for training purpose _remote_method(Agent.report_reward, agent_rref, self.id, reward) if done: break agent 的代码稍微复杂一点，我们将其分成多个部分。 在此示例中，智能体既充当训练者又充当主角色，以便它向多个分布式观察者发送命令以运行剧集，并且还记录本地的所有动作和奖励，这些动作和奖赏将在每个剧集之后的训练阶段使用。 下面的代码显示了Agent构造器，其中大多数行都在初始化各种组件。 最后的循环在其他工作器上远程初始化观察者，并在本地将RRefs保留给这些观察者。 智能体稍后将使用那些观察者RRefs发送命令。 应用无需担心RRefs的寿命。 每个RRef的所有者维护一个引用计数图以跟踪其生命周期，并保证只要该RRef的任何活动用户都不会删除远程数据对象。 有关详细信息，请参考RRef 设计文档。 import gym import numpy as np import torch import torch.distributed.rpc as rpc import torch.optim as optim from torch.distributed.rpc import RRef, rpc_async, remote from torch.distributions import Categorical class Agent: def __init__(self, world_size): self.ob_rrefs = [] self.agent_rref = RRef(self) self.rewards = {} self.saved_log_probs = {} self.policy = Policy() self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2) self.eps = np.finfo(np.float32).eps.item() self.running_reward = 0 self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold for ob_rank in range(1, world_size): ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank)) self.ob_rrefs.append(remote(ob_info, Observer)) self.rewards[ob_info.id] = [] self.saved_log_probs[ob_info.id] = [] 接下来，智能体向观察者公开两个 API，以供他们选择动作和报告奖励。 这些函数仅在智能体上本地运行，但是将由观察者通过 RPC 触发。 class Agent: ... def select_action(self, ob_id, state): state = torch.from_numpy(state).float().unsqueeze(0) probs = self.policy(state) m = Categorical(probs) action = m.sample() self.saved_log_probs[ob_id].append(m.log_prob(action)) return action.item() def report_reward(self, ob_id, reward): self.rewards[ob_id].append(reward) 让我们在智能体上添加run_episode函数，该函数告诉所有观察者执行片段。 在此函数中，它首先创建一个列表，以从异步 RPC 收集期货，然后在所有观察者RRefs上循环以生成异步 RPC。 在这些 RPC 中，智能体还将自身的RRef传递给观察者，以便观察者也可以在智能体上调用函数。 如上所示，每个观察者都将 RPC 返回给智能体，它们是嵌套的 RPC。 在每个剧集之后，saved_log_probs和rewards将包含记录的动作概率和奖励。 class Agent: ... def run_episode(self, n_steps=0): futs = [] for ob_rref in self.ob_rrefs: # make async RPC to kick off an episode on all observers futs.append( rpc_async( ob_rref.owner(), _call_method, args=(Observer.run_episode, ob_rref, self.agent_rref, n_steps) ) ) # wait until all obervers have finished this episode for fut in futs: fut.wait() 最后，在一集之后，智能体需要训练模型，该模型在下面的finish_episode函数中实现。 此函数中没有 RPC，并且大多数是从单线程示例中借用的。 因此，我们跳过描述其内容。 class Agent: ... def finish_episode(self): # joins probs and rewards from different observers into lists R, probs, rewards = 0, [], [] for ob_id in self.rewards: probs.extend(self.saved_log_probs[ob_id]) rewards.extend(self.rewards[ob_id]) # use the minimum observer reward to calculate the running reward min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards]) self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward # clear saved probs and rewards for ob_id in self.rewards: self.rewards[ob_id] = [] self.saved_log_probs[ob_id] = [] policy_loss, returns = [], [] for r in rewards[::-1]: R = r + args.gamma * R returns.insert(0, R) returns = torch.tensor(returns) returns = (returns - returns.mean()) / (returns.std() + self.eps) for log_prob, R in zip(probs, returns): policy_loss.append(-log_prob * R) self.optimizer.zero_grad() policy_loss = torch.cat(policy_loss).sum() policy_loss.backward() self.optimizer.step() return min_reward 使用Policy，Observer和Agent类，我们准备启动多个过程来执行分布式训练。 在此示例中，所有进程都运行相同的run_worker函数，并且它们使用等级来区分其角色。 等级 0 始终是智能体，其他所有等级都是观察者。 智能体通过重复调用run_episode和finish_episode作为主设备，直到运行的奖励超过环境指定的奖励阈值为止。 所有观察者都被动地等待来自智能体的命令。 该代码由rpc.init_rpc和rpc.shutdown包装，它们分别初始化和终止 RPC 实例。 API 页面中提供了更多详细信息。 import os from itertools import count import torch.multiprocessing as mp AGENT_NAME = \"agent\" OBSERVER_NAME=\"obs\" TOTAL_EPISODE_STEP = 100 def run_worker(rank, world_size): os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' if rank == 0: # rank0 is the agent rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size) agent = Agent(world_size) for i_episode in count(1): n_steps = int(TOTAL_EPISODE_STEP / (args.world_size - 1)) agent.run_episode(n_steps=n_steps) last_reward = agent.finish_episode() if i_episode % args.log_interval == 0: print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format( i_episode, last_reward, agent.running_reward)) if agent.running_reward > agent.reward_threshold: print(\"Solved! Running reward is now {}!\".format(agent.running_reward)) break else: # other ranks are the observer rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size) # observers passively waiting for instructions from the agent # block until all rpcs finish, and shutdown the RPC instance rpc.shutdown() mp.spawn( run_worker, args=(args.world_size, ), nprocs=args.world_size, join=True ) 以下是使用world_size = 2进行训练时的一些示例输出。 Episode 10 Last reward: 26.00 Average reward: 10.01 Episode 20 Last reward: 16.00 Average reward: 11.27 Episode 30 Last reward: 49.00 Average reward: 18.62 Episode 40 Last reward: 45.00 Average reward: 26.09 Episode 50 Last reward: 44.00 Average reward: 30.03 Episode 60 Last reward: 111.00 Average reward: 42.23 Episode 70 Last reward: 131.00 Average reward: 70.11 Episode 80 Last reward: 87.00 Average reward: 76.51 Episode 90 Last reward: 86.00 Average reward: 95.93 Episode 100 Last reward: 13.00 Average reward: 123.93 Episode 110 Last reward: 33.00 Average reward: 91.39 Episode 120 Last reward: 73.00 Average reward: 76.38 Episode 130 Last reward: 137.00 Average reward: 88.08 Episode 140 Last reward: 89.00 Average reward: 104.96 Episode 150 Last reward: 97.00 Average reward: 98.74 Episode 160 Last reward: 150.00 Average reward: 100.87 Episode 170 Last reward: 126.00 Average reward: 104.38 Episode 180 Last reward: 500.00 Average reward: 213.74 Episode 190 Last reward: 322.00 Average reward: 300.22 Episode 200 Last reward: 165.00 Average reward: 272.71 Episode 210 Last reward: 168.00 Average reward: 233.11 Episode 220 Last reward: 184.00 Average reward: 195.02 Episode 230 Last reward: 284.00 Average reward: 208.32 Episode 240 Last reward: 395.00 Average reward: 247.37 Episode 250 Last reward: 500.00 Average reward: 335.42 Episode 260 Last reward: 500.00 Average reward: 386.30 Episode 270 Last reward: 500.00 Average reward: 405.29 Episode 280 Last reward: 500.00 Average reward: 443.29 Episode 290 Last reward: 500.00 Average reward: 464.65 Solved! Running reward is now 475.3163778435275! 在此示例中，我们展示了如何使用 RPC 作为通信工具来跨工作器传递数据，以及如何使用 RRef 引用远程对象。 的确，您可以直接在ProcessGroup send和recv API 之上构建整个结构，也可以使用其他通信/ RPC 库。 但是，通过使用torch.distributed.rpc，您可以在后台获得本机支持并不断优化性能。 接下来，我们将展示如何将 RPC 和 RRef 与分布式 Autograd 和分布式优化器结合起来执行分布式模型并行训练。 使用分布式 Autograd 和分布式优化器的分布式 RNN 在本节中，我们将使用 RNN 模型来展示如何使用 RPC API 构建分布式模型并行训练。 示例 RNN 模型非常小，可以轻松地放入单个 GPU 中，但是我们仍将其层划分为两个不同的工作器来演示这一想法。 开发人员可以应用类似的技术在多个设备和机器上分布更大的模型。 RNN 模型设计是从 PyTorch 示例存储库中的词语言模型中借用的，该存储库包含三个主要组件，一个嵌入表，一个LSTM层和一个解码器。 下面的代码将嵌入表和解码器包装到子模块中，以便它们的构造器可以传递给 RPC API。 在EmbeddingTable子模块中，我们有意将Embedding层放在 GPU 上以涵盖用例。 在 v1.4 中，RPC 始终在目标工作线程上创建 CPU 张量参数或返回值。 如果函数使用 GPU 张量，则需要将其显式移动到适当的设备。 class EmbeddingTable(nn.Module): r\"\"\" Encoding layers of the RNNModel \"\"\" def __init__(self, ntoken, ninp, dropout): super(EmbeddingTable, self).__init__() self.drop = nn.Dropout(dropout) self.encoder = nn.Embedding(ntoken, ninp).cuda() self.encoder.weight.data.uniform_(-0.1, 0.1) def forward(self, input): return self.drop(self.encoder(input.cuda()).cpu() class Decoder(nn.Module): def __init__(self, ntoken, nhid, dropout): super(Decoder, self).__init__() self.drop = nn.Dropout(dropout) self.decoder = nn.Linear(nhid, ntoken) self.decoder.bias.data.zero_() self.decoder.weight.data.uniform_(-0.1, 0.1) def forward(self, output): return self.decoder(self.drop(output)) 使用上述子模块，我们现在可以使用 RPC 将它们组合在一起以创建 RNN 模型。 在下面的代码中，ps代表参数服务器，该服务器托管嵌入表和解码器的参数。 构造器使用远程 API 在参数服务器上创建EmbeddingTable对象和Decoder对象，并在本地创建LSTM子模块。 在前进过程中，训练器使用EmbeddingTable RRef查找远程子模块，然后使用 RPC 将输入数据传递到EmbeddingTable，并获取查找结果。 然后，它通过本地LSTM层运行嵌入，最后使用另一个 RPC 将输出发送到Decoder子模块。 通常，要实现分布式模型并行训练，开发人员可以将模型分为多个子模块，调用 RPC 远程创建子模块实例，并在必要时使用RRef查找它们。 正如您在下面的代码中看到的那样，它看起来与单机模型并行训练非常相似。 主要区别是用 RPC 函数替换了Tensor.to(device)。 class RNNModel(nn.Module): def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5): super(RNNModel, self).__init__() # setup embedding table remotely self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout)) # setup LSTM locally self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout) # setup decoder remotely self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout)) def forward(self, input, hidden): # pass input to the remote embedding table and fetch emb tensor back emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input) output, hidden = self.rnn(emb, hidden) # pass output to the rremote decoder and get the decoded output back decoded = _remote_method(Decoder.forward, self.decoder_rref, output) return decoded, hidden 在介绍分布式优化器之前，让我们添加一个辅助函数来生成模型参数的 RRef 列表，该列表将由分布式优化器使用。 在本地训练中，应用可以调用Module.parameters()来获取对所有参数张量的引用，并将其传递给本地优化器以进行后续更新。 但是，由于某些参数存在于远程计算机上，因此同一 API 在分布式训练方案中不起作用。 因此，分布式优化器不采用参数Tensors的列表，而是采用RRefs的列表，每个模型参数一个RRef用于本地和远程模型参数。 辅助函数非常简单，只需调用Module.parameters()并在每个参数上创建一个本地RRef。 def _parameter_rrefs(module): param_rrefs = [] for param in module.parameters(): param_rrefs.append(RRef(param)) return param_rrefs 然后，由于RNNModel包含三个子模块，因此我们需要调用_parameter_rrefs 3 次，并将其包装到另一个辅助函数中。 class RNNModel(nn.Module): ... def parameter_rrefs(self): remote_params = [] # get RRefs of embedding table remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref)) # create RRefs for local parameters remote_params.extend(_parameter_rrefs(self.rnn)) # get RRefs of decoder remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref)) return remote_params 现在，我们准备实现训练循环。 初始化模型参数后，我们创建RNNModel和DistributedOptimizer。 分布式优化器将采用参数RRefs的列表，查找所有不同的所有者工作器，并在每个所有者工作器上创建给定的本地优化器（即，在这种情况下，您也可以使用其他本地优化器SGD） 使用给定的参数（即lr=0.05）。 在训练循环中，它首先创建一个分布式 Autograd 上下文，这将帮助分布式 Autograd 引擎查找梯度和涉及的 RPC 发送/接收函数。 分布式 Autograd 引擎的设计详细信息可以在其设计说明中找到。 然后，它像本地模型一样开始正向传播，并运行分布式后向传递。 对于后向分布，您只需要指定一个根列表，在这种情况下，就是损失Tensor。 分布式 Autograd 引擎将自动遍历分布式图并正确编写梯度。 接下来，它在分布式优化器上运行step函数，该函数将与所有涉及的本地优化器联系以更新模型参数。 与本地训练相比，一个较小的差异是您不需要运行zero_grad()，因为每个 Autograd 上下文都有专用的空间来存储梯度，并且在每次迭代创建上下文时，来自不同迭代的那些梯度不会累积到同一组Tensors。 def run_trainer(): batch = 5 ntoken = 10 ninp = 2 nhid = 3 nindices = 3 nlayers = 4 hidden = ( torch.randn(nlayers, nindices, nhid), torch.randn(nlayers, nindices, nhid) ) model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers) # setup distributed optimizer opt = DistributedOptimizer( optim.SGD, model.parameter_rrefs(), lr=0.05, ) criterion = torch.nn.CrossEntropyLoss() def get_next_batch(): for _ in range(5): data = torch.LongTensor(batch, nindices) % ntoken target = torch.LongTensor(batch, ntoken) % nindices yield data, target # train for 10 iterations for epoch in range(10): for data, target in get_next_batch(): # create distributed autograd context with dist_autograd.context() as context_id: hidden[0].detach_() hidden[1].detach_() output, hidden = model(data, hidden) loss = criterion(output, target) # run distributed backward pass dist_autograd.backward(context_id, [loss]) # run distributed optimizer opt.step(context_id) # not necessary to zero grads since they are # accumulated into the distributed autograd context # which is reset every iteration. print(\"Training epoch {}\".format(epoch)) 最后，让我们添加一些粘合代码以启动参数服务器和训练器流程。 def run_worker(rank, world_size): os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' if rank == 1: rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size) _run_trainer() else: rpc.init_rpc(\"ps\", rank=rank, world_size=world_size) # parameter server do nothing pass # block until all rpcs finish rpc.shutdown() if __name__==\"__main__\": world_size = 2 mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"65.html":{"url":"65.html","title":"使用分布式 RPC 框架实现参数服务器","keywords":"","body":"使用分布式 RPC 框架实现参数服务器 原文：https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html 作者： Rohan Varma 先决条件： PyTorch 分布式概述 RPC API 文档 本教程介绍了一个简单的示例，该示例使用 PyTorch 的分布式 RPC 框架实现参数服务器。 参数服务器框架是一种范例，其中一组服务器存储参数（例如大型嵌入表），并且多个训练人员查询参数服务器以检索最新参数。 这些训练器可以在本地运行训练循环，并偶尔与参数服务器同步以获得最新参数。 有关参数服务器方法的更多信息，请查阅本文。 使用分布式 RPC 框架，我们将构建一个示例，其中多个训练器使用 RPC 与同一个参数服务器进行通信，并使用 RRef 访问远程参数服务器实例上的状态。 每位训练器将通过使用分布式 Autograd 跨多个节点拼接 Autograd 图，以分布式方式启动其专用的反向传递。 注意：本教程介绍了分布式 RPC 框架的用法，该方法可用于将模型拆分到多台计算机上，或用于实现参数服务器训练策略，在该策略中，网络训练器可以获取托管在另一台计算机上的参数。 相反，如果您要跨多个 GPU 复制模型，请参阅分布式数据并行教程。 还有另一个 RPC 教程，涵盖了强化学习和 RNN 用例。 让我们从熟悉的地方开始：导入我们所需的模块并定义一个简单的 ConvNet，它将在 MNIST 数据集上进行训练。 以下网络是从pytorch/examples仓库中定义的网络中广泛采用的。 import argparse import os import time from threading import Lock import torch import torch.distributed.autograd as dist_autograd import torch.distributed.rpc as rpc import torch.multiprocessing as mp import torch.nn as nn import torch.nn.functional as F from torch import optim from torch.distributed.optim import DistributedOptimizer from torchvision import datasets, transforms # --------- MNIST Network to train, from pytorch/examples ----- class Net(nn.Module): def __init__(self, num_gpus=0): super(Net, self).__init__() print(f\"Using {num_gpus} GPUs to train\") self.num_gpus = num_gpus device = torch.device( \"cuda:0\" if torch.cuda.is_available() and self.num_gpus > 0 else \"cpu\") print(f\"Putting first 2 convs on {str(device)}\") # Put conv layers on the first cuda device, or CPU if no cuda device self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device) self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device) # Put rest of the network on the 2nd cuda device, if there is one if \"cuda\" in str(device) and num_gpus > 1: device = torch.device(\"cuda:1\") print(f\"Putting rest of layers on {str(device)}\") self.dropout1 = nn.Dropout2d(0.25).to(device) self.dropout2 = nn.Dropout2d(0.5).to(device) self.fc1 = nn.Linear(9216, 128).to(device) self.fc2 = nn.Linear(128, 10).to(device) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.conv2(x) x = F.max_pool2d(x, 2) x = self.dropout1(x) x = torch.flatten(x, 1) # Move tensor to next device if necessary next_device = next(self.fc1.parameters()).device x = x.to(next_device) x = self.fc1(x) x = F.relu(x) x = self.dropout2(x) x = self.fc2(x) output = F.log_softmax(x, dim=1) return output 接下来，让我们定义一些辅助函数，这些函数将对其余脚本有用。 下面使用rpc_sync和 RRef 来定义一个函数，该函数在远程节点上的对象上调用给定方法。 下面，通过rref参数指定了对远程对象的句柄，并在其拥有的节点rref.owner()上运行它。 在调用者节点上，我们通过使用rpc_sync同步运行此命令，这意味着我们将阻塞直到收到响应。 # --------- Helper Methods -------------------- # On the local node, call a method with first arg as the value held by the # RRef. Other args are passed in as arguments to the function called. # Useful for calling instance methods. method could be any matching function, including # class methods. def call_method(method, rref, *args, **kwargs): return method(rref.local_value(), *args, **kwargs) # Given an RRef, return the result of calling the passed in method on the value # held by the RRef. This call is done on the remote node that owns # the RRef and passes along the given argument. # Example: If the value held by the RRef is of type Foo, then # remote_method(Foo.bar, rref, arg1, arg2) is equivalent to calling # .bar(arg1, arg2) on the remote node and getting the result # back. def remote_method(method, rref, *args, **kwargs): args = [method, rref] + list(args) return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs) 现在，我们准备定义参数服务器。 我们将子类化nn.Module，并将句柄保存到上面定义的网络中。 我们还将保存一个输入设备，该输入设备将是在调用模型之前将输入传输到的设备。 # --------- Parameter Server -------------------- class ParameterServer(nn.Module): def __init__(self, num_gpus=0): super().__init__() model = Net(num_gpus=num_gpus) self.model = model self.input_device = torch.device( \"cuda:0\" if torch.cuda.is_available() and num_gpus > 0 else \"cpu\") 接下来，我们将定义前进通道。 请注意，无论模型输出的设备如何，我们都会将输出移至 CPU，因为分布式 RPC 框架当前仅支持通过 RPC 发送 CPU 张量。 由于有可能在调用者/被调用者上使用不同的设备（CPU/GPU），因此我们有意禁用通过 RPC 发送 CUDA 张量，但在将来的版本中可能会支持此功能。 class ParameterServer(nn.Module): ... def forward(self, inp): inp = inp.to(self.input_device) out = self.model(inp) # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors. # Tensors must be moved in and out of GPU memory due to this. out = out.to(\"cpu\") return out 接下来，我们将定义一些其他函数，可用于训练和验证。 第一个get_dist_gradients将采用分布式 Autograd 上下文 ID，并调用dist_autograd.get_gradients API，以检索由分布式 Autograd 计算的梯度。 可以在分布式 Autograd 文档中找到更多信息。 请注意，由于该框架当前仅支持通过 RPC 发送张量，因此我们还会迭代生成的字典并将每个张量转换为 CPU 张量。 接下来，get_param_rrefs将迭代我们的模型参数，并将它们包装为（本地）RRef。 训练者节点将通过 RPC 调用此方法，并将返回要优化的参数列表。 这是分布式优化器的输入，它需要所有必须优化的参数作为RRef的列表。 # Use dist autograd to retrieve gradients accumulated for this model. # Primarily used for verification. def get_dist_gradients(self, cid): grads = dist_autograd.get_gradients(cid) # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors. # Tensors must be moved in and out of GPU memory due to this. cpu_grads = {} for k, v in grads.items(): k_cpu, v_cpu = k.to(\"cpu\"), v.to(\"cpu\") cpu_grads[k_cpu] = v_cpu return cpu_grads # Wrap local parameters in a RRef. Needed for building the # DistributedOptimizer which optimizes paramters remotely. def get_param_rrefs(self): param_rrefs = [rpc.RRef(param) for param in self.model.parameters()] return param_rrefs 最后，我们将创建用于初始化参数服务器的方法。 请注意，所有过程中只有一个参数服务器实例，并且所有训练器都将与同一参数服务器对话并更新相同的存储模型。 如run_parameter_server所示，服务器本身不采取任何独立的操作； 它等待来自训练者的请求（尚未定义），并通过运行所请求的函数对其作出响应。 # The global parameter server instance. param_server = None # A lock to ensure we only have one parameter server. global_lock = Lock() def get_parameter_server(num_gpus=0): \"\"\" Returns a singleton parameter server to all trainer processes \"\"\" global param_server # Ensure that we get only one handle to the ParameterServer. with global_lock: if not param_server: # construct it once param_server = ParameterServer(num_gpus=num_gpus) return param_server def run_parameter_server(rank, world_size): # The parameter server just acts as a host for the model and responds to # requests from trainers. # rpc.shutdown() will wait for all workers to complete by default, which # in this case means that the parameter server will wait for all trainers # to complete, and then exit. print(\"PS master initializing RPC\") rpc.init_rpc(name=\"parameter_server\", rank=rank, world_size=world_size) print(\"RPC initialized! Running parameter server...\") rpc.shutdown() print(\"RPC shutdown on parameter server.\") 请注意，以上rpc.shutdown()不会立即关闭参数服务器。 相反，它将等待所有工作器（在这种情况下为训练人员）也呼唤rpc.shutdown()。 这样可以保证参数服务器在所有训练人员（尚未定义）完成训练过程之前不会脱机。 接下来，我们将定义TrainerNet类。 这也将是nn.Module的子类，并且我们的__init__方法将使用rpc.remote API 获取对我们的参数服务器的 RRef 或远程引用。 请注意，此处我们没有将参数服务器复制到本地进程，而是可以将self.param_server_rref视为指向驻留在单独进程中的参数服务器的分布式共享指针。 # --------- Trainers -------------------- # nn.Module corresponding to the network trained by this trainer. The # forward() method simply invokes the network on the given parameter # server. class TrainerNet(nn.Module): def __init__(self, num_gpus=0): super().__init__() self.num_gpus = num_gpus self.param_server_rref = rpc.remote( \"parameter_server\", get_parameter_server, args=(num_gpus,)) 接下来，我们将定义一个名为get_global_param_rrefs的方法。 为了激发对这种方法的需求，值得阅读DistributedOptimizer上的文档，尤其是 API 签名。 必须向优化器传递与要优化的远程参数相对应的RRef列表，因此在这里我们获得了必要的RRef。 由于给定TrainerNet与之交互的唯一远程工作器是ParameterServer，因此我们只需在ParameterServer上调用remote_method。 我们使用在ParameterServer类中定义的get_param_rrefs方法。 此方法将RRef的列表返回到需要优化的参数。 请注意，在这种情况下，我们的TrainerNet没有定义自己的参数； 如果确实如此，我们还需要将每个参数都包装在RRef中，并将其包含在DistributedOptimizer的输入中。 class TrainerNet(nn.Module): ... def get_global_param_rrefs(self): remote_params = remote_method( ParameterServer.get_param_rrefs, self.param_server_rref) return remote_params 现在，我们准备定义forward方法，该方法将调用（同步）RPC 以运行ParameterServer上定义的网络的正向传播。 请注意，我们将self.param_server_rref（它是ParameterServer的远程句柄）传递给 RPC 调用。 该调用将向运行ParameterServer的节点发送 RPC，调用forward传递，然后返回与模型输出相对应的Tensor。 class TrainerNet(nn.Module): ... def forward(self, x): model_output = remote_method( ParameterServer.forward, self.param_server_rref, x) return model_output 完全定义好训练器之后，现在该编写我们的神经网络训练循环，该循环将创建我们的网络和优化器，通过网络运行一些输入并计算损失。 训练循环看起来很像本地训练计划，但由于我们的网络在机器之间分布，因此进行了一些修改。 下面，我们初始化TrainerNet并构建一个DistributedOptimizer。 请注意，如上所述，我们必须传入要优化的所有全局参数（跨参与分布式训练的所有节点）。 另外，我们传入要使用的本地优化器，在这种情况下为 SGD。 请注意，我们可以像创建本地优化器一样配置基础优化器算法-optimizer.SGD的所有参数都将正确转发。 例如，我们传入一个自定义学习率，它将用作所有本地优化器的学习率。 def run_training_loop(rank, num_gpus, train_loader, test_loader): # Runs the typical nueral network forward + backward + optimizer step, but # in a distributed fashion. net = TrainerNet(num_gpus=num_gpus) # Build DistributedOptimizer. param_rrefs = net.get_global_param_rrefs() opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03) 接下来，我们定义我们的主要训练循环。 我们遍历了 PyTorch 的DataLoader提供的可迭代项。 在编写典型的前向/后向/优化器循环之前，我们首先将逻辑包装在分布式 Autograd 上下文中。 请注意，这需要记录在模型的正向传播中调用的 RPC，以便可以构造一个适当的图，其中包括在后向传递中所有参与的分布式工作器。 分布式 Autograd 上下文返回context_id，它用作用于累积和优化与特定迭代对应的梯度的标识符。 与调用典型的loss.backward()会启动此本地工作程序的反向传播相反，我们调用dist_autograd.backward()并传递我们的context_id和loss，这是我们希望反向传播从它开始的根。 另外，我们将此context_id传递到优化程序调用中，该调用程序必须能够在所有节点上查找由该特定反向传播计算出的相应梯度。 def run_training_loop(rank, num_gpus, train_loader, test_loader): ... for i, (data, target) in enumerate(train_loader): with dist_autograd.context() as cid: model_output = net(data) target = target.to(model_output.device) loss = F.nll_loss(model_output, target) if i % 5 == 0: print(f\"Rank {rank} training batch {i} loss {loss.item()}\") dist_autograd.backward(cid, [loss]) # Ensure that dist autograd ran successfully and gradients were # returned. assert remote_method( ParameterServer.get_dist_gradients, net.param_server_rref, cid) != {} opt.step(cid) print(\"Training complete!\") print(\"Getting accuracy....\") get_accuracy(test_loader, net) 与传统的本地模型非常相似，下面的内容只是简单地计算了我们训练后模型的准确率。 但是，请注意，我们在上面传递给此函数的net是TrainerNet的实例，因此，正向传播以透明方式调用 RPC。 def get_accuracy(test_loader, model): model.eval() correct_sum = 0 # Use GPU to evaluate if possible device = torch.device(\"cuda:0\" if model.num_gpus > 0 and torch.cuda.is_available() else \"cpu\") with torch.no_grad(): for i, (data, target) in enumerate(test_loader): out = model(data, -1) pred = out.argmax(dim=1, keepdim=True) pred, target = pred.to(device), target.to(device) correct = pred.eq(target.view_as(pred)).sum().item() correct_sum += correct print(f\"Accuracy {correct_sum / len(test_loader.dataset)}\") 接下来，类似于我们将run_parameter_server定义为负责初始化 RPC 的ParameterServer的主循环的方式，让我们为训练者定义一个类似的循环。 所不同的是，我们的训练器必须执行上面定义的训练循环： # Main loop for trainers. def run_worker(rank, world_size, num_gpus, train_loader, test_loader): print(f\"Worker rank {rank} initializing RPC\") rpc.init_rpc( name=f\"trainer_{rank}\", rank=rank, world_size=world_size) print(f\"Worker {rank} done initializing RPC\") run_training_loop(rank, num_gpus, train_loader, test_loader) rpc.shutdown() 请注意，类似于run_parameter_server，rpc.shutdown()默认情况下将等待该节点退出之前，所有训练器和ParameterServer的所有工作器都调用rpc.shutdown()。 这样可确保节点正常终止，并且没有一个节点脱机，而另一个节点则期望其联机。 现在，我们已经完成了特定于训练器和参数服务器的代码，剩下的就是添加代码以启动训练器和参数服务器。 首先，我们必须接受适用于我们的参数服务器和训练器的各种参数。 world_size对应于将参加训练的节点总数，并且是所有训练器和参数服务器的总和。 我们还必须为每个单独的进程传递唯一的rank，从 0（将在其中运行单个参数服务器的地方）到world_size - 1。 master_addr和master_port是可用于标识等级 0 进程在何处运行的参数，并且各个节点将使用它们来相互发现。 要在本地测试此示例，只需将localhost和相同的master_port传递给所有产生的实例。 请注意，出于演示目的，此示例仅支持 0-2 个 GPU，尽管可以扩展该模式以使用其他 GPU。 if __name__ == '__main__': parser = argparse.ArgumentParser( description=\"Parameter-Server RPC based training\") parser.add_argument( \"--world_size\", type=int, default=4, help=\"\"\"Total number of participating processes. Should be the sum of master node and all training nodes.\"\"\") parser.add_argument( \"rank\", type=int, default=None, help=\"Global rank of this process. Pass in 0 for master.\") parser.add_argument( \"num_gpus\", type=int, default=0, help=\"\"\"Number of GPUs to use for training, Currently supports between 0 and 2 GPUs. Note that this argument will be passed to the parameter servers.\"\"\") parser.add_argument( \"--master_addr\", type=str, default=\"localhost\", help=\"\"\"Address of master, will default to localhost if not provided. Master must be able to accept network traffic on the address + port.\"\"\") parser.add_argument( \"--master_port\", type=str, default=\"29500\", help=\"\"\"Port that master is listening on, will default to 29500 if not provided. Master must be able to accept network traffic on the host and port.\"\"\") args = parser.parse_args() assert args.rank is not None, \"must provide rank argument.\" assert args.num_gpus 现在，我们将根据命令行参数创建一个与参数服务器或训练器相对应的过程。 如果传入的等级为 0，我们将创建一个ParameterServer，否则，将创建一个TrainerNet。 请注意，我们正在使用torch.multiprocessing启动与我们要执行的函数相对应的子进程，并使用p.join()从主线程等待该进程完成。 在初始化训练器的情况下，我们还使用 PyTorch 的数据加载器来指定 MNIST 数据集上的训练和测试数据加载器。 processes = [] world_size = args.world_size if args.rank == 0: p = mp.Process(target=run_parameter_server, args=(0, world_size)) p.start() processes.append(p) else: # Get data to train on train_loader = torch.utils.data.DataLoader( datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=32, shuffle=True,) test_loader = torch.utils.data.DataLoader( datasets.MNIST( '../data', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=32, shuffle=True, ) # start training worker on this node p = mp.Process( target=run_worker, args=( args.rank, world_size, args.num_gpus, train_loader, test_loader)) p.start() processes.append(p) for p in processes: p.join() 要在本地运行示例，请在单独的终端窗口中为服务器和要生成的每个工作程序运行以下命令工作程序：python rpc_parameter_server.py --world_size=WORLD_SIZE --rank=RANK。 例如，对于世界大小为 2 的主节点，命令为python rpc_parameter_server.py --world_size=2 --rank=0。 然后可以在单独的窗口中使用命令python rpc_parameter_server.py --world_size=2 --rank=1启动训练器，这将开始使用一台服务器和一台训练器进行训练。 请注意，本教程假定使用 0 到 2 个 GPU 进行训练，并且可以通过将--num_gpus=N传递到训练脚本中来配置此参数。 您可以传入命令行参数--master_addr=ADDRESS和--master_port=PORT来指示主工作器正在监听的地址和端口，例如，以测试在其他机器上运行训练者和主节点的功能。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"66.html":{"url":"66.html","title":"使用 RPC 的分布式管道并行化","keywords":"","body":"使用 RPC 的分布式管道并行化 原文：https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html 作者：Shen Li 先决条件： PyTorch 分布式概述 单机模型并行最佳实践 分布式 RPC 框架入门 RRef 辅助函数： RRef.rpc_sync()， RRef.rpc_async()和 RRef.remote() 本教程使用 Resnet50 模型来演示如何使用torch.distributed.rpc API 实现分布式管道并行性。 可以将其视为单机模型并行最佳实践中讨论的多 GPU 管道并行性的分布式对应物。 注意 本教程需要 PyTorch v1.6.0 或更高版本。 注意 本教程的完整源代码可以在pytorch/examples中找到。 基础知识 上一教程分布式 RPC 框架入门显示了如何使用torch.distributed.rpc为 RNN 模型实现分布式模型并行性。 该教程使用一个 GPU 来托管EmbeddingTable，并且提供的代码可以正常工作。 但是，如果模型驻留在多个 GPU 上，则将需要一些额外的步骤来增加所有 GPU 的摊销利用率。 管道并行性是在这种情况下可以提供帮助的一种范例。 在本教程中，我们使用ResNet50作为示例模型，单机模型并行最佳实践教程也使用了该模型。 类似地，ResNet50模型被分为两个碎片，输入批量被划分为多个拆分，并以流水线方式馈入两个模型碎片。 区别在于，本教程将调用异步 RPC，而不是使用 CUDA 流来并行执行。 因此，本教程中介绍的解决方案也可以跨计算机边界使用。 本教程的其余部分分四个步骤介绍了实现。 第 1 步：对 ResNet50 模型进行分片 这是在两个模型分片中实现ResNet50的准备步骤。 以下代码是从torchvision中的 ResNet 实现中借用的。 ResNetBase模块包含两个 ResNet 碎片的通用构件和属性。 import threading import torch import torch.nn as nn from torchvision.models.resnet import Bottleneck num_classes = 1000 def conv1x1(in_planes, out_planes, stride=1): return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) class ResNetBase(nn.Module): def __init__(self, block, inplanes, num_classes=1000, groups=1, width_per_group=64, norm_layer=None): super(ResNetBase, self).__init__() self._lock = threading.Lock() self._block = block self._norm_layer = nn.BatchNorm2d self.inplanes = inplanes self.dilation = 1 self.groups = groups self.base_width = width_per_group def _make_layer(self, planes, blocks, stride=1): norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if stride != 1 or self.inplanes != planes * self._block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * self._block.expansion, stride), norm_layer(planes * self._block.expansion), ) layers = [] layers.append(self._block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer)) self.inplanes = planes * self._block.expansion for _ in range(1, blocks): layers.append(self._block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def parameter_rrefs(self): return [RRef(p) for p in self.parameters()] 现在，我们准备定义两个模型碎片。 对于构造器，我们只需将所有 ResNet50 层分为两部分，然后将每个部分移至提供的设备中。 两个分片的forward函数获取输入数据的RRef，在本地获取数据，然后将其移至所需的设备。 将所有层应用于输入后，它将输出移至 CPU 并返回。 这是因为当调用方和被调用方中的设备数量不匹配时，RPC API 要求张量驻留在 CPU 上，以避免无效的设备错误。 class ResNetShard1(ResNetBase): def __init__(self, device, *args, **kwargs): super(ResNetShard1, self).__init__( Bottleneck, 64, num_classes=num_classes, *args, **kwargs) self.device = device self.seq = nn.Sequential( nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False), self._norm_layer(self.inplanes), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), self._make_layer(64, 3), self._make_layer(128, 4, stride=2) ).to(self.device) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) def forward(self, x_rref): x = x_rref.to_here().to(self.device) with self._lock: out = self.seq(x) return out.cpu() class ResNetShard2(ResNetBase): def __init__(self, device, *args, **kwargs): super(ResNetShard2, self).__init__( Bottleneck, 512, num_classes=num_classes, *args, **kwargs) self.device = device self.seq = nn.Sequential( self._make_layer(256, 6, stride=2), self._make_layer(512, 3, stride=2), nn.AdaptiveAvgPool2d((1, 1)), ).to(self.device) self.fc = nn.Linear(512 * self._block.expansion, num_classes).to(self.device) def forward(self, x_rref): x = x_rref.to_here().to(self.device) with self._lock: out = self.fc(torch.flatten(self.seq(x), 1)) return out.cpu() 第 2 步：将 ResNet50 模型片段拼接到一个模块中 然后，我们创建一个DistResNet50模块来组装两个分片并实现流水线并行逻辑。 在构造器中，我们使用两个rpc.remote调用分别将两个分片放在两个不同的 RPC 工作器上，并保持RRef到两个模型部分，以便可以在正向传播中引用它们。 forward函数将输入批量分为多个微批量，并将这些微批量以流水线方式馈送到两个模型部件。 它首先使用rpc.remote调用将第一个分片应用于微批量，然后将返回的中间输出RRef转发到第二个模型分片。 之后，它将收集所有微输出的Future，并在循环后等待所有它们。 请注意，remote()和rpc_async()都立即返回并异步运行。 因此，整个循环是非阻塞的，并将同时启动多个 RPC。 中间输出y_rref保留了两个模型零件上一个微批量的执行顺序。 微批量的执行顺序无关紧要。 最后，正向函数将所有微批量的输出连接到一个单一的输出张量中并返回。 parameter_rrefs函数是简化分布式优化器构造的助手，将在以后使用。 class DistResNet50(nn.Module): def __init__(self, num_split, workers, *args, **kwargs): super(DistResNet50, self).__init__() self.num_split = num_split # Put the first part of the ResNet50 on workers[0] self.p1_rref = rpc.remote( workers[0], ResNetShard1, args = (\"cuda:0\",) + args, kwargs = kwargs ) # Put the second part of the ResNet50 on workers[1] self.p2_rref = rpc.remote( workers[1], ResNetShard2, args = (\"cuda:1\",) + args, kwargs = kwargs ) def forward(self, xs): out_futures = [] for x in iter(xs.split(self.split_size, dim=0)): x_rref = RRef(x) y_rref = self.p1_rref.remote().forward(x_rref) z_fut = self.p2_rref.rpc_async().forward(y_rref) out_futures.append(z_fut) return torch.cat(torch.futures.wait_all(out_futures)) def parameter_rrefs(self): remote_params = [] remote_params.extend(self.p1_rref.remote().parameter_rrefs().to_here()) remote_params.extend(self.p2_rref.remote().parameter_rrefs().to_here()) return remote_params 步骤 3：定义训练循环 定义模型后，让我们实现训练循环。 我们使用专门的“主”工作器来准备随机输入和标签，并控制分布式反向传递和分布式优化器步骤。 它首先创建DistResNet50模块的实例。 它指定每个批量的微批数量，并提供两个 RPC 工作程序的名称（即worker1和worker2）。 然后，它定义损失函数，并使用parameter_rrefs()帮助器创建DistributedOptimizer以获取参数RRefs的列表。 然后，主训练循环与常规本地训练非常相似，除了它使用dist_autograd向后启动并为反向和优化器step()提供context_id之外。 import torch.distributed.autograd as dist_autograd import torch.optim as optim from torch.distributed.optim import DistributedOptimizer num_batches = 3 batch_size = 120 image_w = 128 image_h = 128 def run_master(num_split): # put the two model parts on worker1 and worker2 respectively model = DistResNet50(num_split, [\"worker1\", \"worker2\"]) loss_fn = nn.MSELoss() opt = DistributedOptimizer( optim.SGD, model.parameter_rrefs(), lr=0.05, ) one_hot_indices = torch.LongTensor(batch_size) \\ .random_(0, num_classes) \\ .view(batch_size, 1) for i in range(num_batches): print(f\"Processing batch {i}\") # generate random inputs and labels inputs = torch.randn(batch_size, 3, image_w, image_h) labels = torch.zeros(batch_size, num_classes) \\ .scatter_(1, one_hot_indices, 1) with dist_autograd.context() as context_id: outputs = model(inputs) dist_autograd.backward(context_id, [loss_fn(outputs, labels)]) opt.step(context_id) 第 4 步：启动 RPC 进程 最后，下面的代码显示了所有进程的目标函数。 主要逻辑在run_master中定义。 工作器被动地等待主服务器发出的命令，因此只需运行init_rpc和shutdown即可，其中默认情况下shutdown会阻塞，直到所有 RPC 参与者都完成。 import os import time import torch.multiprocessing as mp def run_worker(rank, world_size, num_split): os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=128) if rank == 0: rpc.init_rpc( \"master\", rank=rank, world_size=world_size, rpc_backend_options=options ) run_master(num_split) else: rpc.init_rpc( f\"worker{rank}\", rank=rank, world_size=world_size, rpc_backend_options=options ) pass # block until all rpcs finish rpc.shutdown() if __name__==\"__main__\": world_size = 3 for num_split in [1, 2, 4, 8]: tik = time.time() mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True) tok = time.time() print(f\"number of splits = {num_split}, execution time = {tok - tik}\") 下面的输出显示通过增加每批中的拆分数量而获得的加速。 $ python main.py Processing batch 0 Processing batch 1 Processing batch 2 number of splits = 1, execution time = 16.45062756538391 Processing batch 0 Processing batch 1 Processing batch 2 number of splits = 2, execution time = 12.329529762268066 Processing batch 0 Processing batch 1 Processing batch 2 number of splits = 4, execution time = 10.164430618286133 Processing batch 0 Processing batch 1 Processing batch 2 number of splits = 8, execution time = 9.076049566268921 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"67.html":{"url":"67.html","title":"使用异步执行实现批量 RPC 处理","keywords":"","body":"使用异步执行实现批量 RPC 处理 原文：https://pytorch.org/tutorials/intermediate/rpc_async_execution.html 作者：Shen Li 先决条件： PyTorch 分布式概述 分布式 RPC 框架入门 使用分布式 RPC 框架实现参数服务器 RPC 异步执行装饰器 本教程演示了如何使用@rpc.functions.async_execution装饰器来构建批量 RPC 应用，该装饰器通过减少阻止的 RPC 线程数和合并被调用方上的 CUDA 操作来帮助加快训练速度。 这使用 TorchServer 的相同想法进行批量推断。 注意 本教程需要 PyTorch v1.6.0 或更高版本。 基础知识 先前的教程显示了使用torch.distributed.rpc构建分布式训练应用的步骤，但并未详细说明在处理 RPC 请求时被调用方发生的情况。 从 PyTorch v1.5 开始，每个 RPC 请求都会在被调用方上阻塞一个线程，以在该请求中执行该函数，直到该函数返回为止。 这适用于许多用例，但有一个警告。 如果用户函数例如通过嵌套 RPC 调用在 IO 上阻塞，或者例如在等待其他 RPC 请求解除阻塞的信号时阻塞，则被调用方上的 RPC 线程将必须空闲，直到 IO 完成或发生信令事件为止。 结果，RPC 被调用者可能使用了不必要的更多线程。 造成此问题的原因是 RPC 将用户函数视为黑盒，并且几乎不了解该函数会发生什么。 为了允许用户函数产生和释放 RPC 线程，需要向 RPC 系统提供更多提示。 从 v1.6.0 开始，PyTorch 通过引入两个新概念来解决此问题： torch.futures.Future 类型封装了异步执行，还支持安装回调函数。 一个@rpc.functions.async_execution装饰器，允许应用告诉被调用方目标函数将返回将来的函数，并且在执行期间可以暂停并产生多次。 使用这两个工具，应用代码可以将用户函数分解为多个较小的函数，将它们作为Future对象上的回调链接在一起，然后返回包含最终结果的Future。 在被调用方，当获取Future对象时，它还将安装后续的 RPC 响应准备和通讯作为回调，这将在最终结果准备好时触发。 这样，被调用者不再需要阻塞一个线程并等待直到最终返回值准备就绪。 有关简单示例，请参考@rpc.functions.async_execution 的 API 文档。 除了减少被调用方上的空闲线程数之外，这些工具还有助于使批量 RPC 处理更容易，更快捷。 本教程的以下两节演示了如何使用@rpc.functions.async_execution装饰器来构建分布式批更新参数服务器和批量强化学习应用。 批量更新参数服务器 考虑具有一个参数服务器（PS）和多个训练器的同步参数服务器训练应用。 在此应用中，PS 保留参数并等待所有训练器报告坡度。 在每次迭代中，它都会等到收到所有训练者的梯度后，再一次更新所有参数。 下面的代码显示 PS 类的实现。 update_and_fetch_model方法是用@rpc.functions.async_execution装饰的，将由训练器调用。 每次调用都会返回一个Future对象，该对象将填充有更新的模型。 大多数训练器发起的调用仅将梯度累积到.grad字段，立即返回，并在 PS 上产生 RPC 线程。 最后到达的训练器将触发优化器步骤，并消耗所有先前报告的梯度。 然后，它使用更新的模型设置future_model，该模型又通过Future对象通知其他训练器的所有先前请求，并将更新后的模型发送给所有训练器。 import threading import torchvision import torch import torch.distributed.rpc as rpc from torch import optim num_classes, batch_update_size = 30, 5 class BatchUpdateParameterServer(object): def __init__(self, batch_update_size=batch_update_size): self.model = torchvision.models.resnet50(num_classes=num_classes) self.lock = threading.Lock() self.future_model = torch.futures.Future() self.batch_update_size = batch_update_size self.curr_update_size = 0 self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9) for p in self.model.parameters(): p.grad = torch.zeros_like(p) def get_model(self): return self.model @staticmethod @rpc.functions.async_execution def update_and_fetch_model(ps_rref, grads): # Using the RRef to retrieve the local PS instance self = ps_rref.local_value() with self.lock: self.curr_update_size += 1 # accumulate gradients into .grad field for p, g in zip(self.model.parameters(), grads): p.grad += g # Save the current future_model and return it to make sure the # returned Future object holds the correct model even if another # thread modifies future_model before this thread returns. fut = self.future_model if self.curr_update_size >= self.batch_update_size: # update the model for p in self.model.parameters(): p.grad /= self.batch_update_size self.curr_update_size = 0 self.optimizer.step() self.optimizer.zero_grad() # by settiing the result on the Future object, all previous # requests expecting this updated model will be notified and # the their responses will be sent accordingly. fut.set_result(self.model) self.future_model = torch.futures.Future() return fut 对于训练器，它们都使用来自 PS 的相同参数集进行初始化。 在每次迭代中，每位训练器首先进行前进和后退操作，以局部生成梯度。 然后，每个训练器都使用 RPC 向 PS 报告其梯度，并通过同一 RPC 请求的返回值取回更新的参数。 在训练器的实现中，目标函数是否标记有@rpc.functions.async_execution都没有关系。 训练器只需使用rpc_sync调用update_and_fetch_model，这会阻塞训练器，直到返回更新的模型。 batch_size, image_w, image_h = 20, 64, 64 class Trainer(object): def __init__(self, ps_rref): self.ps_rref, self.loss_fn = ps_rref, torch.nn.MSELoss() self.one_hot_indices = torch.LongTensor(batch_size) \\ .random_(0, num_classes) \\ .view(batch_size, 1) def get_next_batch(self): for _ in range(6): inputs = torch.randn(batch_size, 3, image_w, image_h) labels = torch.zeros(batch_size, num_classes) \\ .scatter_(1, self.one_hot_indices, 1) yield inputs.cuda(), labels.cuda() def train(self): name = rpc.get_worker_info().name # get initial model parameters m = self.ps_rref.rpc_sync().get_model().cuda() # start training for inputs, labels in self.get_next_batch(): self.loss_fn(m(inputs), labels).backward() m = rpc.rpc_sync( self.ps_rref.owner(), BatchUpdateParameterServer.update_and_fetch_model, args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]), ).cuda() 在本教程中，我们将跳过启动多个进程的代码，有关完整实现，请参考示例回购。 请注意，可以在没有@rpc.functions.async_execution装饰器的情况下实现批量。 但是，这将需要在 PS 上阻塞更多的 RPC 线程，或者使用另一轮 RPC 来获取更新的模型，后者将增加代码的复杂性和通信开销。 本节使用一个简单的参数服务器训练示例来说明如何使用@rpc.functions.async_execution装饰器实现批量 RPC 应用。 在下一节中，我们将使用批量重新实现上一分布式 RPC 框架入门指南中的强化学习示例，并演示其对训练速度的影响。 批量 CartPole 求解器 本节以 OpenAI Gym 中的 CartPole-v1 为例，说明批量 RPC 的性能影响。 请注意，我们的目标是演示@rpc.functions.async_execution的用法，而不是构建最佳的 CartPole 求解器或解决大多数不同的 RL 问题，我们使用非常简单的策略和奖励计算策略，并将重点放在多观察者单智能体的批量 RPC 实现。 我们使用与前面的教程类似的Policy模型，如下所示。 与上一教程相比，不同之处在于其构造器使用了一个附加的batch参数来控制F.softmax的dim参数，因为进行批量时，forward函数中的x参数包含来自多个观察者的状态，因此尺寸需要适当更改。 其他所有内容保持不变。 import argparse import torch.nn as nn import torch.nn.functional as F parser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example') parser.add_argument('--gamma', type=float, default=1.0, metavar='G', help='discount factor (default: 1.0)') parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)') parser.add_argument('--num-episode', type=int, default=10, metavar='E', help='number of episodes (default: 10)') args = parser.parse_args() torch.manual_seed(args.seed) class Policy(nn.Module): def __init__(self, batch=True): super(Policy, self).__init__() self.affine1 = nn.Linear(4, 128) self.dropout = nn.Dropout(p=0.6) self.affine2 = nn.Linear(128, 2) self.dim = 2 if batch else 1 def forward(self, x): x = self.affine1(x) x = self.dropout(x) x = F.relu(x) action_scores = self.affine2(x) return F.softmax(action_scores, dim=self.dim) Observer的构造器也会相应地进行调整。 它还带有batch参数，该参数控制用于选择动作的Agent函数。 在批量模式下，它将调用Agent上的select_action_batch函数，该函数将很快出现，并且该函数将以@rpc.functions.async_execution装饰。 import gym import torch.distributed.rpc as rpc class Observer: def __init__(self, batch=True): self.id = rpc.get_worker_info().id - 1 self.env = gym.make('CartPole-v1') self.env.seed(args.seed) self.select_action = Agent.select_action_batch if batch else Agent.select_action 与之前的教程分布式 RPC 框架入门相比，观察者的行为略有不同。 它不会在环境停止时退出，而是始终在每个剧集中运行n_steps迭代。 当环境返回时，观察者只需重置环境并重新开始。 通过这种设计，智能体将从每个观察者那里收到固定数量的状态，因此可以将它们打包成固定大小的张量。 在每个步骤中，Observer使用 RPC 将其状态发送到Agent，并通过返回值获取操作。 在每个剧集的结尾，它将所有步骤的奖励返还给Agent。 注意，Agent将使用 RPC 调用此run_episode函数。 因此，此函数中的rpc_sync调用将是嵌套的 RPC 调用。 我们也可以将此函数标记为@rpc.functions.async_execution，以避免阻塞Observer上的一个线程。 但是，由于瓶颈是Agent而不是Observer，因此可以在Observer进程中阻塞一个线程。 import torch class Observer: ... def run_episode(self, agent_rref, n_steps): state, ep_reward = self.env.reset(), NUM_STEPS rewards = torch.zeros(n_steps) start_step = 0 for step in range(n_steps): state = torch.from_numpy(state).float().unsqueeze(0) # send the state to the agent to get an action action = rpc.rpc_sync( agent_rref.owner(), self.select_action, args=(agent_rref, self.id, state) ) # apply the action to the environment, and get the reward state, reward, done, _ = self.env.step(action) rewards[step] = reward if done or step + 1 >= n_steps: curr_rewards = rewards[start_step:(step + 1)] R = 0 for i in range(curr_rewards.numel() -1, -1, -1): R = curr_rewards[i] + args.gamma * R curr_rewards[i] = R state = self.env.reset() if start_step == 0: ep_reward = min(ep_reward, step - start_step + 1) start_step = step + 1 return [rewards, ep_reward] Agent的构造器还采用batch参数，该参数控制如何对动作概率进行批量。 在批量模式下，saved_log_probs包含一张张量列表，其中每个张量包含一个步骤中所有观察者的动作抢夺。 如果不进行批量，则saved_log_probs是字典，其中的键是观察者 ID，值是该观察者的动作概率列表。 import threading from torch.distributed.rpc import RRef class Agent: def __init__(self, world_size, batch=True): self.ob_rrefs = [] self.agent_rref = RRef(self) self.rewards = {} self.policy = Policy(batch).cuda() self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2) self.running_reward = 0 for ob_rank in range(1, world_size): ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank)) self.ob_rrefs.append(rpc.remote(ob_info, Observer, args=(batch,))) self.rewards[ob_info.id] = [] self.states = torch.zeros(len(self.ob_rrefs), 1, 4) self.batch = batch self.saved_log_probs = [] if batch else {k:[] for k in range(len(self.ob_rrefs))} self.future_actions = torch.futures.Future() self.lock = threading.Lock() self.pending_states = len(self.ob_rrefs) 非批量select_acion只需运行状态抛出策略，保存动作概率，然后立即将动作返回给观察者。 from torch.distributions import Categorical class Agent: ... @staticmethod def select_action(agent_rref, ob_id, state): self = agent_rref.local_value() probs = self.policy(state.cuda()) m = Categorical(probs) action = m.sample() self.saved_log_probs[ob_id].append(m.log_prob(action)) return action.item() 使用批量时，状态以观察者 id 为行 ID 存储在 2D 张量self.states中。 然后，它通过将回调函数安装到批量生成的self.future_actions Future对象上来链接Future，该对象将使用使用该观察者 ID 索引的特定行进行填充。 最后到达的观察者一口气通过策略运行所有批量状态，并相应地设置self.future_actions。 发生这种情况时，将触发安装在self.future_actions上的所有回调函数，并使用它们的返回值来填充链接的Future对象，该对象进而通知Agent为所有先前的 RPC 请求准备和传达来自其他观察者的响应。 class Agent: ... @staticmethod @rpc.functions.async_execution def select_action_batch(agent_rref, ob_id, state): self = agent_rref.local_value() self.states[ob_id].copy_(state) future_action = self.future_actions.then( lambda future_actions: future_actions.wait()[ob_id].item() ) with self.lock: self.pending_states -= 1 if self.pending_states == 0: self.pending_states = len(self.ob_rrefs) probs = self.policy(self.states.cuda()) m = Categorical(probs) actions = m.sample() self.saved_log_probs.append(m.log_prob(actions).t()[0]) future_actions = self.future_actions self.future_actions = torch.futures.Future() future_actions.set_result(actions.cpu()) return future_action 现在，让我们定义如何将不同的 RPC 函数结合在一起。 Agent控制每个剧集的执行。 它首先使用rpc_async在所有观察者上开始该剧集，并阻止将由观察者奖励填充的返还期货。 请注意，以下代码使用 RRef 帮助器ob_rref.rpc_async()在具有提供的参数的ob_rref RRef 的所有者上启动run_episode函数。 然后将保存的动作概率和返回的观察者奖励转换为期望的数据格式，并开始训练步骤。 最后，它将重置所有状态并返回当前剧集的奖励。 此函数是运行一集的入口。 class Agent: ... def run_episode(self, n_steps=0): futs = [] for ob_rref in self.ob_rrefs: # make async RPC to kick off an episode on all observers futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps)) # wait until all obervers have finished this episode rets = torch.futures.wait_all(futs) rewards = torch.stack([ret[0] for ret in rets]).cuda().t() ep_rewards = sum([ret[1] for ret in rets]) / len(rets) # stack saved probs into one tensor if self.batch: probs = torch.stack(self.saved_log_probs) else: probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))] probs = torch.stack(probs) policy_loss = -probs * rewards / len(rets) policy_loss.sum().backward() self.optimizer.step() self.optimizer.zero_grad() # reset variables self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))} self.states = torch.zeros(len(self.ob_rrefs), 1, 4) # calculate running rewards self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward return ep_rewards, self.running_reward 其余代码是正常的进程启动和日志记录，与其他 RPC 教程类似。 在本教程中，所有观察者都被动地等待来自智能体的命令。 有关完整的实现，请参考示例回购。 def run_worker(rank, world_size, n_episode, batch, print_log=True): os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' if rank == 0: # rank0 is the agent rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size) agent = Agent(world_size, batch) for i_episode in range(n_episode): last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS) if print_log: print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format( i_episode, last_reward, running_reward)) else: # other ranks are the observer rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size) # observers passively waiting for instructions from agents rpc.shutdown() def main(): for world_size in range(2, 12): delays = [] for batch in [True, False]: tik = time.time() mp.spawn( run_worker, args=(world_size, args.num_episode, batch), nprocs=world_size, join=True ) tok = time.time() delays.append(tok - tik) print(f\"{world_size}, {delays[0]}, {delays[1]}\") if __name__ == '__main__': main() 批量 RPC 有助于将操作推断合并为较少的 CUDA 操作，从而减少了摊销的开销。 上面的main函数使用不同数量的观察者（从 1 到 10）在批量和无批量模式下运行相同的代码。下图使用默认参数值绘制了不同世界大小的执行时间。 结果证实了我们的期望，即批量有助于加快训练速度。 了解更多 批量更新参数服务器的源代码 批量 CartPole 求解器 分布式 Autograd 分布式管道并行性 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"68.html":{"url":"68.html","title":"将分布式DataParallel与分布式 RPC 框架相结合","keywords":"","body":"将分布式DataParallel与分布式 RPC 框架相结合 原文：https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html 作者： Pritam Damania 本教程使用一个简单的示例演示如何将DistributedDataParallel（DDP）与分布式 RPC 框架结合使用，以将分布式数据并行性与分布式模型并行性结合在一起，以训练简单模型。 该示例的源代码可以在中找到。 先前的教程分布式数据并行入门和分布式 RPC 框架入门分别描述了如何执行分布式数据并行训练和分布式模型并行训练。 虽然，有几种训练范例，您可能想将这两种技术结合起来。 例如： 如果我们的模型具有稀疏部分（较大的嵌入表）和密集部分（FC 层），则可能需要将嵌入表放在参数服务器上，并使用DistributedDataParallel。 分布式 RPC 框架可用于在参数服务器上执行嵌入查找。 如 PipeDream 论文中所述，启用混合并行性。 我们可以使用分布式 RPC 框架在多个工作程序之间流水线化模型的各个阶段，并使用DistributedDataParallel复制每个阶段（如果需要）。 在本教程中，我们将介绍上述情况 1。 我们的设置中共有 4 个工作器，如下所示： 1 个主机，负责在参数服务器上创建嵌入表（nn.EmbeddingBag）。 主人还会在两个教练上驱动训练循环。 1 参数服务器，它基本上将嵌入表保存在内存中，并响应来自主服务器和训练器的 RPC。 2 个训练器，用于存储 FC 层（线性线性），并使用DistributedDataParallel在它们之间进行复制。 训练人员还负责执行前进，后退和优化器步骤。 整个训练过程执行如下： 主服务器在参数服务器上创建一个嵌入表，并为其保留一个 RRef。 然后，主持人开始在训练器上进行训练循环，并将嵌入表 RRef 传递给训练器。 训练器创建一个HybridModel，该HybridModel首先使用主机提供的嵌入表 RRef 执行嵌入查找，然后执行包装在 DDP 中的 FC 层。 训练者执行模型的正向传播，并使用分布式 Autograd 使用损失执行反向传递。 作为向后遍历的一部分，将首先计算 FC 层的梯度，并通过 DDP 中的allreduce将其同步到所有训练器。 接下来，分布式 Autograd 将梯度传播到参数服务器，在该服务器中更新嵌入表的梯度。 最后，分布式优化器用于更新所有参数。 注意 如果您将 DDP 和 RPC 结合使用，则应始终使用分布式 Autograd 进行反向传播。 现在，让我们详细介绍每个部分。 首先，我们需要先设置所有工作器，然后才能进行任何训练。 我们创建 4 个过程，使等级 0 和 1 是我们的训练器，等级 2 是主控制器，等级 3 是参数服务器。 我们使用 TCP init_method 在所有 4 个工作器上初始化 RPC 框架。 RPC 初始化完成后，主服务器使用rpc.remote在参数服务器上创建EmbeddingBag。 然后，主控制器通过使用rpc_async在每个教练上调用_run_trainer，循环遍历每个教练并开始训练循环。 最后，主人在退出之前等待所有训练结束。 训练器首先使用init_process_group为world_size = 2的 DDP 初始化ProcessGroup（对于两个训练器）。 接下来，他们使用 TCP init_method初始化 RPC 框架。 请注意，RPC 初始化和ProcessGroup初始化中的端口不同。 这是为了避免两个框架的初始化之间的端口冲突。 初始化完成后，训练器只需等待主服务器的_run_trainer RPC。 参数服务器只是初始化 RPC 框架，并等待来自训练者和主服务器的 RPC。 def run_worker(rank, world_size): r\"\"\" A wrapper function that initializes RPC, calls the function, and shuts down RPC. \"\"\" os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' rpc_backend_options = TensorPipeRpcBackendOptions() rpc_backend_options.init_method='tcp://localhost:29501' # Rank 2 is master, 3 is ps and 0 and 1 are trainers. if rank == 2: rpc.init_rpc( \"master\", rank=rank, world_size=world_size, rpc_backend_options=rpc_backend_options) # Build the embedding table on the ps. emb_rref = rpc.remote( \"ps\", torch.nn.EmbeddingBag, args=(NUM_EMBEDDINGS, EMBEDDING_DIM), kwargs={\"mode\": \"sum\"}) # Run the training loop on trainers. futs = [] for trainer_rank in [0, 1]: trainer_name = \"trainer{}\".format(trainer_rank) fut = rpc.rpc_async( trainer_name, _run_trainer, args=(emb_rref, rank)) futs.append(fut) # Wait for all training to finish. for fut in futs: fut.wait() elif rank 在讨论训练器的详细信息之前，让我们介绍一下训练器使用的HybridModel。 如下所述，使用对参数服务器上嵌入表（emb_rref）的 RRef 和用于 DDP 的device初始化HybridModel。 模型的初始化在 DDP 中包装了nn.Linear层，以在所有训练器之间复制和同步该层。 该模型的前进方法非常简单。 它使用 RRef 帮助程序在参数服务器上执行嵌入查找，并将其输出传递到 FC 层。 class HybridModel(torch.nn.Module): r\"\"\" The model consists of a sparse part and a dense part. The dense part is an nn.Linear module that is replicated across all trainers using DistributedDataParallel. The sparse part is an nn.EmbeddingBag that is stored on the parameter server. The model holds a Remote Reference to the embedding table on the parameter server. \"\"\" def __init__(self, emb_rref, device): super(HybridModel, self).__init__() self.emb_rref = emb_rref self.fc = DDP(torch.nn.Linear(16, 8).cuda(device), device_ids=[device]) self.device = device def forward(self, indices, offsets): emb_lookup = self.emb_rref.rpc_sync().forward(indices, offsets) return self.fc(emb_lookup.cuda(self.device)) 接下来，让我们看看训练器上的设置。 训练者首先使用对参数服务器上嵌入表的 RRef 及其自身等级创建上述HybridModel。 现在，我们需要检索要使用DistributedOptimizer优化的所有参数的 RRef 列表。 为了从参数服务器中检索嵌入表的参数，我们定义了一个简单的辅助函数_retrieve_embedding_parameters，该函数基本上遍历了嵌入表的所有参数并返回 RRef 的列表。 训练器通过 RPC 在参数服务器上调用此方法，以接收所需参数的 RRef 列表。 由于DistributedOptimizer始终将需要优化的参数的 RRef 列表，因此我们甚至需要为 FC 层的本地参数创建 RRef。 这是通过遍历model.parameters()，为每个参数创建 RRef 并将其附加到列表来完成的。 请注意，model.parameters()仅返回本地参数，不包含emb_rref。 最后，我们使用所有 RRef 创建我们的DistributedOptimizer，并定义CrossEntropyLoss函数。 def _retrieve_embedding_parameters(emb_rref): param_rrefs = [] for param in emb_rref.local_value().parameters(): param_rrefs.append(RRef(param)) return param_rrefs def _run_trainer(emb_rref, rank): r\"\"\" Each trainer runs a forward pass which involves an embedding lookup on the parameter server and running nn.Linear locally. During the backward pass, DDP is responsible for aggregating the gradients for the dense part (nn.Linear) and distributed autograd ensures gradients updates are propagated to the parameter server. \"\"\" # Setup the model. model = HybridModel(emb_rref, rank) # Retrieve all model parameters as rrefs for DistributedOptimizer. # Retrieve parameters for embedding table. model_parameter_rrefs = rpc.rpc_sync( \"ps\", _retrieve_embedding_parameters, args=(emb_rref,)) # model.parameters() only includes local parameters. for param in model.parameters(): model_parameter_rrefs.append(RRef(param)) # Setup distributed optimizer opt = DistributedOptimizer( optim.SGD, model_parameter_rrefs, lr=0.05, ) criterion = torch.nn.CrossEntropyLoss() 现在，我们准备介绍在每个训练器上运行的主要训练循环。 get_next_batch只是一个辅助函数，用于生成随机输入和训练目标。 我们针对多个周期和每个批量运行训练循环： 为分布式 Autograd 设置分布式 Autograd 上下文。 运行模型的正向传播并检索其输出。 使用损失函数，根据我们的输出和目标计算损失。 使用分布式 Autograd 使用损失执行分布式反向传递。 最后，运行“分布式优化器”步骤以优化所有参数。 def get_next_batch(rank): for _ in range(10): num_indices = random.randint(20, 50) indices = torch.LongTensor(num_indices).random_(0, NUM_EMBEDDINGS) # Generate offsets. offsets = [] start = 0 batch_size = 0 while start 整个示例的源代码可以在这里找到。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2021-02-06 11:45:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}